{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pF8beWv0lrgc"
   },
   "source": [
    "# Web Scraping Yahoo! Finance using Python\n",
    "\n",
    "A detailed guide for web scraping https://finance.yahoo.com/ using **requests**, **BeautifulSoup**, **Selenium**, **HTML tags** & embedded **JSON** data.\n",
    "\n",
    "![](https://imgur.com/7jMFOcE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FULGhEjlrge"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "**What is Web scraping?**<br>\n",
    "Web scraping is the process of extracting and parsing data from websites in an automated fashion using a computer program. It's a useful technique for creating datasets for research and learning.\n",
    "\n",
    "\n",
    "**Objective**<br>\n",
    "The main objective of this tutorial is to showcase different web scraping methods which can be applied to any web page. \n",
    "This is for educational purposes only. Please read the Terms & Conditions carefully for any website whether you can legally use the data. \n",
    "\n",
    "In this Project we will perform web scraping using following 3 techniques based on the problem statement.\n",
    "* use `requests`, `BeautifulSoup` and `HTML tags` to extract web page\n",
    "* use `Selenium` to scrape data from dynamically loading websites \n",
    "* use embedded `JSON`data to scrape website "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CFXNNENlrgf"
   },
   "source": [
    "**The problem statement**<br>\n",
    "1. Scrape **Stock Market News** (url : https://finance.yahoo.com/topic/stock-market-news/) :<br>\n",
    "    This web page shows latest **news** related to **stock market**, we will try to extract data from this web page and store it in `CSV` (comma-separated values) file. The file layout would be as mentioned below.\n",
    "    ```\n",
    "    source,headline,url,content,image\n",
    "    <source of the news>,<news head line>,<news url>,<news content>,<news thumbnail image>\n",
    "    ```\n",
    "\n",
    "2. Scrape **Cryptocurrencies** (url : https://finance.yahoo.com/cryptocurrencies) :<br>\n",
    "    This yahoo finance web page is showing list of trending **Cryptocurrencies** in tabular format, we will perform the web scraping to retrieve first 10 columns for top 100 **Cryptocurrencies** in `CSV` format.\n",
    "    ```\n",
    "    Symbol,Name,Price (Intraday),Change,% Change,Market Cap,Volume in Currency (Since 0:00 UTC),\n",
    "    Volume in Currency (24Hr),Total Volume All Currencies (24Hr),Circulating Supply\n",
    "    BTC-USD,Bitcoin USD,\"43,312.13\",-947.50,-2.14%,821.76B,27.727B,27.727B,27.727B,18.973M\n",
    "\n",
    "    ```\n",
    "        \n",
    "3. Scrape **Market Events Calendar** (url : https://finance.yahoo.com/calendar) :<br> \n",
    "    This page is showing **date-wise market events**, user have option to select the date and choose any one of the following market event **Earnings**, **Stock Splits**, **Economic Events** & **IPO**. Our aim is to create script which can be run for any single date and market event which grabs the data and load it in `CSV` format.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Z9UQZf8lrgg"
   },
   "source": [
    "**Prerequisites**\n",
    "* Knowledge of Python\n",
    "* Basic knowledge of HTML although it is not necessary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbMRHIhblrgh"
   },
   "source": [
    "**How to run the Code**<br>\n",
    "You can execute the code using \"Run\" button on the top of this page and selecting **\"Run on Colab\"** or **\"Run Locally\"** \n",
    "<br>\n",
    "<br>\n",
    "**Setup and Tools**<br>\n",
    "<u>Run on Colab :</u> \n",
    "    You will need to provide the Google login to run this notebook on Colab.<br>\n",
    "<u>Run Locally :</u> Download and install [Anaconda](https://www.anaconda.com/) framework, We will be using Jupyter Notebook for writing the & executing code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YjeIdedlrgh"
   },
   "source": [
    "**Code Re-usability & Version control**\n",
    "\n",
    "You can make changes and save your version of the notebook to [Jovian](https://jovian.ai/) by executing following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4779,
     "status": "ok",
     "timestamp": 1646378957162,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "iLLVpn2olrgi"
   },
   "outputs": [],
   "source": [
    "!pip install jovian --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1646378957163,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "iTKJFEYglrgj"
   },
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "executionInfo": {
     "elapsed": 1861,
     "status": "ok",
     "timestamp": 1646378959013,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "AxYOBC1xlrgj",
    "outputId": "cc12d408-507c-4287-b7ce-f771943c28d1"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"vinodvidhole/yahoo-finance-web-scraper\" on https://jovian.ai/\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/vinodvidhole/yahoo-finance-web-scraper\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/vinodvidhole/yahoo-finance-web-scraper'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute this to save new versions of the notebook\n",
    "jovian.commit(project=\"yahoo-finance-web-scraper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SWl2eoMlrgk"
   },
   "source": [
    "## 1. Scrape Stock Market News\n",
    "\n",
    "In this section we will learn basic Python web scraping technique using `requests`, `BeautifulSoup` and `HTML tags`. The objective here is to perform web scraping of [Yahoo! finance Stock Market News](https://finance.yahoo.com/topic/stock-market-news/)\n",
    "\n",
    "![](https://i.imgur.com/1I0Btau.jpg)\n",
    "\n",
    "Lets kick start with the first objective. Here's an outline of the steps we'll follow<br>\n",
    "**1.1 Download & Parse web page using `requests` and `BeautifulSoup`**<br>\n",
    "**1.2 Exploring and locating Elements**<br>\n",
    "**1.3 Extract & Compile the information into python list**<br>\n",
    "**1.4 Save the extracted information to a CSV file**<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSis_3UJlrgl"
   },
   "source": [
    "### 1.1 Download & Parse webpage using requests and BeautifulSoup\n",
    "\n",
    "First step is to install [`requests`](https://docs.python-requests.org/en/latest/) & [`beautifulsoup4`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) Libraries using `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 11314,
     "status": "ok",
     "timestamp": 1646378970321,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "zXJKsNyQlrgl"
   },
   "outputs": [],
   "source": [
    "!pip install requests --quiet\n",
    "!pip install beautifulsoup4 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1646378970322,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "L_gdMI8llrgm"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1d4YUeDlrgm"
   },
   "source": [
    "The library is now installed and imported.<br>\n",
    "\n",
    "To download the page, we can use `requests.get`, which returns a response object. the HTML information of web page is captured in `response.text`.<br>\n",
    "`response.ok` & [`response.status_code`](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status) can be used for error trapping &  tracking.<br> \n",
    "Finally we can use `BeautifulSoup` to parse the HTML data, this will return `bs4.BeautifulSoup` object  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rD6E93Trlrgm"
   },
   "source": [
    "Lets create a function to perform this step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1646378970322,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "-FIURHiqlrgm"
   },
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    \"\"\"Download a webpage and return a beautiful soup doc\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if not response.ok:\n",
    "        print('Status code:', response.status_code)\n",
    "        raise Exception('Failed to load page {}'.format(url))\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "susW06Hwlrgn"
   },
   "source": [
    "calling function `get_page` and analyze the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 465,
     "status": "ok",
     "timestamp": 1646378970782,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "cGWBiRCJlrgn"
   },
   "outputs": [],
   "source": [
    "my_url = 'https://finance.yahoo.com/topic/stock-market-news/' \n",
    "doc = get_page(my_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1646378970782,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "jvwJU_ULlrgn",
    "outputId": "d843e210-6916-47a3-98be-49e94cb4da7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of doc:  <class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "print('Type of doc: ',type(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqEwtKgTlrgn"
   },
   "source": [
    "You can access different properties of HTML web page from doc, following example will display Title of the web page.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 181,
     "status": "ok",
     "timestamp": 1646378970960,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "S0xvsoyelrgo",
    "outputId": "090295a9-563b-46fa-ca3d-c401f4eeb2fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Latest Stock Market News</title>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.find('title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hbs8VJS9lrgo"
   },
   "source": [
    "We can now use the function `get_page` to download any web page and parse it using beautiful soup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvSMJZeClrgo"
   },
   "source": [
    "### 1.2 Exploring and locating Elements\n",
    "Now its time to explore the elements to find the required data point from the web page. Web pages are written in a language called HTML (Hyper Text Markup Language).  HTML is a fairly simple language comprised of *tags*  (also called *nodes* or *elements*) e.g. `<a href=\"https://finance.yahoo.com/\" target=\"_blank\">Go to Yahoo! Finance</a>`. An HTML tag has three parts:\n",
    "\n",
    "\n",
    "\n",
    "1. **Name**: (`html`, `head`, `body`, `div`, etc.) Indicates what the tag represents and how a browser should interpret the information inside it.\n",
    "2. **Attributes**: (`href`, `target`, `class`, `id`, etc.) Properties of tag used by the browser to customize how a tag is displayed and decide what happens on user interactions.\n",
    "3. **Children**: A tag can contain some text or other tags or both between the opening and closing segments, e.g., `<div>Some content</div>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAFbXWN2lrgo"
   },
   "source": [
    "Now lets inspect the webpage source code by right-click and select the \"Inspect\" option. First we need to identify the tag which represents the news listing.\n",
    "\n",
    "## TODO FIX BELOW "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C97r1w_5lrgo"
   },
   "source": [
    "![](https://media.giphy.com/media/RQpW64jdiQG8LNCGsZ/giphy.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fr5ofFpFlrgo"
   },
   "source": [
    "In this case we can see the `<div>` tag having class name `\"Ov(h) Pend(44px) Pstart(25px)\"` is representing news listing, we can apply `find_all` method to grab this information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1646378970960,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "WF_cAVptlrgo"
   },
   "outputs": [],
   "source": [
    "div_tags = doc.find_all('div', {'class': \"Ov(h) Pend(44px) Pstart(25px)\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_Y4L3Kglrgp"
   },
   "source": [
    "Total elements in the `<div>` tag list is matching with the numbers of news displaying in the webpage , so we are heading towards right direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1646378970961,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "FniLi1nElrgp",
    "outputId": "be01aa10-4467-42ad-a6c6-96901f74694a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(div_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4acfugiWlrgp"
   },
   "source": [
    "Next step to inspect the individual `<div>` tag and try to find more information. I am using \"Visual Studio Code\", but you can use any tool as simple as notepad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1646378970961,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "paFm23Amlrgp",
    "outputId": "195e1565-6ad1-44fd-f13e-f378770d3c67"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"Ov(h) Pend(44px) Pstart(25px)\"><div class=\"C(#959595) Fz(11px) D(ib) Mb(6px)\">Barrons.com</div><h3 class=\"Mb(5px)\"><a class=\"js-content-viewer wafer-caas Fw(b) Fz(18px) Lh(23px) LineClamp(2,46px) Fz(17px)--sm1024 Lh(19px)--sm1024 LineClamp(2,38px)--sm1024 mega-item-header-link Td(n) C(#0078ff):h C(#000) LineClamp(2,46px) LineClamp(2,38px)--sm1024 not-isInStreamVideoEnabled\" data-uuid=\"9cd12c94-4e50-3e71-ab97-fde275ed13b4\" data-wf-caas-prefetch=\"1\" data-wf-caas-uuid=\"9cd12c94-4e50-3e71-ab97-fde275ed13b4\" href=\"/m/9cd12c94-4e50-3e71-ab97-fde275ed13b4/8-experts-explain-what-the.html\"><u class=\"StretchedBox\"></u>8 Experts Explain What the Ukraine War Means for Markets</a></h3><p class=\"Fz(14px) Lh(19px) Fz(13px)--sm1024 Lh(17px)--sm1024 LineClamp(2,38px) LineClamp(2,34px)--sm1024 M(0)\">FEATURES - MAIN  The fog of this particularly bitter war in Ukraine has crept into global markets. Investors confront a rapidly changing geopolitical scene atop an economy already struggling to cope with inflation, supply-chain woes, and looming interest-rate hikes.</p></div>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_tags[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaY4rqHxlrgp"
   },
   "source": [
    "![](https://i.imgur.com/ncnfg0z.png)\n",
    "\n",
    "Luckily most of the required data points are available in this `<div>`, so we can use `find` method to grab each items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 159,
     "status": "ok",
     "timestamp": 1646378971115,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "Hn02jUnUlrgp",
    "outputId": "50e3c350-9f35-43f6-fe99-06fe67b9b334"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:  Barrons.com\n",
      "Head Line : 8 Experts Explain What the Ukraine War Means for Markets\n"
     ]
    }
   ],
   "source": [
    "print(\"Source: \", div_tags[1].find('div').text)\n",
    "print(\"Head Line : {}\".format(div_tags[1].find('a').text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54ry5o3-lrgp"
   },
   "source": [
    "If any tag is not accessible directly, then you can use methods like `findParent()` or `'findChild()` to point to the required tag.\n",
    "\n",
    "![](https://i.imgur.com/OnOAtT2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1646378971115,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "BYm7cqy2lrgp",
    "outputId": "d24762d1-d0dc-454c-a209-db8a32cc93a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image URL:  https://s.yimg.com/uu/api/res/1.2/gCbg0OfapgNefhBwPUuGDQ--~B/Zmk9c3RyaW07aD0xMjM7cT04MDt3PTIyMDthcHBpZD15dGFjaHlvbg--/https://s.yimg.com/uu/api/res/1.2/ojdUsX1p3aFadEgMIlbtTw--~B/aD02NDA7dz0xMjgwO2FwcGlkPXl0YWNoeW9u/https://media.zenfs.com/en/Barrons.com/14a0d9cff5bd13ab27dde3653243b580.cf.jpg\n"
     ]
    }
   ],
   "source": [
    "print(\"Image URL: \",div_tags[1].findParent().find('img')['src'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_rZ3hP_lrgq"
   },
   "source": [
    "Key Takeout from this exercise is to identify the optimal tag which will provide us required information. Sometimes its straight forward, sometimes you will have to perform little more research.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylJlxrEhlrgq"
   },
   "source": [
    "### 1.3 Extract & Compile the information into python list\n",
    "\n",
    "Now we've identified all required tags and information, Let's put this together in the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1646378971115,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "Ck_Buvi8lrgr"
   },
   "outputs": [],
   "source": [
    "def get_news_tags(doc):\n",
    "    \"\"\"Get the list of tags containing news information\"\"\"\n",
    "    news_class = \"Ov(h) Pend(44px) Pstart(25px)\" ## class name of div tag \n",
    "    news_list  = doc.find_all('div', {'class': news_class})\n",
    "    return news_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u93jXLLslrgr"
   },
   "source": [
    "sample run of the function `get_news_tags`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1646378971116,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "byS55R-9lrgr"
   },
   "outputs": [],
   "source": [
    "my_news_tags = get_news_tags(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfXnnMASlrgr"
   },
   "source": [
    "we will create one more function, to parse individual `<div>` tag and return the information in dictionary form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1646378971116,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "bsuD1-O4lrgr"
   },
   "outputs": [],
   "source": [
    "BASE_URL = 'https://finance.yahoo.com' #Global Variable \n",
    "\n",
    "def parse_news(news_tag):\n",
    "    \"\"\"Get the news data point and return dictionary\"\"\"\n",
    "    news_source = news_tag.find('div').text #source\n",
    "    news_headline = news_tag.find('a').text #heading\n",
    "    news_url = news_tag.find('a')['href'] #link\n",
    "    news_content = news_tag.find('p').text #content\n",
    "    news_image = news_tag.findParent().find('img')['src'] #thumb image\n",
    "    return { 'source' : news_source,\n",
    "            'headline' : news_headline,\n",
    "            'url' : BASE_URL + news_url,\n",
    "            'content' : news_content,\n",
    "            'image' : news_image\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q88hBshllrgr"
   },
   "source": [
    "Lets test this `parse_news` function on first `<div>` tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1646378971116,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "7gO3YYf2lrgr",
    "outputId": "6803b5b3-a7f9-4c36-96d7-7a1303a8e6c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'Bloomberg',\n",
       " 'headline': 'Elizabeth Warren Says Wall Street ‘Undermining’ Russia Sanctions',\n",
       " 'url': 'https://finance.yahoo.com/news/elizabeth-warren-says-wall-street-025751915.html',\n",
       " 'content': '(Bloomberg) -- Senator Elizabeth Warren, a vocal critic of Wall Street, said banks are “undermining” sanctions on Russia by snapping up the nation’s corporate bonds and suggesting clients buy assets on the cheap. Most Read from BloombergUkraine Update: UN Security Council to Meet, Warren Blasts BanksRussian Forces Occupy Site of Nuclear Plant as Fire ContainedUkraine Update: Russian Troops Occupy Nuclear Plant SiteWhite House Weighs Ban on Russian Oil Imports as Congress FumesPutin’s Financial I',\n",
       " 'image': 'https://s.yimg.com/uu/api/res/1.2/vb.zUj7a.FrPnJG2VGoYwg--~B/Zmk9c3RyaW07aD0xMjM7cT04MDt3PTIyMDthcHBpZD15dGFjaHlvbg--/https://s.yimg.com/uu/api/res/1.2/FMGWyktHVgDXiOsIA7Gr7w--~B/aD00MDAwO3c9NjAwMDthcHBpZD15dGFjaHlvbg--/https://media.zenfs.com/en/bloomberg_markets_842/8d794fbbc3901e15cb6034fbbc8e4052.cf.jpg'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_news(my_news_tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4Jt8PhXlrgr"
   },
   "source": [
    "We can use the `get_news_tags` & `parse_news` functions to pars news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ruE_5Q2lrgs"
   },
   "source": [
    "### 1.4 Save the extracted information to a CSV file\n",
    "\n",
    "This is the last step of this section, We are going to use Python library [`pandas`](https://pandas.pydata.org/docs/) to save the data in CSV format. Install and then Import the pandas Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 6306,
     "status": "ok",
     "timestamp": 1646378977418,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "fFiWEPAalrgs"
   },
   "outputs": [],
   "source": [
    "!pip install pandas --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1646378977419,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "zMrbCInvlrgs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JoZJq6GTlrgs"
   },
   "source": [
    "Now we will create one final function, in this function we will use all previously created helper functions.<br>\n",
    "The `get_page` function will download HTML page,then we can pass the result in `get_news_tags` to identify list of `<div>` tags for news.<br>\n",
    "After that we will use [List Comprehension](https://www.w3schools.com/python/python_lists_comprehension.asp) technique to pars each `<div>` tag using `parse_news`, the output will be in the form of `lists` of `dictionaries`<br>\n",
    "Finally we will use `DataFrame` method to create pandas [dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) and use `to_csv` method to store required data in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1646378977419,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "8evbvhJklrgs"
   },
   "outputs": [],
   "source": [
    "def scrape_yahoo_news(url, path=None):\n",
    "    \"\"\"Get the yahoo finance market news and write them to CSV file \"\"\"\n",
    "    if path is None:\n",
    "        path = 'stock-market-news.csv'\n",
    "        \n",
    "    print('Requesting html page')\n",
    "    doc = get_page(url)\n",
    "\n",
    "    print('Extracting news tags')\n",
    "    news_list = get_news_tags(doc)\n",
    "\n",
    "    print('Parsing news tags')\n",
    "    news_data = [parse_news(news_tag) for news_tag in news_list]\n",
    "\n",
    "    print('Save the data to a CSV')\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    news_df.to_csv(path, index=None)\n",
    "    \n",
    "    #This return statement is optional, we are doing this just analyze the final output \n",
    "    return news_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGYzOftflrgs"
   },
   "source": [
    "It's time to test the `scrape_yahoo_news` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1646378977761,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "ZhSeuZ6Pi8MD",
    "outputId": "9cb09a31-de02-4a4c-841f-d3eab18f3df1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting html page\n",
      "Extracting news tags\n",
      "Parsing news tags\n",
      "Save the data to a CSV\n"
     ]
    }
   ],
   "source": [
    "YAHOO_NEWS_URL = BASE_URL+'/topic/stock-market-news/'\n",
    "news_df = scrape_yahoo_news(YAHOO_NEWS_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBD6nB50lrgs"
   },
   "source": [
    "The \"stock-market-news.csv\" should be available in File --> Open Menu, you can download the file or directly open it on browser. Please verify the file content and compare it with the actual information available on the webpage.\n",
    "\n",
    "You can also check the data by grabbing few rows form the data frame returned by the `scrape_yahoo_news` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "executionInfo": {
     "elapsed": 178,
     "status": "ok",
     "timestamp": 1646378977934,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "1segLgXIlrgs",
    "outputId": "ec68e28b-6ddc-4ada-ef18-4245356f5b05"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>headline</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>Elizabeth Warren Says Wall Street ‘Undermining...</td>\n",
       "      <td>https://finance.yahoo.com/news/elizabeth-warre...</td>\n",
       "      <td>(Bloomberg) -- Senator Elizabeth Warren, a voc...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/vb.zUj7a.FrP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Barrons.com</td>\n",
       "      <td>8 Experts Explain What the Ukraine War Means f...</td>\n",
       "      <td>https://finance.yahoo.com/m/9cd12c94-4e50-3e71...</td>\n",
       "      <td>FEATURES - MAIN  The fog of this particularly ...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/gCbg0OfapgNe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MarketWatch</td>\n",
       "      <td>These money and investing tips can help you th...</td>\n",
       "      <td>https://finance.yahoo.com/m/3b362bd9-1aeb-35eb...</td>\n",
       "      <td>MUTUAL FUNDS WEEKLY  Don’t miss these top mone...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/dWWIelUkEj.h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>SMBC Nikko Says Staff Arrested for Alleged Mar...</td>\n",
       "      <td>https://finance.yahoo.com/news/smbc-nikko-says...</td>\n",
       "      <td>(Bloomberg) -- Tokyo Prosecutors have arrested...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/utbbiYadNZMW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Barrons.com</td>\n",
       "      <td>War in Ukraine Is Warping Markets, Policy, Ind...</td>\n",
       "      <td>https://finance.yahoo.com/m/3114df57-1528-30ee...</td>\n",
       "      <td>Investment newsletters examine the impact of R...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/Ytyg18H0_yee...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        source                                           headline  \\\n",
       "0    Bloomberg  Elizabeth Warren Says Wall Street ‘Undermining...   \n",
       "1  Barrons.com  8 Experts Explain What the Ukraine War Means f...   \n",
       "2  MarketWatch  These money and investing tips can help you th...   \n",
       "3    Bloomberg  SMBC Nikko Says Staff Arrested for Alleged Mar...   \n",
       "4  Barrons.com  War in Ukraine Is Warping Markets, Policy, Ind...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://finance.yahoo.com/news/elizabeth-warre...   \n",
       "1  https://finance.yahoo.com/m/9cd12c94-4e50-3e71...   \n",
       "2  https://finance.yahoo.com/m/3b362bd9-1aeb-35eb...   \n",
       "3  https://finance.yahoo.com/news/smbc-nikko-says...   \n",
       "4  https://finance.yahoo.com/m/3114df57-1528-30ee...   \n",
       "\n",
       "                                             content  \\\n",
       "0  (Bloomberg) -- Senator Elizabeth Warren, a voc...   \n",
       "1  FEATURES - MAIN  The fog of this particularly ...   \n",
       "2  MUTUAL FUNDS WEEKLY  Don’t miss these top mone...   \n",
       "3  (Bloomberg) -- Tokyo Prosecutors have arrested...   \n",
       "4  Investment newsletters examine the impact of R...   \n",
       "\n",
       "                                               image  \n",
       "0  https://s.yimg.com/uu/api/res/1.2/vb.zUj7a.FrP...  \n",
       "1  https://s.yimg.com/uu/api/res/1.2/gCbg0OfapgNe...  \n",
       "2  https://s.yimg.com/uu/api/res/1.2/dWWIelUkEj.h...  \n",
       "3  https://s.yimg.com/uu/api/res/1.2/utbbiYadNZMW...  \n",
       "4  https://s.yimg.com/uu/api/res/1.2/Ytyg18H0_yee...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptBKMG_Alrgt"
   },
   "source": [
    "**Summary** : Hopefully I was able to explain this simple but very powerful Python technique to scrape the yahoo finance market news. These steps can be used to scrape any web page, you just have to little research to identify required <tags> and use relevant python methods to collect the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaOUKkUOlrgt"
   },
   "source": [
    "## 2. Scrape Cryptocurrencies\n",
    "\n",
    "In phase One we were able to scrape the [yahoo market news](https://finance.yahoo.com/topic/stock-market-news/) web page. However If you've noticed, as we scroll down the web page more news will appear at the bottom of the page. This is called dynamic page loading. Previous technique is a basic Python method useful to scrape static data, To scrape the dynamically loading data will use a different method called webs craping using **Selenium**. Lets move ahead with this topic. The goal of this section is extract top listing [Crypto currencies](https://finance.yahoo.com/cryptocurrencies) from Yahoo! finance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuViJR0Elrgt"
   },
   "source": [
    "![](https://i.imgur.com/sF6k0Pk.jpg)\n",
    "\n",
    "\n",
    "Here's an outline of the steps we'll follow<br>\n",
    "**2.1 Introduction of selenium**<br>\n",
    "**2.2 Downloads & Installation**<br>\n",
    "**2.3 Install & Import libraries**<br>\n",
    "**2.4 Create Web Driver**<br>\n",
    "**2.5 Exploring and locating Elements**<br>\n",
    "**2.6 Extract & Compile the information into python list**<br>\n",
    "**2.7 Save the extracted information to a CSV file**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvUPJgUEQWZo"
   },
   "source": [
    "### 2.1 Introduction of selenium\n",
    "\n",
    "**[Selenium](https://www.selenium.dev/)** is an open-source web-based automation tool. Python language and other languages are used with Selenium for testing as well as web scraping. Here we will use Chrome browser, but you can try on any browser.<br>\n",
    "\n",
    "**Why you should use Selenium?**\n",
    "- Clicking on buttons\n",
    "- Filling forms\n",
    "- Scrolling\n",
    "- Taking a screen-shot\n",
    "- Refreshing the page\n",
    "\n",
    "You can find proper documentation on selenium [here](https://selenium-python.readthedocs.io/)<br>\n",
    "\n",
    "Following methods will help to find elements in a webpage (these methods will return a list):\n",
    "- `find_elements_by_name`\n",
    "- `find_elements_by_xpath`\n",
    "- `find_elements_by_link_text`\n",
    "- `find_elements_by_partial_link_text`\n",
    "- `find_elements_by_tag_name`\n",
    "- `find_elements_by_class_name`\n",
    "- `find_elements_by_css_selector`\n",
    "\n",
    "In this tutorial we will use only `find_elements_by_xpath` and `find_elements_by_tag_name` You can find complete documentation of these methods [here](https://selenium-python.readthedocs.io/locating-elements.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5IXSU3_QWZs"
   },
   "source": [
    "### 2.2 Downloads & Installation \n",
    "\n",
    "Unlike previous section, here we'll have to do some prep work to implement this method. We will need to install Selenium & proper web browser driver<br>\n",
    "\n",
    "If you are using **Google Colab** platform then execute following code to perform Initial installation. This piece of code `'google.colab' in str(get_ipython())` is used to identify the Google Colab platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10715,
     "status": "ok",
     "timestamp": 1646378988645,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "ot7iEH0QQWZs",
    "outputId": "3c1f009e-7d4c-40c4-a402-efe9ec6bb194"
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Google CoLab Installation')\n",
    "    !apt update --quiet\n",
    "    !apt install chromium-chromedriver --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kj9N6faQWZs"
   },
   "source": [
    "To run it on **Locally** you will need **Webdriver for Chrome** in your machine. You can download it from this link https://chromedriver.chromium.org/downloads and just copy the file in the folder where we will create the python file (No need of installation). But make sure that the driver‘s version matches that of the Chrome browser installed on the local machine.\n",
    "\n",
    "## TODO ADD image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGOzu85gQWZs"
   },
   "source": [
    "### 2.3 Install & Import libraries\n",
    "\n",
    "Now lets Install the required libraries. Please note that there are some platform specific libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11007,
     "status": "ok",
     "timestamp": 1646378999643,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "WmDBSb_qQWZt",
    "outputId": "c08191ce-2cdb-484d-db30-244b06646af6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "library Installation\n",
      "Not running on CoLab\n"
     ]
    }
   ],
   "source": [
    "print('library Installation')\n",
    "if 'google.colab' not in str(get_ipython()):\n",
    "    print('Not running on CoLab')\n",
    "    #!pip install webdriver-manager --upgrade --quiet\n",
    "else:\n",
    "    print('Running on CoLab')\n",
    "    \n",
    "!pip install selenium --quiet\n",
    "!pip install pandas --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pn1J-U1PQWZt"
   },
   "source": [
    "Once the Libraries installation is done, next step is to import all the required modules / libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1646378999643,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "PaHNd1hSQWZt",
    "outputId": "e4e64a4f-a939-41c6-877c-e742cc9e167f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library Import\n",
      "Not running on CoLab\n",
      "Common Library Import\n"
     ]
    }
   ],
   "source": [
    "print('Library Import')\n",
    "if 'google.colab' not in str(get_ipython()):\n",
    "    print('Not running on CoLab')\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    #from webdriver_manager.chrome import ChromeDriverManager\n",
    "    import os\n",
    "else:\n",
    "    print('Running on CoLab')\n",
    "    \n",
    "print('Common Library Import')\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlfwVrnVQWZt"
   },
   "source": [
    "So all the necessary prep work is done, lets move ahead to implement this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ms87q9huQWZu"
   },
   "source": [
    "### 2.4 Create Web Driver\n",
    "\n",
    "In this step first we will create the instance of Chrome WebDriver using `webdriver.Chrome()` method. and then the `driver.get()` method will navigate to a page given by the URL. In this case also there is slight variation based on platform, Also passed `options` parameters for e.g. `--headless` option will load the driver in background. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1646378999860,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "cAn7GekCQWZu",
    "outputId": "64982b60-dc1e-45c0-97c4-317f8033ae88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on CoLab\n"
     ]
    }
   ],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Running on CoLab')\n",
    "    def get_driver(url):\n",
    "        \"\"\"Return web driver\"\"\"\n",
    "        colab_options = webdriver.ChromeOptions()\n",
    "        colab_options.add_argument('--no-sandbox')\n",
    "        colab_options.add_argument('--disable-dev-shm-usage')\n",
    "        colab_options.add_argument('--headless')\n",
    "        driver = webdriver.Chrome(options=colab_options)\n",
    "        driver.get(url)\n",
    "        return driver\n",
    "else:\n",
    "    print('Not running on CoLab')\n",
    "    def get_driver(url):\n",
    "        \"\"\"Return web driver\"\"\"\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        chrome_options.add_argument('--headless')\n",
    "        #serv = Service(ChromeDriverManager().install())\n",
    "        serv = Service(os.getcwd()+'/chromedriver')\n",
    "        driver = webdriver.Chrome(options=chrome_options, service=serv)\n",
    "        driver.get(url)\n",
    "        return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnCtyw66QWZu"
   },
   "source": [
    "lets run the function `get_driver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 3839,
     "status": "ok",
     "timestamp": 1646379003695,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "rd7DJYjaQWZu"
   },
   "outputs": [],
   "source": [
    "driver = get_driver('https://finance.yahoo.com/cryptocurrencies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOF6149XQWZu"
   },
   "source": [
    "### 2.5 Exploring and locating Elements\n",
    "\n",
    "This is almost similar step that we have done in phase 1, We will try to identify relevant information like `<tags>`, `class` , `XPath` etc from the web page. So lets do right-click and select the \"Inspect\" to do further analysis.\n",
    "\n",
    "As the webpage is showing cryptocurrency information in the Table form. We can grab the table header by using tag `<th>`, we will use find_elements by TAG to get the table headers. These headers will be used columns for CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 606,
     "status": "ok",
     "timestamp": 1646379004295,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "zR29b9xH8di9",
    "outputId": "a13ded34-92c0-48da-df17-d7d4d90dbd18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbol\n",
      "Price (Intraday)\n"
     ]
    }
   ],
   "source": [
    "header = driver.find_elements(By.TAG_NAME, value= 'th')\n",
    "print(header[0].text)\n",
    "print(header[2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNZ0kwDhQWZv"
   },
   "source": [
    "Now we will create a helper function to get first 10 columns from header, we have used List comprehension with conditions. you can also check out usage of `enumerate` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1646379004296,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "QTfDpiWrQWZv"
   },
   "outputs": [],
   "source": [
    "def get_table_header(driver):\n",
    "    \"\"\"Return Table columns in list form \"\"\"\n",
    "    header = driver.find_elements(By.TAG_NAME, value= 'th')\n",
    "    header_list = [item.text for index, item in enumerate(header) if index < 10]\n",
    "    return header_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqW7UgNwQWZv"
   },
   "source": [
    "Next we find out number of rows available in a Page, you can see table rows are placed in `<tr>` tag, we can capture the `XPath` by selection `<tr>` tag the Right Click -> Copy -> Copy XPath.\n",
    "\n",
    "## TODO IMAGE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlfJ632vQWZv"
   },
   "source": [
    "So we get the  XPath value as `//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr[1]`, lets use this with `find_element()` & `By.XPATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 166,
     "status": "ok",
     "timestamp": 1646379004598,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "U3FVWjftQWZv",
    "outputId": "2d7e6e54-d708-49e1-d9c1-71fdfc210ec3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BTC-USD\\nBitcoin USD 39,044.73 -2,347.13 -5.67% 740.868B 27.59B 27.59B 27.59B 18.975M'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt=driver.find_element(By.XPATH, value='//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr[1]').text\n",
    "txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6JcCdb1Trrp"
   },
   "source": [
    "Above `XPath` points to first row, we can get rid of row number part from XPath and use it with `find_elements` to get hold of all the available rows. Lets implement this in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1646379004599,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "dd9kJccjToHK"
   },
   "outputs": [],
   "source": [
    "def get_table_rows(driver):\n",
    "    \"\"\"Get number of rows available on the page \"\"\"\n",
    "    tablerows = len(driver.find_elements(By.XPATH, value='//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr'))\n",
    "    return tablerows    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 180,
     "status": "ok",
     "timestamp": 1646379004775,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "0v1phXVMRn2E",
    "outputId": "14be94b3-edd5-4bbd-bd3e-471eef2f83d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "print(get_table_rows(driver))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A45Ekufq8di9"
   },
   "source": [
    "Similarly we can take the XPath for any column value.\n",
    "## TODO IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0dHoGoVVzTv"
   },
   "source": [
    "This is the XPAth for a column `//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr[1]/td[2]`.<br>\n",
    "If you noticed the the number after `tr` & `td` represents the `row_number` and `column_number`, lets check this with `find_element()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1646379004775,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "1MNZ9bGilrgt",
    "outputId": "31123efd-4a5e-465d-aa70-17fdf7322e03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bitcoin USD'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.find_element(By.XPATH, value='//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr[1]/td[2]').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2KhgeZAW7d8"
   },
   "source": [
    "So we can change the `row_number` & `column_number` in `XPath` and loop it through row count and column count to get all the available column values. Lets generalize this and put it in a function. We will get the data for one row at a time and return column value in the form of dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1646379004776,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "pEorS2flYikH"
   },
   "outputs": [],
   "source": [
    "def parse_table_rows(rownum, driver, header_list):\n",
    "    \"\"\"get the data for one row at a time and return column value in the form of dictionary\"\"\"\n",
    "    row_dictionary = {}\n",
    "    time.sleep(1/3)\n",
    "    for index , item in enumerate(header_list):\n",
    "        column_xpath = '//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr[{}]/td[{}]'.format(rownum, index+1)\n",
    "        row_dictionary[item] = driver.find_element(By.XPATH, value=column_xpath).text\n",
    "    return row_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96fzN44rXu-u"
   },
   "source": [
    "The Yahoo! Finance web page is showing only 25 Cryptocurrencies per page and user will have click `Next` button to load next sets of crypto currencies. This is called as **Pagination**. This is the main reason we are implementing selenium method to handle the events like pagination. you can perform multiple events like clicking, scrolling , refreshing etc. on a webpage using selenium methods.\n",
    "\n",
    "Now we will grab the `XPath` of `Next` button, find the element using `find_element` method and after that we can perform click action using `.click()` method \n",
    "\n",
    "## TODO IMAGE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 361,
     "status": "ok",
     "timestamp": 1646379005131,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "s45SMjxligLN"
   },
   "outputs": [],
   "source": [
    "button_element = driver.find_element(By.XPATH, value = '//*[@id=\"scr-res-table\"]/div[2]/button[3]')\n",
    "button_element.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osU8bvFYeIgu"
   },
   "source": [
    "In this section we have learned how to get required data points, and perform events on webpage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1646379005327,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "J0CKtbmTf5aF"
   },
   "outputs": [],
   "source": [
    "driver.quit() #terminating driver from test runs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdpxUXQCd5LH"
   },
   "source": [
    "### 2.6 Extract & Compile the information into python list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rL_yJMXSNAD"
   },
   "source": [
    "Lets put all the pieces in the puzzle, we will pass the integer `total_crypto` i.e. numbers of rows to be scraped (in this case 100 rows) in the function. Parse each row from the page and append the data in the `List` till the total parsed row count reach to `total_crypto`. In addition we will perform `Next` button click if we are at the last row of the table. \n",
    "\n",
    "**Please Note** : Here to identify the `Next` button element we have used [WebDriverWait](https://www.selenium.dev/selenium/docs/api/java/org/openqa/selenium/support/ui/WebDriverWait.html) class instead of using `find_element()` method. In this technique we can pass some wait-time before grabbing the element. This type of implememtation is done to avoid the [`StaleElementReferenceException`](https://stackoverflow.com/questions/27003423/staleelementreferenceexception-on-python-selenium).\n",
    "\n",
    "Code Sample:\n",
    "```\n",
    "element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"scr-res-table\"]/div[2]/button[3]')))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1646379005328,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "FgH4XUG9SNAD"
   },
   "outputs": [],
   "source": [
    "def parse_multiple_pages(driver, total_crypto):\n",
    "    \"\"\"Loop through each row, perform Next button click at the end of page \n",
    "    return total_crypto numbers of rows \n",
    "    \"\"\"\n",
    "    table_data = []\n",
    "    page_num = 1\n",
    "    is_scraping = True\n",
    "    header_list = get_table_header(driver)\n",
    "\n",
    "    while is_scraping:\n",
    "        table_rows = get_table_rows(driver)\n",
    "        print('Found {} rows on Page : {}'.format(table_rows, page_num))\n",
    "        print('Parsing Page : {}'.format(page_num))\n",
    "        table_data += [parse_table_rows(i, driver, header_list) for i in range (1, table_rows + 1)]\n",
    "        total_count = len(table_data)\n",
    "        print('Total rows scraped : {}'.format(total_count))\n",
    "        if total_count >= total_crypto:\n",
    "            print('Done Parsing..')\n",
    "            is_scraping = False\n",
    "        else:    \n",
    "            print('Clicking Next Button')\n",
    "            element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"scr-res-table\"]/div[2]/button[3]')))\n",
    "            element.click() \n",
    "            page_num += 1\n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZL49rJqd5aa"
   },
   "source": [
    "### 2.7 Save the extracted information to a CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wG_ZTSEoigLN"
   },
   "source": [
    "This is the last step of this section, we are creating a last function which will be the placeholder for all helper functions and at the and we will save the data in CSV format using `pd.to_csv` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1646379005328,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "a8AxKJLrYikH"
   },
   "outputs": [],
   "source": [
    "def scrape_yahoo_crypto(url, total_crypto, path=None):\n",
    "    \"\"\"Get the list of yahoo finance crypto-currencies and write them to CSV file \"\"\"\n",
    "    if path is None:\n",
    "        path = 'crypto-currencies.csv'\n",
    "    print('Creating driver')\n",
    "    driver = get_driver(url)    \n",
    "    table_data = parse_multiple_pages(driver, total_crypto)\n",
    "    driver.close()\n",
    "    print('Save the data to a CSV')\n",
    "    table_df = pd.DataFrame(table_data)\n",
    "    #print(table_df)\n",
    "    table_df.to_csv(path, index=None)\n",
    "    #This return statement is optional, we are doing this just analyze the final output \n",
    "    return table_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJc4JNBaSNAD"
   },
   "source": [
    "Now its time to scrape some cryoptos!!! , we will scrape top 100 cryptos in Yahoo! Finance webpage by calling `scrape_yahoo_crypto` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1fEtHtL2jXxR",
    "outputId": "410ece35-9b03-4ebf-9626-d50c7b0d7628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating driver\n",
      "Found 25 rows on Page : 1\n",
      "Parsing Page : 1\n",
      "Total rows scraped : 25\n",
      "Clicking Next Button\n",
      "Found 25 rows on Page : 2\n",
      "Parsing Page : 2\n",
      "Total rows scraped : 50\n",
      "Clicking Next Button\n",
      "Found 25 rows on Page : 3\n",
      "Parsing Page : 3\n",
      "Total rows scraped : 75\n",
      "Clicking Next Button\n",
      "Found 25 rows on Page : 4\n",
      "Parsing Page : 4\n",
      "Total rows scraped : 100\n",
      "Done Parsing..\n",
      "Save the data to a CSV\n"
     ]
    }
   ],
   "source": [
    "YAHOO_FINANCE_URL = BASE_URL+'/cryptocurrencies'\n",
    "TOTAL_CRYPTO = 100\n",
    "crypto_df = scrape_yahoo_crypto(YAHOO_FINANCE_URL, TOTAL_CRYPTO,'crypto-currencies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tf7cgOSVYikI"
   },
   "source": [
    "The \"crypto-currencies.csv\" should be available in File --> Open Menu, you can download the file or directly open it on browser. Please verify the file content and compare it with the actual information available on the webpage.\n",
    "\n",
    "You can also check the data by grabbing few rows form the data frame returned by the `scrape_yahoo_crypto` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "4-fahUk4SNAE"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Name</th>\n",
       "      <th>Price (Intraday)</th>\n",
       "      <th>Change</th>\n",
       "      <th>% Change</th>\n",
       "      <th>Market Cap</th>\n",
       "      <th>Volume in Currency (Since 0:00 UTC)</th>\n",
       "      <th>Volume in Currency (24Hr)</th>\n",
       "      <th>Total Volume All Currencies (24Hr)</th>\n",
       "      <th>Circulating Supply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BTC-USD</td>\n",
       "      <td>Bitcoin USD</td>\n",
       "      <td>39,044.73</td>\n",
       "      <td>-2,347.13</td>\n",
       "      <td>-5.67%</td>\n",
       "      <td>740.868B</td>\n",
       "      <td>27.59B</td>\n",
       "      <td>27.59B</td>\n",
       "      <td>27.59B</td>\n",
       "      <td>18.975M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ETH-USD</td>\n",
       "      <td>Ethereum USD</td>\n",
       "      <td>2,628.84</td>\n",
       "      <td>-98.05</td>\n",
       "      <td>-3.60%</td>\n",
       "      <td>315.016B</td>\n",
       "      <td>12.978B</td>\n",
       "      <td>12.978B</td>\n",
       "      <td>12.978B</td>\n",
       "      <td>119.831M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USDT-USD</td>\n",
       "      <td>Tether USD</td>\n",
       "      <td>1.0001</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>-0.01%</td>\n",
       "      <td>79.723B</td>\n",
       "      <td>58.566B</td>\n",
       "      <td>58.566B</td>\n",
       "      <td>58.566B</td>\n",
       "      <td>79.713B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BNB-USD</td>\n",
       "      <td>Binance Coin USD</td>\n",
       "      <td>374.96</td>\n",
       "      <td>-19.91</td>\n",
       "      <td>-5.04%</td>\n",
       "      <td>61.913B</td>\n",
       "      <td>1.727B</td>\n",
       "      <td>1.727B</td>\n",
       "      <td>1.727B</td>\n",
       "      <td>165.117M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USDC-USD</td>\n",
       "      <td>USD Coin USD</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>+0.0007</td>\n",
       "      <td>+0.07%</td>\n",
       "      <td>52.872B</td>\n",
       "      <td>4.434B</td>\n",
       "      <td>4.434B</td>\n",
       "      <td>4.434B</td>\n",
       "      <td>52.872B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Symbol              Name Price (Intraday)     Change % Change Market Cap  \\\n",
       "0   BTC-USD       Bitcoin USD        39,044.73  -2,347.13   -5.67%   740.868B   \n",
       "1   ETH-USD      Ethereum USD         2,628.84     -98.05   -3.60%   315.016B   \n",
       "2  USDT-USD        Tether USD           1.0001    -0.0001   -0.01%    79.723B   \n",
       "3   BNB-USD  Binance Coin USD           374.96     -19.91   -5.04%    61.913B   \n",
       "4  USDC-USD      USD Coin USD           1.0000    +0.0007   +0.07%    52.872B   \n",
       "\n",
       "  Volume in Currency (Since 0:00 UTC) Volume in Currency (24Hr)  \\\n",
       "0                              27.59B                    27.59B   \n",
       "1                             12.978B                   12.978B   \n",
       "2                             58.566B                   58.566B   \n",
       "3                              1.727B                    1.727B   \n",
       "4                              4.434B                    4.434B   \n",
       "\n",
       "  Total Volume All Currencies (24Hr) Circulating Supply  \n",
       "0                             27.59B            18.975M  \n",
       "1                            12.978B           119.831M  \n",
       "2                            58.566B            79.713B  \n",
       "3                             1.727B           165.117M  \n",
       "4                             4.434B            52.872B  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crypto_df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ion8RYbZ8di_"
   },
   "source": [
    "**Summary** : Hope you've enjoyed this tutorial. Selenium enables us to perform multiple actions on the web browser which is really very handy to scrape different type of data from any webpage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-klaoqb8di_"
   },
   "source": [
    "## 3. Scrape Market Events Calendar\n",
    "\n",
    "This is the final segment of the tutorial, in this section we will learn how to extract embedded [JSON](https://www.w3schools.com/js/js_json_intro.asp) formatted data which can be easily converted to Python dictionary. Problem statement for section is to scrape date-wise market events from [Yahoo! finance](https://finance.yahoo.com/calendar)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/bKQoAjs.png)\n",
    "\n",
    "Here's an outline of the steps we'll follow<br>\n",
    "**3.1 Install & Import libraries**<br>\n",
    "**3.2 Download & Parse web page**<br>\n",
    "**3.3 Get Embedded Json data**<br>\n",
    "**3.4 Locating Json Keys**<br>\n",
    "**3.5 Pagination & Compiling the information into python list**<br>\n",
    "**3.6 Save the extracted information to a CSV file**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Install & Import libraries\n",
    "\n",
    "First step to install and import Python Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests --quiet\n",
    "!pip install beautifulsoup4 --quiet\n",
    "!pip install pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "2m7QniEX8di_"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets move ahead with next step "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Download & Parse web page\n",
    "\n",
    "This is exactly same step that we've performed to download webpage in section 1.1 , Here we have used [custom header](https://docs.python-requests.org/en/master/user/quickstart/#custom-headers) in `requests.get()`\n",
    "\n",
    "Most of the things are explained in section 1.1, so lets create the helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "qFhyZG558di_"
   },
   "outputs": [],
   "source": [
    "def get_event_page(scraper_url):\n",
    "    \"\"\"Download a webpage and return a beautiful soup doc\"\"\"\n",
    "    headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
    "                  \"(KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19582\"\n",
    "    }\n",
    "    response = requests.get(scraper_url, headers=headers)\n",
    "    if not response.ok:\n",
    "        print('Status code:', response.status_code)\n",
    "        raise Exception('Failed to fetch web page ' + scraper_url)\n",
    "    # Construct a beautiful soup document\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_event_page('https://finance.yahoo.com/calendar/earnings?from=2022-02-27&to=2022-03-05&day=2022-02-28')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Get Embedded Json data\n",
    "\n",
    "\n",
    "In this step we will locate the Jason formated data, Open the web page and do Right Click --> View Page Source, If you scroll down to source page you will notice the [Json](https://www.w3schools.com/whatis/whatis_json.asp) formated data. Luckily this information is `<script>` tag which contain following text `/* -- Data -- */`.\n",
    "\n",
    "# TODO ADD IMAGE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use [Regular expressions](https://docs.python.org/3/library/re.html) to get text  inside `<script>` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'\\s--\\sData\\s--\\s')\n",
    "script_data = doc.find('script', text=pattern).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further  the `Json` formated string has first key as `context` and it ends at 12 characters from the end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(function (root) {\n",
      "/* -- Data -- */\n",
      "root.App || (root.App = {});\n",
      "root.App.now = 1646462537721;\n",
      "root.App.main = {\"context\":{\"dispatcher\":{\"stores\":{\"P\n",
      "odal\":{\"strings\":1},\"tdv2-wafer-header\":{\"strings\":1},\"yahoodotcom-layout\":{\"strings\":1}}},\"options\":{\"defaultBundle\":\"td-app-finance\"}}}};\n",
      "}(this));\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(script_data[:150])\n",
    "print(script_data[-150:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can grab the Json string using Python slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "start  = script_data.find('context')-2\n",
    "json_text  = script_data[start:-12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use `json.loads()`method to convert Jason string into Python Dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_dictionary = json.loads(json_text)\n",
    "type(parsed_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating function using above information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "rgcsfbSx8di_"
   },
   "outputs": [],
   "source": [
    "def get_json_dictionary(doc):\n",
    "    \"\"\"Get Json formated data in the form of Python Dictionary\"\"\"\n",
    "    pattern = re.compile(r'\\s--\\sData\\s--\\s')\n",
    "    script_data = doc.find('script', text=pattern).text\n",
    "    \n",
    "    start  = script_data.find('context')-2\n",
    "    json_text  = script_data[start:-12]\n",
    "    \n",
    "    parsed_dictionary = json.loads(json_text)\n",
    "    return parsed_dictionary    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Locating Json Keys\n",
    "\n",
    "So basically the Json text is multi level nested dictionaries, and some keys are used to store all the meta data displayed on the webpage. In this section we will identify the keys for the data we are trying to scrape.\n",
    "\n",
    "We'll need some `Json Formatter` tool to navigate through multiple keys, I am using online tool https://jsonblob.com/, However you can choose any tool.\n",
    "\n",
    "We will write the Json text into `my_json_file.json` file, then grab the file content and paste it to the left panel of https://jsonblob.com/. The JSON Blob it will do nice formatting, we can easily navigate through each Keys and search any item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "SUWFo_MG8di_"
   },
   "outputs": [],
   "source": [
    "with open('my_json_file.json', 'w') as file:\n",
    "    file.write(json_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now next step is to find the Required Key location, Lets search the company name `3D Systems Corporation` displayed in the webpage in the [JSON Blob](https://jsonblob.com/) formatter.\n",
    "![](https://i.imgur.com/Iv8b7vl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/jpJYOCy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the table data is stored in the `rows` key, and we can track down the parent keys as shown in the above screen, lets checkout the content of `row` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ticker': 'DDD',\n",
       "  'companyshortname': '3D Systems Corporation',\n",
       "  'startdatetime': '2022-02-28T16:05:00.000Z',\n",
       "  'startdatetimetype': 'TAS',\n",
       "  'epsestimate': 0.03,\n",
       "  'epsactual': 0.09,\n",
       "  'epssurprisepct': 181.25,\n",
       "  'timeZoneShortName': 'EST',\n",
       "  'gmtOffsetMilliSeconds': -18000000,\n",
       "  'quoteType': 'EQUITY'},\n",
       " {'ticker': 'AIABF',\n",
       "  'companyshortname': 'Capital A Berhad',\n",
       "  'startdatetime': '2022-02-28T08:28:00.000Z',\n",
       "  'startdatetimetype': 'TAS',\n",
       "  'epsestimate': -0.28,\n",
       "  'epsactual': -0.22,\n",
       "  'epssurprisepct': 21.58,\n",
       "  'timeZoneShortName': 'EST',\n",
       "  'gmtOffsetMilliSeconds': -18000000,\n",
       "  'quoteType': 'EQUITY'},\n",
       " {'ticker': 'ATRA',\n",
       "  'companyshortname': 'Atara Biotherapeutics, Inc.',\n",
       "  'startdatetime': '2022-02-28T16:05:00.000Z',\n",
       "  'startdatetimetype': 'TAS',\n",
       "  'epsestimate': -0.69,\n",
       "  'epsactual': -0.96,\n",
       "  'epssurprisepct': -39.94,\n",
       "  'timeZoneShortName': 'EST',\n",
       "  'gmtOffsetMilliSeconds': -18000000,\n",
       "  'quoteType': 'EQUITY'}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_dictionary['context']['dispatcher']['stores']['ScreenerResultsStore']['results']['rows'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows on the Current page : 100\n"
     ]
    }
   ],
   "source": [
    "print('Total Rows on the Current page :',len(parsed_dictionary['context']['dispatcher']['stores']['ScreenerResultsStore']['results']['rows']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sub-dictionary is showing all the data displayed on current page.<br>\n",
    "You can do more research and exploration to get different information from the web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows for the search criteria : 216\n"
     ]
    }
   ],
   "source": [
    "print('Total Rows for the search criteria :',parsed_dictionary['context']['dispatcher']['stores']['ScreenerResultsStore']['results']['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'data': 'ticker', 'content': 'Symbol'},\n",
       " {'data': 'companyshortname', 'content': 'Company Name'},\n",
       " {'data': 'startdatetime', 'content': 'Event Start Date'},\n",
       " {'data': 'startdatetimetype', 'content': 'Event Start Time'},\n",
       " {'data': 'epsestimate', 'content': 'EPS Estimate'},\n",
       " {'data': 'epsactual', 'content': 'Reported EPS'},\n",
       " {'data': 'epssurprisepct', 'content': 'Surprise (%)'},\n",
       " {'data': 'timeZoneShortName', 'content': 'Timezone short name'},\n",
       " {'data': 'gmtOffsetMilliSeconds', 'content': 'GMT Offset'}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Columns\")\n",
    "parsed_dictionary['context']['dispatcher']['stores']['ScreenerResultsStore']['results']['columns']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Create the functions using above information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "wyX1dtPy8di_"
   },
   "outputs": [],
   "source": [
    "def get_total_rows(parsed_dictionary):\n",
    "    '''Get the Total Rows for the search criteria & Columns detail''' \n",
    "    total_rows = parsed_dictionary['context']['dispatcher']['stores']['ScreenerResultsStore']['results']['total']\n",
    "    return total_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "435fS9Vv8di_"
   },
   "outputs": [],
   "source": [
    "def get_page_rows(parsed_dictionary):\n",
    "    \"\"\"Get the Content current page\"\"\"    \n",
    "    data_dictionary = parsed_dictionary['context']['dispatcher']['stores']['ScreenerResultsStore']['results']['rows']\n",
    "    return data_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Pagination & Compiling the information into python list\n",
    "\n",
    "As we saw in the previous section how to handle `Pagination` using selenium methods, here we'll learn new technique of accessing multiple pages.<br>\n",
    "\n",
    "Most of the times webpage url gets changed runtime depending on the user selection, e.g. In below screen-shot I selected the **Earnings** for **1-March-2022**. You can notice how that information is passed in the url. \n",
    "![](https://i.imgur.com/h5QU99h.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, when i click next button `offset`& `size` values gets changed in the url.\n",
    "![](https://i.imgur.com/jYa1vq5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can figure out the pattern & structure of the url and how it affects the page navigation.<br> \n",
    "\n",
    "In this case webpage url pattern is mentioned below:<br>\n",
    "- Following values are used for calendar event type \n",
    "`event_types = ['splits','economic','ipo','earnings']`\n",
    "- Date is passed in `yyyy-mm-dd` format\n",
    "- Page number is controlled by `offset` value (for first page `offset=0`)\n",
    "- Maximum numbers of rows in a page is assigned to `size`\n",
    "\n",
    "Based on the above information we can build url runtime and download the page then extract the information, this is how we handle the pagination.<br>\n",
    "\n",
    "Putting all things together lets create a function. In this function we will pass `event_type` and `date`, then we will calculate the total rows for matching criteria using `get_columns_and_total_rows` function. Maximum rows per page is constant (i.e 100), so we can build iterating summation logic to calculate total number of pages involved for current criteria and extract each page data in the loop.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "3magUBX48di_"
   },
   "outputs": [],
   "source": [
    "def scrape_all_pages(event_type, date):\n",
    "    \"\"\"Loop through each row and return lists of data dictiionary\"\"\"\n",
    "    YAHOO_CAL_URL = BASE_URL+'/calendar/{}?day={}&offset={}&size={}'\n",
    "    max_rows_per_page = '100' # this indicates max rows per page \n",
    "    page_number = 1\n",
    "    final_data_dictionary = []\n",
    "    \n",
    "    while page_number > 0:\n",
    "        print(\"Pricessing page # {}\".format(page_number))\n",
    "        page_url = str((page_number - 1 ) * int(max_rows_per_page))\n",
    "        scrape_url = YAHOO_CAL_URL.format(event_type, date, page_url, max_rows_per_page)\n",
    "        print(\"Scrape url for page {} is {}\".format(page_number,scrape_url))\n",
    "        page_doc = get_event_page(scrape_url)\n",
    "        parse_dict = get_json_dictionary(page_doc)\n",
    "        if page_number == 1:\n",
    "            total_rows = get_total_rows(parse_dict)        \n",
    "        final_data_dictionary += get_page_rows(parse_dict)\n",
    "        if len(final_data_dictionary) >= total_rows:\n",
    "            page_number = 0\n",
    "            return final_data_dictionary\n",
    "        page_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Save the extracted information to a CSV file\n",
    "\n",
    "In this last section, we will save the data to csv format using `pd.DataFrame()` & `to_csv()` and call everything in a single placeholder function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "VLfoy_33QWZz"
   },
   "outputs": [],
   "source": [
    "def scrape_yahoo_calendar(event_types, date_param):\n",
    "    \"\"\"Get the list of yahoo finance calendar and write them to CSV file \"\"\"\n",
    "    for event in event_types:\n",
    "        print('Web Scraping for ', event  )\n",
    "        data_dict = scrape_all_pages(event, date_param)\n",
    "        scraped_df = pd.DataFrame(data_dict)\n",
    "        scraped_df.to_csv(event+'_'+date_param+'.csv',index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calling final function `scrape_yahoo_calendar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "WpvYTWyx8djA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web Scraping for  splits\n",
      "Pricessing page # 1\n",
      "Scrape url for page 1 is https://finance.yahoo.com/calendar/splits?day=2022-02-28&offset=0&size=100\n",
      "Web Scraping for  economic\n",
      "Pricessing page # 1\n",
      "Scrape url for page 1 is https://finance.yahoo.com/calendar/economic?day=2022-02-28&offset=0&size=100\n",
      "Web Scraping for  ipo\n",
      "Pricessing page # 1\n",
      "Scrape url for page 1 is https://finance.yahoo.com/calendar/ipo?day=2022-02-28&offset=0&size=100\n",
      "Web Scraping for  earnings\n",
      "Pricessing page # 1\n",
      "Scrape url for page 1 is https://finance.yahoo.com/calendar/earnings?day=2022-02-28&offset=0&size=100\n",
      "Pricessing page # 2\n",
      "Scrape url for page 2 is https://finance.yahoo.com/calendar/earnings?day=2022-02-28&offset=100&size=100\n",
      "Pricessing page # 3\n",
      "Scrape url for page 3 is https://finance.yahoo.com/calendar/earnings?day=2022-02-28&offset=200&size=100\n"
     ]
    }
   ],
   "source": [
    "BASE_URL = 'https://finance.yahoo.com' #Global Variable \n",
    "date_param = '2022-02-28'\n",
    "\n",
    "event_types = ['splits','economic','ipo','earnings']\n",
    "scrape_yahoo_calendar(event_types, date_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total 4 csv files \"event_type_yyyy-mm-dd.csv\" should be available in File --> Open Menu, you can download the file or directly open it on browser. Please verify the file content and compare it with the actual information available on the webpage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpW06QKr8djA"
   },
   "source": [
    "**Summary** : This is very useful technique which can be easily replicable. Without writing any customized code we were able to extract the data from multiple types of web pages just by changing one variable (in this case `event_type`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "References to links you found useful\n",
    "\n",
    "- https://htmldog.com/guides/html/\n",
    "- https://selenium-python.readthedocs.io/index.html\n",
    "- https://stackoverflow.com/questions/27003423/staleelementreferenceexception-on-python-selenium\n",
    "- https://www.w3schools.com/js/js_json_intro.asp\n",
    "- https://hhsm95.dev/blog/the-importance-of-using-user-agent-to-scraping-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "Ideas for future work<br>\n",
    "- Automate this process using [AWS Lambda](https://aws.amazon.com/lambda/) to download daily market calendar, crypto-currencies & market news in CSV format.\n",
    "- Move the old files to an Archive folder append date-stamp to the file if required also  delete the Archived files older than 2 weeks.\n",
    "- Process the raw data extracted from third technique using different methods of pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this Tutorial we Implement following web scraping techniques.\n",
    " - Using requests, BeautifulSoup and HTML tags to extract web page.\n",
    " - Using Selenium to scrape data from dynamically loading websites.\n",
    " - Using embedded Json format data to scrape website .\n",
    "\n",
    "I hope I was able to teach you these webscraping methods and I hope you can use this knowledge to scrape any website.\n",
    "\n",
    "Thank you for reading. Happy coding!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "7aSVLdJ1lrgw"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"vinodvidhole/yahoo-finance-web-scraper\" on https://jovian.ai/\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/vinodvidhole/yahoo-finance-web-scraper\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/vinodvidhole/yahoo-finance-web-scraper'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project=\"yahoo-finance-web-scraper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJ6o34es8djA"
   },
   "source": [
    "- check lets , now , its & time word\n",
    "- check grammar & double click each markdown to check spell check \n",
    "- images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "DvSMJZeClrgo"
   ],
   "name": "yahoo-finance-web-scraper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
