{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pF8beWv0lrgc"
   },
   "source": [
    "# Web Scraping Yahoo! Finance using Python\n",
    "\n",
    "A detailed guide for web scraping https://finance.yahoo.com/ using **requests**, **BeautifulSoup**, **Selenium**, **HTML tags** & embedded **JSON** data.\n",
    "\n",
    "![](https://imgur.com/7jMFOcE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FULGhEjlrge"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "**What is Web scraping?**<br>\n",
    "Web scraping is the process of extracting and parsing data from websites in an automated fashion using a computer program. It's a useful technique for creating datasets for research and learning.\n",
    "\n",
    "\n",
    "**Objective**<br>\n",
    "The main objective of this tutorial is to showcase different web scraping methods which can be applied to any web page. \n",
    "This is for educational purposes only. Please read the Terms & Conditions carefully for any website to see whether you can legally use the data. \n",
    "\n",
    "In this project, we will perform web scraping using the following 3 techniques based on the problem statement.\n",
    "* use `requests`, `BeautifulSoup` and `HTML tags` to extract web page\n",
    "* use `Selenium` to scrape data from dynamically loading websites \n",
    "* use embedded `JSON` data to scrape website "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CFXNNENlrgf"
   },
   "source": [
    "**The problem statement**<br>\n",
    "1. Scrape **Stock Market News** (url : https://finance.yahoo.com/topic/stock-market-news/) :<br>\n",
    "    This web page shows the latest **news** related to **stock market**, we will try to extract data from this web page and store it in a `CSV` (comma-separated values) file. The file layout would be as mentioned below.\n",
    "    ```\n",
    "    source,headline,url,content,image\n",
    "    <source of the news>,<news head line>,<news url>,<news content>,<news thumbnail image>\n",
    "    ```\n",
    "\n",
    "2. Scrape **Cryptocurrencies** (url : https://finance.yahoo.com/cryptocurrencies) :<br>\n",
    "    This Yahoo! finance web page shows list of trending **Cryptocurrencies** in tabular format, we will perform the web scraping to retrieve first 10 columns for top 100 **Cryptocurrencies** in `CSV` format.\n",
    "    ```\n",
    "    Symbol,Name,Price (Intraday),Change,% Change,Market Cap,Volume in Currency (Since 0:00 UTC),\n",
    "    Volume in Currency (24Hr),Total Volume All Currencies (24Hr),Circulating Supply\n",
    "    BTC-USD,Bitcoin USD,\"43,312.13\",-947.50,-2.14%,821.76B,27.727B,27.727B,27.727B,18.973M\n",
    "\n",
    "    ```\n",
    "        \n",
    "3. Scrape **Market Events Calendar** (url : https://finance.yahoo.com/calendar) :<br> \n",
    "    This page shows **date-wise market events**, user have the option to select the date and choose any one of the following market events **Earnings**, **Stock Splits**, **Economic Events** & **IPO**. Our aim is to create a script which can be run for any single date and market event which grabs the data and loads in `CSV` format.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Z9UQZf8lrgg"
   },
   "source": [
    "**Prerequisites**\n",
    "* Knowledge of Python\n",
    "* Basic knowledge of HTML although it is not necessary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbMRHIhblrgh"
   },
   "source": [
    "**How to run the Code**<br>\n",
    "You can execute the code using \"Run\" button on the top of this page and selecting **\"Run on Colab\"** or **\"Run Locally\"** \n",
    "<br>\n",
    "<br>\n",
    "**Setup and Tools**<br>\n",
    "<u>Run on Colab :</u> \n",
    "    You will need to provide the Google login to run this notebook on Colab.<br>\n",
    "<u>Run Locally :</u> Download and install [Anaconda](https://www.anaconda.com/) framework, We will be using Jupyter Notebook for writing & executing code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YjeIdedlrgh"
   },
   "source": [
    "**Code Re-usability & Version control**\n",
    "\n",
    "You can make changes and save your version of the notebook to [Jovian](https://jovian.ai/) by executing following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4378,
     "status": "ok",
     "timestamp": 1646465762342,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "iLLVpn2olrgi"
   },
   "outputs": [],
   "source": [
    "!pip install jovian --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1646465762343,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "iTKJFEYglrgj"
   },
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "executionInfo": {
     "elapsed": 5664,
     "status": "ok",
     "timestamp": 1646465767989,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "AxYOBC1xlrgj",
    "outputId": "b3d74e68-57f8-4502-9fdc-e858c5219249"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"vinodvidhole/yahoo-finance-web-scraper\" on https://jovian.ai/\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/vinodvidhole/yahoo-finance-web-scraper\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/vinodvidhole/yahoo-finance-web-scraper'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute this to save new versions of the notebook\n",
    "jovian.commit(project=\"yahoo-finance-web-scraper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SWl2eoMlrgk"
   },
   "source": [
    "## 1. Scrape Stock Market News\n",
    "\n",
    "In this section we will learn basic Python web scraping technique using `requests`, `BeautifulSoup` and `HTML tags`. The objective here is to perform web scraping of [Yahoo! finance Stock Market News](https://finance.yahoo.com/topic/stock-market-news/)\n",
    "\n",
    "![](https://i.imgur.com/1I0Btau.jpg)\n",
    "\n",
    "Let's kick start with the first objective. Here's an outline of the steps we'll follow<br>\n",
    "**1.1 Download & Parse web page using `requests` and `BeautifulSoup`**<br>\n",
    "**1.2 Exploring and locating Elements**<br>\n",
    "**1.3 Extract & Compile the information into python list**<br>\n",
    "**1.4 Save the extracted information to a CSV file**<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSis_3UJlrgl"
   },
   "source": [
    "### 1.1 Download & Parse webpage using requests and BeautifulSoup\n",
    "\n",
    "First step is to install [`requests`](https://docs.python-requests.org/en/latest/) & [`beautifulsoup4`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) Libraries using `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 11426,
     "status": "ok",
     "timestamp": 1646465779408,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "zXJKsNyQlrgl"
   },
   "outputs": [],
   "source": [
    "!pip install requests --quiet\n",
    "!pip install beautifulsoup4 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 209,
     "status": "ok",
     "timestamp": 1646465779595,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "L_gdMI8llrgm"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1d4YUeDlrgm"
   },
   "source": [
    "The libraries are installed and imported.<br>\n",
    "\n",
    "To download the page, we can use `requests.get`, which returns a response object. the HTML information of web page is captured in `response.text`.<br>\n",
    "`response.ok` & [`response.status_code`](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status) can be used for error trapping &  tracking.<br> \n",
    "Finally we can use `BeautifulSoup` to parse the HTML data, this will return `bs4.BeautifulSoup` object. \n",
    "\n",
    "We can create a function to perform this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1646465779596,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "-FIURHiqlrgm"
   },
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    \"\"\"Download a webpage and return a beautiful soup doc\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if not response.ok:\n",
    "        print('Status code:', response.status_code)\n",
    "        raise Exception('Failed to load page {}'.format(url))\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "susW06Hwlrgn"
   },
   "source": [
    "calling function `get_page` and analyze the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1646465779972,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "cGWBiRCJlrgn"
   },
   "outputs": [],
   "source": [
    "my_url = 'https://finance.yahoo.com/topic/stock-market-news/' \n",
    "doc = get_page(my_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1646465779973,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "jvwJU_ULlrgn",
    "outputId": "034e362c-9c49-4eb4-ee94-f9e2632dd0b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of doc:  <class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "print('Type of doc: ',type(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqEwtKgTlrgn"
   },
   "source": [
    "You can access different properties of HTML web page from doc, following example will display Title of the web page.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1646465779973,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "S0xvsoyelrgo",
    "outputId": "9a2bcbe6-ac69-4f9e-e02f-bfc69195de19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Latest Stock Market News</title>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.find('title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hbs8VJS9lrgo"
   },
   "source": [
    "We can use the function `get_page` to download any web page and parse it using beautiful soup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvSMJZeClrgo"
   },
   "source": [
    "### 1.2 Exploring and locating Elements\n",
    "Now its time to explore the elements to find the required data point from the web page. Web pages are written in a language called HTML (Hyper Text Markup Language).  HTML is a fairly simple language comprised of *tags*  (also called *nodes* or *elements*) e.g. `<a href=\"https://finance.yahoo.com/\" target=\"_blank\">Go to Yahoo! Finance</a>`. An HTML tag has three parts:\n",
    "\n",
    "\n",
    "\n",
    "1. **Name**: (`html`, `head`, `body`, `div`, etc.) Indicates what the tag represents and how a browser should interpret the information inside it.\n",
    "2. **Attributes**: (`href`, `target`, `class`, `id`, etc.) Properties of tag used by the browser to customize how a tag is displayed and decide what happens on user interactions.\n",
    "3. **Children**: A tag can contain some text or other tags or both between the opening and closing segments, e.g., `<div>Some content</div>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAFbXWN2lrgo"
   },
   "source": [
    "Let's inspect the webpage source code by right-click and select the \"Inspect\" option. First we need to identify the tag which represents the news listing.\n",
    "\n",
    "![](https://i.imgur.com/pGwXU1J.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fr5ofFpFlrgo"
   },
   "source": [
    "In this case we can see the `<div>` tag having class name `\"Ov(h) Pend(44px) Pstart(25px)\"` is representing news listing, we can apply `find_all` method to grab this information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1646465780155,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "WF_cAVptlrgo"
   },
   "outputs": [],
   "source": [
    "div_tags = doc.find_all('div', {'class': \"Ov(h) Pend(44px) Pstart(25px)\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_Y4L3Kglrgp"
   },
   "source": [
    "Total elements in the `<div>` tag list is matching with the numbers of news displaying in the webpage , so we are heading towards right direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1646465780156,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "FniLi1nElrgp",
    "outputId": "6b637bb7-e311-4f80-b5e0-b36d2bfb76dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(div_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4acfugiWlrgp"
   },
   "source": [
    "Next step to inspect the individual `<div>` tag and try to find more information. I am using \"Visual Studio Code\", but you can use any tool as simple as notepad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1646465780156,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "paFm23Amlrgp",
    "outputId": "1257bbd3-a5aa-4e1d-c2ce-00c246a65fb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"Ov(h) Pend(44px) Pstart(25px)\"><div class=\"C(#959595) Fz(11px) D(ib) Mb(6px)\">TheStreet.com</div><h3 class=\"Mb(5px)\"><a class=\"js-content-viewer wafer-caas Fw(b) Fz(18px) Lh(23px) LineClamp(2,46px) Fz(17px)--sm1024 Lh(19px)--sm1024 LineClamp(2,38px)--sm1024 mega-item-header-link Td(n) C(#0078ff):h C(#000) LineClamp(2,46px) LineClamp(2,38px)--sm1024 not-isInStreamVideoEnabled\" data-uuid=\"c0f061d5-dfab-3cf1-8bb7-1a2b8fc88694\" data-wf-caas-prefetch=\"1\" data-wf-caas-uuid=\"c0f061d5-dfab-3cf1-8bb7-1a2b8fc88694\" href=\"/m/c0f061d5-dfab-3cf1-8bb7-1a2b8fc88694/the-stock-market-has-no-mercy.html\"><u class=\"StretchedBox\"></u>The Stock Market Has No Mercy For Tesla Rivals</a></h3><p class=\"Fz(14px) Lh(19px) Fz(13px)--sm1024 Lh(17px)--sm1024 LineClamp(2,38px) LineClamp(2,34px)--sm1024 M(0)\">The love at first sight between investors and young manufacturers of electric vehicles seems to have died.  As for electric and hydrogen truck maker Nikola, its stock has lost at least 33.6% of its market value compared to its IPO price of $10.</p></div>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_tags[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaY4rqHxlrgp"
   },
   "source": [
    "![](https://i.imgur.com/ncnfg0z.png)\n",
    "\n",
    "Luckily most of the required data points are available in this `<div>`, so we can use `find` method to grab each items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1646465780156,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "Hn02jUnUlrgp",
    "outputId": "57b856bb-2719-4878-926f-3038d2cfd6bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:  TheStreet.com\n",
      "Head Line : The Stock Market Has No Mercy For Tesla Rivals\n"
     ]
    }
   ],
   "source": [
    "print(\"Source: \", div_tags[1].find('div').text)\n",
    "print(\"Head Line : {}\".format(div_tags[1].find('a').text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54ry5o3-lrgp"
   },
   "source": [
    "If any tag is not accessible directly, then you can use methods like `findParent()` or `'findChild()` to point to the required tag.\n",
    "\n",
    "![](https://i.imgur.com/OnOAtT2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1646465780157,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "BYm7cqy2lrgp",
    "outputId": "4b7ff0cf-ff97-4103-9893-c05105bb8217"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image URL:  https://s.yimg.com/uu/api/res/1.2/SO2ugXDssbG6ktlpzC3mOg--~B/Zmk9c3RyaW07aD0xMjM7cT04MDt3PTIyMDthcHBpZD15dGFjaHlvbg--/https://s.yimg.com/uu/api/res/1.2/cP.5sgYHwRi1sxoeGT3WDw--~B/aD0xMDgwO3c9MTkyMDthcHBpZD15dGFjaHlvbg--/https://media.zenfs.com/en/thestreet.com/e6d13390cbdb46512b13cca1155bfe67.cf.jpg\n"
     ]
    }
   ],
   "source": [
    "print(\"Image URL: \",div_tags[1].findParent().find('img')['src'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_rZ3hP_lrgq"
   },
   "source": [
    "Key Takeout from this exercise is to identify the optimal tag which will provide us required information. Mostly this is straight forward, but sometimes you will have to perform little more research.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylJlxrEhlrgq"
   },
   "source": [
    "### 1.3 Extract & Compile the information into python list\n",
    "\n",
    "We've identified all required tags and information, Let's put this together in the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1646465780157,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "Ck_Buvi8lrgr"
   },
   "outputs": [],
   "source": [
    "def get_news_tags(doc):\n",
    "    \"\"\"Get the list of tags containing news information\"\"\"\n",
    "    news_class = \"Ov(h) Pend(44px) Pstart(25px)\" ## class name of div tag \n",
    "    news_list  = doc.find_all('div', {'class': news_class})\n",
    "    return news_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u93jXLLslrgr"
   },
   "source": [
    "sample run of the function `get_news_tags`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1646465780157,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "byS55R-9lrgr"
   },
   "outputs": [],
   "source": [
    "my_news_tags = get_news_tags(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfXnnMASlrgr"
   },
   "source": [
    "we will create one more function, to parse individual `<div>` tag and return the information in dictionary form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1646465780310,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "bsuD1-O4lrgr"
   },
   "outputs": [],
   "source": [
    "BASE_URL = 'https://finance.yahoo.com' #Global Variable \n",
    "\n",
    "def parse_news(news_tag):\n",
    "    \"\"\"Get the news data point and return dictionary\"\"\"\n",
    "    news_source = news_tag.find('div').text #source\n",
    "    news_headline = news_tag.find('a').text #heading\n",
    "    news_url = news_tag.find('a')['href'] #link\n",
    "    news_content = news_tag.find('p').text #content\n",
    "    news_image = news_tag.findParent().find('img')['src'] #thumb image\n",
    "    return { 'source' : news_source,\n",
    "            'headline' : news_headline,\n",
    "            'url' : BASE_URL + news_url,\n",
    "            'content' : news_content,\n",
    "            'image' : news_image\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q88hBshllrgr"
   },
   "source": [
    "Testing the `parse_news` function for first `<div>` tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1646465780310,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "7gO3YYf2lrgr",
    "outputId": "f0299a31-88b1-42af-cdc8-1116f392601e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'TheStreet.com',\n",
       " 'headline': \"Etoro Wants to Make Amends After Liquidating Its clients' Russian Stocks\",\n",
       " 'url': 'https://finance.yahoo.com/m/bc18874c-fafb-3752-80eb-4730cc90aae5/etoro-wants-to-make-amends.html',\n",
       " 'content': 'The trading platform took an unprecedented action without warning its customers who do not hide their anger and frustration.',\n",
       " 'image': 'https://s.yimg.com/uu/api/res/1.2/kx.1v0quv6udevKA.nNY6w--~B/Zmk9c3RyaW07aD0xMjM7cT04MDt3PTIyMDthcHBpZD15dGFjaHlvbg--/https://s.yimg.com/uu/api/res/1.2/fCDt2jRhF0BKJ59MX.AHNw--~B/aD0xMDgwO3c9MTkyMDthcHBpZD15dGFjaHlvbg--/https://media.zenfs.com/en/thestreet.com/1e9bd7b19084e252e958c5108068ef2e.cf.jpg'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_news(my_news_tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4Jt8PhXlrgr"
   },
   "source": [
    "We can use the `get_news_tags` & `parse_news` functions to pars news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ruE_5Q2lrgs"
   },
   "source": [
    "### 1.4 Save the extracted information to a CSV file\n",
    "\n",
    "This is the last step of this section, We are going to use Python library [`pandas`](https://pandas.pydata.org/docs/) to save the data in CSV format. Install and then Import the pandas Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 6458,
     "status": "ok",
     "timestamp": 1646465786765,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "fFiWEPAalrgs"
   },
   "outputs": [],
   "source": [
    "!pip install pandas --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 563,
     "status": "ok",
     "timestamp": 1646465787322,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "zMrbCInvlrgs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JoZJq6GTlrgs"
   },
   "source": [
    "Creating wrapper function which will call previously created helper functions.<br>\n",
    "\n",
    "The `get_page` function will download HTML page,then we can pass the result in `get_news_tags` to identify list of `<div>` tags for news.<br>\n",
    "After that we will use [List Comprehension](https://www.w3schools.com/python/python_lists_comprehension.asp) technique to pars each `<div>` tag using `parse_news`, the output will be in the form of `lists` of `dictionaries`<br>\n",
    "Finally we will use `DataFrame` method to create pandas [dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) and use `to_csv` method to store required data in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1646465787323,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "8evbvhJklrgs"
   },
   "outputs": [],
   "source": [
    "def scrape_yahoo_news(url, path=None):\n",
    "    \"\"\"Get the yahoo finance market news and write them to CSV file \"\"\"\n",
    "    if path is None:\n",
    "        path = 'stock-market-news.csv'\n",
    "        \n",
    "    print('Requesting html page')\n",
    "    doc = get_page(url)\n",
    "\n",
    "    print('Extracting news tags')\n",
    "    news_list = get_news_tags(doc)\n",
    "\n",
    "    print('Parsing news tags')\n",
    "    news_data = [parse_news(news_tag) for news_tag in news_list]\n",
    "\n",
    "    print('Save the data to a CSV')\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    news_df.to_csv(path, index=None)\n",
    "    \n",
    "    #This return statement is optional, we are doing this just analyze the final output \n",
    "    return news_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGYzOftflrgs"
   },
   "source": [
    "Scraping the news using `scrape_yahoo_news` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 187,
     "status": "ok",
     "timestamp": 1646465787507,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "ZhSeuZ6Pi8MD",
    "outputId": "d68cf5d5-0c20-490e-f905-7ffd961bc65e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting html page\n",
      "Extracting news tags\n",
      "Parsing news tags\n",
      "Save the data to a CSV\n"
     ]
    }
   ],
   "source": [
    "YAHOO_NEWS_URL = BASE_URL+'/topic/stock-market-news/'\n",
    "news_df = scrape_yahoo_news(YAHOO_NEWS_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBD6nB50lrgs"
   },
   "source": [
    "The \"stock-market-news.csv\" should be available in File $\\rightarrow$ Open Menu, you can download the file or directly open it on browser. Please verify the file content and compare it with the actual information available on the webpage.\n",
    "\n",
    "You can also check the data by grabbing few rows from the data frame returned by the `scrape_yahoo_news` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "executionInfo": {
     "elapsed": 191,
     "status": "ok",
     "timestamp": 1646465787696,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "1segLgXIlrgs",
    "outputId": "74c2bed2-b3c6-499e-d316-31b03a92a00c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>headline</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TheStreet.com</td>\n",
       "      <td>Etoro Wants to Make Amends After Liquidating I...</td>\n",
       "      <td>https://finance.yahoo.com/m/bc18874c-fafb-3752...</td>\n",
       "      <td>The trading platform took an unprecedented act...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/kx.1v0quv6ud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TheStreet.com</td>\n",
       "      <td>The Stock Market Has No Mercy For Tesla Rivals</td>\n",
       "      <td>https://finance.yahoo.com/m/c0f061d5-dfab-3cf1...</td>\n",
       "      <td>The love at first sight between investors and ...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/SO2ugXDssbG6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Investor's Business Daily</td>\n",
       "      <td>Dow Jones Futures: Market Correction Heading F...</td>\n",
       "      <td>https://finance.yahoo.com/m/2df44452-c887-3bb3...</td>\n",
       "      <td>The major indexes are nearing February lows as...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/ZZL4kX1McSx_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>China Is Hidden Risk for Emerging Markets That...</td>\n",
       "      <td>https://finance.yahoo.com/news/china-hidden-ri...</td>\n",
       "      <td>(Bloomberg) -- As traders grapple with the bre...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/h4T0Z237N9bc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>After Whiplash Week in Markets, Traders Prep f...</td>\n",
       "      <td>https://finance.yahoo.com/news/whiplash-week-m...</td>\n",
       "      <td>(Bloomberg) -- Traders around the world are ge...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/LcnI7yBSK_1c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      source  \\\n",
       "0              TheStreet.com   \n",
       "1              TheStreet.com   \n",
       "2  Investor's Business Daily   \n",
       "3                  Bloomberg   \n",
       "4                  Bloomberg   \n",
       "\n",
       "                                            headline  \\\n",
       "0  Etoro Wants to Make Amends After Liquidating I...   \n",
       "1     The Stock Market Has No Mercy For Tesla Rivals   \n",
       "2  Dow Jones Futures: Market Correction Heading F...   \n",
       "3  China Is Hidden Risk for Emerging Markets That...   \n",
       "4  After Whiplash Week in Markets, Traders Prep f...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://finance.yahoo.com/m/bc18874c-fafb-3752...   \n",
       "1  https://finance.yahoo.com/m/c0f061d5-dfab-3cf1...   \n",
       "2  https://finance.yahoo.com/m/2df44452-c887-3bb3...   \n",
       "3  https://finance.yahoo.com/news/china-hidden-ri...   \n",
       "4  https://finance.yahoo.com/news/whiplash-week-m...   \n",
       "\n",
       "                                             content  \\\n",
       "0  The trading platform took an unprecedented act...   \n",
       "1  The love at first sight between investors and ...   \n",
       "2  The major indexes are nearing February lows as...   \n",
       "3  (Bloomberg) -- As traders grapple with the bre...   \n",
       "4  (Bloomberg) -- Traders around the world are ge...   \n",
       "\n",
       "                                               image  \n",
       "0  https://s.yimg.com/uu/api/res/1.2/kx.1v0quv6ud...  \n",
       "1  https://s.yimg.com/uu/api/res/1.2/SO2ugXDssbG6...  \n",
       "2  https://s.yimg.com/uu/api/res/1.2/ZZL4kX1McSx_...  \n",
       "3  https://s.yimg.com/uu/api/res/1.2/h4T0Z237N9bc...  \n",
       "4  https://s.yimg.com/uu/api/res/1.2/LcnI7yBSK_1c...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptBKMG_Alrgt"
   },
   "source": [
    "**Summary** : Hopefully I was able to explain this simple but very powerful Python technique to scrape the yahoo finance market news. These steps can be used to scrape any web page, you just have to do little research to identify required `<tags>` and use relevant python methods to collect the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaOUKkUOlrgt"
   },
   "source": [
    "## 2. Scrape Cryptocurrencies\n",
    "\n",
    "In phase One we were able to scrape the [yahoo market news](https://finance.yahoo.com/topic/stock-market-news/) web page. However If you've noticed, as we scroll down the web page more news will appear at the bottom of the page. This is called dynamic page loading. Previous technique is a basic Python method useful to scrape static data, To scrape the dynamically loading data will use a different method called webs craping using **Selenium**. Let's move ahead with this topic. The goal of this section is extract top listing [Crypto currencies](https://finance.yahoo.com/cryptocurrencies) from Yahoo! finance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuViJR0Elrgt"
   },
   "source": [
    "![](https://i.imgur.com/sF6k0Pk.jpg)\n",
    "\n",
    "\n",
    "Here's an outline of the steps we'll follow<br>\n",
    "**2.1 Introduction of selenium**<br>\n",
    "**2.2 Downloads & Installation**<br>\n",
    "**2.3 Install & Import libraries**<br>\n",
    "**2.4 Create Web Driver**<br>\n",
    "**2.5 Exploring and locating Elements**<br>\n",
    "**2.6 Extract & Compile the information into python list**<br>\n",
    "**2.7 Save the extracted information to a CSV file**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvUPJgUEQWZo"
   },
   "source": [
    "### 2.1 Introduction of selenium\n",
    "\n",
    "**[Selenium](https://www.selenium.dev/)** is an open-source web-based automation tool. Python language and other languages are used with Selenium for testing as well as web scraping. Here we will use Chrome browser, but you can try on any browser.<br>\n",
    "\n",
    "**Why you should use Selenium?**\n",
    "- Clicking on buttons\n",
    "- Filling forms\n",
    "- Scrolling\n",
    "- Taking a screen-shot\n",
    "- Refreshing the page\n",
    "\n",
    "You can find proper documentation on selenium [here](https://selenium-python.readthedocs.io/)<br>\n",
    "\n",
    "Following methods will help to find elements in a webpage (these methods will return a list):\n",
    "- `find_elements_by_name`\n",
    "- `find_elements_by_xpath`\n",
    "- `find_elements_by_link_text`\n",
    "- `find_elements_by_partial_link_text`\n",
    "- `find_elements_by_tag_name`\n",
    "- `find_elements_by_class_name`\n",
    "- `find_elements_by_css_selector`\n",
    "\n",
    "In this tutorial we will use only `find_elements_by_xpath` and `find_elements_by_tag_name` You can find complete documentation of these methods [here](https://selenium-python.readthedocs.io/locating-elements.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5IXSU3_QWZs"
   },
   "source": [
    "### 2.2 Downloads & Installation \n",
    "\n",
    "Unlike previous section, here we'll have to do some prep work to implement this method. We will need to install Selenium & proper web browser driver<br>\n",
    "\n",
    "If you are using **Google Colab** platform then execute following code to perform Initial installation. This piece of code `'google.colab' in str(get_ipython())` is used to identify the Google Colab platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41042,
     "status": "ok",
     "timestamp": 1646465828735,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "ot7iEH0QQWZs",
    "outputId": "ad82a03e-ff48-47ce-fc6d-0fd36430cf11"
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Google CoLab Installation')\n",
    "    !apt update --quiet\n",
    "    !apt install chromium-chromedriver --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kj9N6faQWZs"
   },
   "source": [
    "To run it on **Locally** you will need **Webdriver for Chrome** in your machine. You can download it from this link https://chromedriver.chromium.org/downloads and just copy the file in the folder where we will create the python file (No need of installation). But make sure that the driver‘s version matches the the Chrome browser version installed on the local machine.\n",
    "\n",
    "![](https://i.imgur.com/FvQ586e.gif)\n",
    "![](https://i.imgur.com/wQbjRIU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGOzu85gQWZs"
   },
   "source": [
    "### 2.3 Install & Import libraries\n",
    "\n",
    "Installation of the required libraries. Please note that there are some platform specific libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10493,
     "status": "ok",
     "timestamp": 1646465839226,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "WmDBSb_qQWZt",
    "outputId": "9f1f9bc6-aca2-4078-b8f7-5d492eaed162"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "library Installation\n",
      "Not running on CoLab\n"
     ]
    }
   ],
   "source": [
    "print('library Installation')\n",
    "if 'google.colab' not in str(get_ipython()):\n",
    "    print('Not running on CoLab')\n",
    "    #!pip install webdriver-manager --upgrade --quiet\n",
    "else:\n",
    "    print('Running on CoLab')\n",
    "    \n",
    "!pip install selenium --quiet\n",
    "!pip install pandas --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pn1J-U1PQWZt"
   },
   "source": [
    "Once the Libraries installation is done, next step is to import all the required modules / libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1646465839226,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "PaHNd1hSQWZt",
    "outputId": "c05c0e77-8c4c-4ba0-a069-fd09c9b864b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library Import\n",
      "Not running on CoLab\n",
      "Common Library Import\n"
     ]
    }
   ],
   "source": [
    "print('Library Import')\n",
    "if 'google.colab' not in str(get_ipython()):\n",
    "    print('Not running on CoLab')\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    #from webdriver_manager.chrome import ChromeDriverManager\n",
    "    import os\n",
    "else:\n",
    "    print('Running on CoLab')\n",
    "    \n",
    "print('Common Library Import')\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlfwVrnVQWZt"
   },
   "source": [
    "So all the necessary prep work is done, Let's  move ahead to implement this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ms87q9huQWZu"
   },
   "source": [
    "### 2.4 Create Web Driver\n",
    "\n",
    "In this step first we will create the instance of Chrome WebDriver using `webdriver.Chrome()` method. and then the `driver.get()` method will navigate to a page given by the URL. In this case also there is slight variation based on platform, Also passed `options` parameters for e.g. `--headless` option will load the driver in background. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1646465839394,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "cAn7GekCQWZu",
    "outputId": "66f2a784-d8e5-42c6-b0a6-226e8dfbc3c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on CoLab\n"
     ]
    }
   ],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Running on CoLab')\n",
    "    def get_driver(url):\n",
    "        \"\"\"Return web driver\"\"\"\n",
    "        colab_options = webdriver.ChromeOptions()\n",
    "        colab_options.add_argument('--no-sandbox')\n",
    "        colab_options.add_argument('--disable-dev-shm-usage')\n",
    "        colab_options.add_argument('--headless')\n",
    "        driver = webdriver.Chrome(options=colab_options)\n",
    "        driver.get(url)\n",
    "        return driver\n",
    "else:\n",
    "    print('Not running on CoLab')\n",
    "    def get_driver(url):\n",
    "        \"\"\"Return web driver\"\"\"\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        chrome_options.add_argument('--headless')\n",
    "        #serv = Service(ChromeDriverManager().install())\n",
    "        serv = Service(os.getcwd()+'/chromedriver')\n",
    "        driver = webdriver.Chrome(options=chrome_options, service=serv)\n",
    "        driver.get(url)\n",
    "        return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnCtyw66QWZu"
   },
   "source": [
    "Test run of `get_driver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 4841,
     "status": "ok",
     "timestamp": 1646465844233,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "rd7DJYjaQWZu"
   },
   "outputs": [],
   "source": [
    "driver = get_driver('https://finance.yahoo.com/cryptocurrencies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOF6149XQWZu"
   },
   "source": [
    "### 2.5 Exploring and locating Elements\n",
    "\n",
    "This is almost similar step that we have done in phase 1, We will try to identify relevant information like `<tags>`, `class` , `XPath` etc from the web page. Right-click and select the \"Inspect\" to do further analysis.\n",
    "\n",
    "As the webpage is showing cryptocurrency information in the Table form. We can grab the table header by using tag `<th>`, we will use find_elements by TAG to get the table headers. These headers will be used columns for CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 797,
     "status": "ok",
     "timestamp": 1646465845024,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "zR29b9xH8di9",
    "outputId": "892fd3bd-c5ec-4fcd-bc08-f8cf1b99c9f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbol\n",
      "Price (Intraday)\n"
     ]
    }
   ],
   "source": [
    "header = driver.find_elements(By.TAG_NAME, value= 'th')\n",
    "print(header[0].text)\n",
    "print(header[2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNZ0kwDhQWZv"
   },
   "source": [
    "Creating a helper function to get first 10 columns from header, we have used List comprehension with conditions. you can also check out usage of `enumerate` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1646465845025,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "QTfDpiWrQWZv"
   },
   "outputs": [],
   "source": [
    "def get_table_header(driver):\n",
    "    \"\"\"Return Table columns in list form \"\"\"\n",
    "    header = driver.find_elements(By.TAG_NAME, value= 'th')\n",
    "    header_list = [item.text for index, item in enumerate(header) if index < 10]\n",
    "    return header_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqW7UgNwQWZv"
   },
   "source": [
    "Next we find out number of rows available in a Page, you can see table rows are placed in `<tr>` tag, we can capture the `XPath` by selection `<tr>` tag the Right Click $\\rightarrow$ Copy $\\rightarrow$ Copy XPath.\n",
    "\n",
    "![](https://i.imgur.com/DVAYMzY.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlfJ632vQWZv"
   },
   "source": [
    "So we get the  XPath value as `//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr[1]`, Let's use this with `find_element()` & `By.XPATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1646465845226,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "U3FVWjftQWZv",
    "outputId": "9e4240a5-490a-4293-ebcd-86fe7a59f6ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BTC-USD\\nBitcoin USD 39,122.67 -415.29 -1.05% 742.389B 18.031B 18.031B 18.031B 18.976M'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt=driver.find_element(By.XPATH, value='//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr[1]').text\n",
    "txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6JcCdb1Trrp"
   },
   "source": [
    "Above `XPath` points to first row, we can get rid of row number part from XPath and use it with `find_elements` to get hold of all the available rows. Let's implement this in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1646465845423,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "dd9kJccjToHK"
   },
   "outputs": [],
   "source": [
    "def get_table_rows(driver):\n",
    "    \"\"\"Get number of rows available on the page \"\"\"\n",
    "    tablerows = len(driver.find_elements(By.XPATH, value='//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr'))\n",
    "    return tablerows    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1646465845423,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "0v1phXVMRn2E",
    "outputId": "5121e8bc-e07f-47ed-aaef-b2f6fac934e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "print(get_table_rows(driver))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A45Ekufq8di9"
   },
   "source": [
    "Similarly we can take the XPath for any column value.\n",
    "\n",
    "![](https://i.imgur.com/aT3I3Ur.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0dHoGoVVzTv"
   },
   "source": [
    "This is the XPAth for a column `//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr[1]/td[2]`.<br>\n",
    "If you noticed the the number after `tr` & `td` represents the `row_number` and `column_number`, we can check this with `find_element()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 156,
     "status": "ok",
     "timestamp": 1646465845576,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "1MNZ9bGilrgt",
    "outputId": "8ae1909f-a1d7-48d8-a511-3d33f6a205ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bitcoin USD'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.find_element(By.XPATH, value='//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr[1]/td[2]').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2KhgeZAW7d8"
   },
   "source": [
    "So we can change the `row_number` & `column_number` in `XPath` and loop it through row count and column count to get all the available column values. Let's generalize this and put it in a function. We will get the data for one row at a time and return column value in the form of dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1646465845577,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "pEorS2flYikH"
   },
   "outputs": [],
   "source": [
    "def parse_table_rows(rownum, driver, header_list):\n",
    "    \"\"\"get the data for one row at a time and return column value in the form of dictionary\"\"\"\n",
    "    row_dictionary = {}\n",
    "    time.sleep(1/3)\n",
    "    for index , item in enumerate(header_list):\n",
    "        column_xpath = '//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr[{}]/td[{}]'.format(rownum, index+1)\n",
    "        row_dictionary[item] = driver.find_element(By.XPATH, value=column_xpath).text\n",
    "    return row_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96fzN44rXu-u"
   },
   "source": [
    "The Yahoo! Finance web page is showing only 25 Cryptocurrencies per page and user will have click `Next` button to load next sets of crypto currencies. This is called as **Pagination**. This is the main reason we are implementing selenium method to handle the events like pagination. you can perform multiple events like clicking, scrolling , refreshing etc. on a webpage using selenium methods.\n",
    "\n",
    "Now we will grab the `XPath` of `Next` button, find the element using `find_element` method and after that we can perform click action using `.click()` method \n",
    "\n",
    "![](https://i.imgur.com/tCxQKfR.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1646465845981,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "s45SMjxligLN"
   },
   "outputs": [],
   "source": [
    "button_element = driver.find_element(By.XPATH, value = '//*[@id=\"scr-res-table\"]/div[2]/button[3]')\n",
    "button_element.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osU8bvFYeIgu"
   },
   "source": [
    "In this section we have learned how to get required data points, and perform events on webpage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1646465845982,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "J0CKtbmTf5aF"
   },
   "outputs": [],
   "source": [
    "driver.quit() #terminating driver from test runs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdpxUXQCd5LH"
   },
   "source": [
    "### 2.6 Extract & Compile the information into python list\n",
    "\n",
    "Let's put all the pieces in the puzzle, we will pass the integer `total_crypto` i.e. numbers of rows to be scraped (in this case 100 rows) in the function. Parse each row from the page and append the data in the `List` till the total parsed row count reach to `total_crypto`. In addition we will perform `Next` button click if we are at the last row of the table. \n",
    "\n",
    "**Please Note** : Here to identify the `Next` button element we have used [WebDriverWait](https://www.selenium.dev/selenium/docs/api/java/org/openqa/selenium/support/ui/WebDriverWait.html) class instead of using `find_element()` method. In this technique we can pass some wait-time before grabbing the element. This type of implememtation is done to avoid the [`StaleElementReferenceException`](https://stackoverflow.com/questions/27003423/staleelementreferenceexception-on-python-selenium).\n",
    "\n",
    "Code Sample:\n",
    "```\n",
    "element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"scr-res-table\"]/div[2]/button[3]')))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 195,
     "status": "ok",
     "timestamp": 1646465846174,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "FgH4XUG9SNAD"
   },
   "outputs": [],
   "source": [
    "def parse_multiple_pages(driver, total_crypto):\n",
    "    \"\"\"Loop through each row, perform Next button click at the end of page \n",
    "    return total_crypto numbers of rows \n",
    "    \"\"\"\n",
    "    table_data = []\n",
    "    page_num = 1\n",
    "    is_scraping = True\n",
    "    header_list = get_table_header(driver)\n",
    "\n",
    "    while is_scraping:\n",
    "        table_rows = get_table_rows(driver)\n",
    "        print('Found {} rows on Page : {}'.format(table_rows, page_num))\n",
    "        print('Parsing Page : {}'.format(page_num))\n",
    "        table_data += [parse_table_rows(i, driver, header_list) for i in range (1, table_rows + 1)]\n",
    "        total_count = len(table_data)\n",
    "        print('Total rows scraped : {}'.format(total_count))\n",
    "        if total_count >= total_crypto:\n",
    "            print('Done Parsing..')\n",
    "            is_scraping = False\n",
    "        else:    \n",
    "            print('Clicking Next Button')\n",
    "            element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"scr-res-table\"]/div[2]/button[3]')))\n",
    "            element.click() \n",
    "            page_num += 1\n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZL49rJqd5aa"
   },
   "source": [
    "### 2.7 Save the extracted information to a CSV file\n",
    "\n",
    "This is the last step of this section, we are creating a last function which will be the placeholder for all helper functions and at the and we will save the data in CSV format using `pd.to_csv` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1646465846174,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "a8AxKJLrYikH"
   },
   "outputs": [],
   "source": [
    "def scrape_yahoo_crypto(url, total_crypto, path=None):\n",
    "    \"\"\"Get the list of yahoo finance crypto-currencies and write them to CSV file \"\"\"\n",
    "    if path is None:\n",
    "        path = 'crypto-currencies.csv'\n",
    "    print('Creating driver')\n",
    "    driver = get_driver(url)    \n",
    "    table_data = parse_multiple_pages(driver, total_crypto)\n",
    "    driver.close()\n",
    "    print('Save the data to a CSV')\n",
    "    table_df = pd.DataFrame(table_data)\n",
    "    #print(table_df)\n",
    "    table_df.to_csv(path, index=None)\n",
    "    #This return statement is optional, we are doing this just analyze the final output \n",
    "    return table_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJc4JNBaSNAD"
   },
   "source": [
    "Time to scrape some cryoptos!!! , we will scrape top 100 cryptos in Yahoo! Finance webpage by calling `scrape_yahoo_crypto` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 77680,
     "status": "ok",
     "timestamp": 1646465923852,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "1fEtHtL2jXxR",
    "outputId": "64392e07-b143-4d06-c43f-bd32e114e919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating driver\n",
      "Found 25 rows on Page : 1\n",
      "Parsing Page : 1\n",
      "Total rows scraped : 25\n",
      "Clicking Next Button\n",
      "Found 25 rows on Page : 2\n",
      "Parsing Page : 2\n",
      "Total rows scraped : 50\n",
      "Clicking Next Button\n",
      "Found 25 rows on Page : 3\n",
      "Parsing Page : 3\n",
      "Total rows scraped : 75\n",
      "Clicking Next Button\n",
      "Found 25 rows on Page : 4\n",
      "Parsing Page : 4\n",
      "Total rows scraped : 100\n",
      "Done Parsing..\n",
      "Save the data to a CSV\n"
     ]
    }
   ],
   "source": [
    "YAHOO_FINANCE_URL = BASE_URL+'/cryptocurrencies'\n",
    "TOTAL_CRYPTO = 100\n",
    "crypto_df = scrape_yahoo_crypto(YAHOO_FINANCE_URL, TOTAL_CRYPTO,'crypto-currencies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tf7cgOSVYikI"
   },
   "source": [
    "The \"crypto-currencies.csv\" should be available in File $\\rightarrow$ Open Menu, you can download the file or directly open it on browser. Please verify the file content and compare it with the actual information available on the webpage.\n",
    "\n",
    "You can also check the data by grabbing few rows form the data frame returned by the `scrape_yahoo_crypto` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1646465923853,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "4-fahUk4SNAE",
    "outputId": "61d18f58-a07e-4663-eb72-ad9eeaa7d080"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Name</th>\n",
       "      <th>Price (Intraday)</th>\n",
       "      <th>Change</th>\n",
       "      <th>% Change</th>\n",
       "      <th>Market Cap</th>\n",
       "      <th>Volume in Currency (Since 0:00 UTC)</th>\n",
       "      <th>Volume in Currency (24Hr)</th>\n",
       "      <th>Total Volume All Currencies (24Hr)</th>\n",
       "      <th>Circulating Supply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BTC-USD</td>\n",
       "      <td>Bitcoin USD</td>\n",
       "      <td>39,122.67</td>\n",
       "      <td>-415.29</td>\n",
       "      <td>-1.05%</td>\n",
       "      <td>742.389B</td>\n",
       "      <td>18.031B</td>\n",
       "      <td>18.031B</td>\n",
       "      <td>18.031B</td>\n",
       "      <td>18.976M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ETH-USD</td>\n",
       "      <td>Ethereum USD</td>\n",
       "      <td>2,640.22</td>\n",
       "      <td>-35.59</td>\n",
       "      <td>-1.33%</td>\n",
       "      <td>316.43B</td>\n",
       "      <td>7.875B</td>\n",
       "      <td>7.875B</td>\n",
       "      <td>7.875B</td>\n",
       "      <td>119.85M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USDT-USD</td>\n",
       "      <td>Tether USD</td>\n",
       "      <td>1.0003</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>-0.01%</td>\n",
       "      <td>79.734B</td>\n",
       "      <td>40.582B</td>\n",
       "      <td>40.582B</td>\n",
       "      <td>40.582B</td>\n",
       "      <td>79.713B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BNB-USD</td>\n",
       "      <td>Binance Coin USD</td>\n",
       "      <td>382.86</td>\n",
       "      <td>+0.68</td>\n",
       "      <td>+0.18%</td>\n",
       "      <td>63.217B</td>\n",
       "      <td>1.258B</td>\n",
       "      <td>1.258B</td>\n",
       "      <td>1.258B</td>\n",
       "      <td>165.117M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USDC-USD</td>\n",
       "      <td>USD Coin USD</td>\n",
       "      <td>0.999583</td>\n",
       "      <td>-0.000997</td>\n",
       "      <td>-0.10%</td>\n",
       "      <td>52.919B</td>\n",
       "      <td>2.854B</td>\n",
       "      <td>2.854B</td>\n",
       "      <td>2.854B</td>\n",
       "      <td>52.941B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Symbol              Name Price (Intraday)     Change % Change Market Cap  \\\n",
       "0   BTC-USD       Bitcoin USD        39,122.67    -415.29   -1.05%   742.389B   \n",
       "1   ETH-USD      Ethereum USD         2,640.22     -35.59   -1.33%    316.43B   \n",
       "2  USDT-USD        Tether USD           1.0003    -0.0001   -0.01%    79.734B   \n",
       "3   BNB-USD  Binance Coin USD           382.86      +0.68   +0.18%    63.217B   \n",
       "4  USDC-USD      USD Coin USD         0.999583  -0.000997   -0.10%    52.919B   \n",
       "\n",
       "  Volume in Currency (Since 0:00 UTC) Volume in Currency (24Hr)  \\\n",
       "0                             18.031B                   18.031B   \n",
       "1                              7.875B                    7.875B   \n",
       "2                             40.582B                   40.582B   \n",
       "3                              1.258B                    1.258B   \n",
       "4                              2.854B                    2.854B   \n",
       "\n",
       "  Total Volume All Currencies (24Hr) Circulating Supply  \n",
       "0                            18.031B            18.976M  \n",
       "1                             7.875B            119.85M  \n",
       "2                            40.582B            79.713B  \n",
       "3                             1.258B           165.117M  \n",
       "4                             2.854B            52.941B  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crypto_df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ion8RYbZ8di_"
   },
   "source": [
    "**Summary** : Hope you've enjoyed this tutorial. Selenium enables us to perform multiple actions on the web browser which is really very handy to scrape different type of data from any webpage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-klaoqb8di_"
   },
   "source": [
    "## 3. Scrape Market Events Calendar\n",
    "\n",
    "This is the final segment of the tutorial, in this section we will learn how to extract embedded [JSON](https://www.w3schools.com/js/js_json_intro.asp) formatted data which can be easily converted to Python dictionary. Problem statement for section is to scrape date-wise market events from [Yahoo! finance](https://finance.yahoo.com/calendar)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EsDeulcAJAV"
   },
   "source": [
    "![](https://i.imgur.com/bKQoAjs.png)\n",
    "\n",
    "Here's an outline of the steps we'll follow<br>\n",
    "**3.1 Install & Import libraries**<br>\n",
    "**3.2 Download & Parse web page**<br>\n",
    "**3.3 Get Embedded Json data**<br>\n",
    "**3.4 Locating Json Keys**<br>\n",
    "**3.5 Pagination & Compiling the information into python list**<br>\n",
    "**3.6 Save the extracted information to a CSV file**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30uQai53AJAV"
   },
   "source": [
    "### 3.1 Install & Import libraries\n",
    "\n",
    "First step to install and import Python Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10191,
     "status": "ok",
     "timestamp": 1646465934039,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "5h5BUiRoAJAV",
    "outputId": "f31a06db-a223-4164-8218-71bca423d4e0"
   },
   "outputs": [],
   "source": [
    "!pip install requests --quiet\n",
    "!pip install beautifulsoup4 --quiet\n",
    "!pip install pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1646465934040,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "2m7QniEX8di_"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HV8zDkvJAJAV"
   },
   "source": [
    "### 3.2 Download & Parse web page\n",
    "\n",
    "This is exactly same step that we've performed to download webpage in section 1.1 , Here we have used [custom header](https://docs.python-requests.org/en/master/user/quickstart/#custom-headers) in `requests.get()`\n",
    "\n",
    "Most of the things are explained in section 1.1, creating the helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1646465934040,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "qFhyZG558di_"
   },
   "outputs": [],
   "source": [
    "def get_event_page(scraper_url):\n",
    "    \"\"\"Download a webpage and return a beautiful soup doc\"\"\"\n",
    "    headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
    "                  \"(KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19582\"\n",
    "    }\n",
    "    response = requests.get(scraper_url, headers=headers)\n",
    "    if not response.ok:\n",
    "        print('Status code:', response.status_code)\n",
    "        raise Exception('Failed to fetch web page ' + scraper_url)\n",
    "    # Construct a beautiful soup document\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 718,
     "status": "ok",
     "timestamp": 1646465934753,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "qBYaeOWDAJAV"
   },
   "outputs": [],
   "source": [
    "doc = get_event_page('https://finance.yahoo.com/calendar/earnings?from=2022-02-27&to=2022-03-05&day=2022-02-28')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7TGB4SZAJAV"
   },
   "source": [
    "### 3.3 Get Embedded Json data\n",
    "\n",
    "\n",
    "In this step we will locate the Jason formated data, Open the web page and do Right Click $\\rightarrow$ View Page Source, If you scroll down to source page you will notice the [Json](https://www.w3schools.com/whatis/whatis_json.asp) formated data. Luckily this information is `<script>` tag which contain following text `/* -- Data -- */`.\n",
    "\n",
    "![](https://i.imgur.com/2xlpNbw.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcePCvsCAJAV"
   },
   "source": [
    "we will use [Regular expressions](https://docs.python.org/3/library/re.html) to get text  inside `<script>` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1646465934754,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "YkAMhCnRAJAW"
   },
   "outputs": [],
   "source": [
    "pattern = re.compile(r'\\s--\\sData\\s--\\s')\n",
    "#script_data = doc.find('script', text=pattern).text\n",
    "script_data = doc.find('script', text=pattern).contents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_t2zpCGAJAW"
   },
   "source": [
    "Further the `Json` formated string has first key as `context` and it ends at 12 characters from the end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1646465934987,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "AecrY-WkAJAW",
    "outputId": "18d20447-cd87-4826-ad55-06bcfeebba43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(function (root) {\n",
      "/* -- Data -- */\n",
      "root.App || (root.App = {});\n",
      "root.App.now = 1646585550274;\n",
      "root.App.main = {\"context\":{\"dispatcher\":{\"stores\":{\"P\n",
      "odal\":{\"strings\":1},\"tdv2-wafer-header\":{\"strings\":1},\"yahoodotcom-layout\":{\"strings\":1}}},\"options\":{\"defaultBundle\":\"td-app-finance\"}}}};\n",
      "}(this));\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(script_data[:150])\n",
    "print(script_data[-150:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiYPaUpPAJAW"
   },
   "source": [
    "So we can grab the Json string using Python slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1646465934987,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "-WYEbgQWAJAW"
   },
   "outputs": [],
   "source": [
    "start  = script_data.find('context')-2\n",
    "json_text  = script_data[start:-12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "605zLEjAAJAW"
   },
   "source": [
    "Using `json.loads()`method to convert Jason string into Python Dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1646465934988,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "ShAdpupKAJAW",
    "outputId": "17abaff8-3e40-4409-f5d2-778251b4c109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_dictionary = json.loads(json_text)\n",
    "type(parsed_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUjp0BqkAJAW"
   },
   "source": [
    "Creating function using above information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1646465934988,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "rgcsfbSx8di_"
   },
   "outputs": [],
   "source": [
    "def get_json_dictionary(doc):\n",
    "    \"\"\"Get Json formated data in the form of Python Dictionary\"\"\"\n",
    "    pattern = re.compile(r'\\s--\\sData\\s--\\s')\n",
    "    script_data = doc.find('script', text=pattern).text\n",
    "    script_data = doc.find('script', text=pattern).contents[0]\n",
    "    \n",
    "    start  = script_data.find('context')-2\n",
    "    json_text  = script_data[start:-12]\n",
    "    \n",
    "    parsed_dictionary = json.loads(json_text)\n",
    "    return parsed_dictionary    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIolGWTkAJAW"
   },
   "source": [
    "### 3.4 Locating Json Keys\n",
    "\n",
    "So basically the Json text is multi level nested dictionaries, and some keys are used to store all the meta data displayed on the webpage. In this section we will identify the keys for the data we are trying to scrape.\n",
    "\n",
    "We'll need some `Json Formatter` tool to navigate through multiple keys, I am using online tool https://jsonblob.com/, However you can choose any tool.\n",
    "\n",
    "We will write the Json text into `my_json_file.json` file, then grab the file content and paste it to the left panel of https://jsonblob.com/. The JSON Blob it will do nice formatting, we can easily navigate through each Keys and search any item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1646465934988,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "SUWFo_MG8di_"
   },
   "outputs": [],
   "source": [
    "with open('my_json_file.json', 'w') as file:\n",
    "    file.write(json_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8FwV_y1AJAW"
   },
   "source": [
    "Next step is to find the Required Key location, Let's search the company name `3D Systems Corporation` displayed in the webpage in the [JSON Blob](https://jsonblob.com/) formatter.\n",
    "![](https://i.imgur.com/Iv8b7vl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XklVfiv-AJAW"
   },
   "source": [
    "![](https://i.imgur.com/jpJYOCy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKY1688xAJAX"
   },
   "source": [
    "You can see the table data is stored in the `rows` key, and we can track down the parent keys as shown in the above screen, checkout the content of `row` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1646465934988,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "00fa9Sx9AJAX",
    "outputId": "3ea176de-788b-40a1-83cb-7197a516bb8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ticker': 'DDD',\n",
       "  'companyshortname': '3D Systems Corporation',\n",
       "  'startdatetime': '2022-02-28T16:05:00.000Z',\n",
       "  'startdatetimetype': 'TAS',\n",
       "  'epsestimate': 0.03,\n",
       "  'epsactual': 0.09,\n",
       "  'epssurprisepct': 181.25,\n",
       "  'timeZoneShortName': 'EST',\n",
       "  'gmtOffsetMilliSeconds': -18000000,\n",
       "  'quoteType': 'EQUITY'},\n",
       " {'ticker': 'AMBA',\n",
       "  'companyshortname': 'Ambarella, Inc.',\n",
       "  'startdatetime': '2022-02-28T16:31:00.000Z',\n",
       "  'startdatetimetype': 'TAS',\n",
       "  'epsestimate': 0.42,\n",
       "  'epsactual': 0.45,\n",
       "  'epssurprisepct': 6.13,\n",
       "  'timeZoneShortName': 'EST',\n",
       "  'gmtOffsetMilliSeconds': -18000000,\n",
       "  'quoteType': 'EQUITY'},\n",
       " {'ticker': 'AAON',\n",
       "  'companyshortname': 'AAON, Inc.',\n",
       "  'startdatetime': '2022-02-28T16:01:00.000Z',\n",
       "  'startdatetimetype': 'TAS',\n",
       "  'epsestimate': 0.28,\n",
       "  'epsactual': 0.18,\n",
       "  'epssurprisepct': -35.02,\n",
       "  'timeZoneShortName': 'EST',\n",
       "  'gmtOffsetMilliSeconds': -18000000,\n",
       "  'quoteType': 'EQUITY'}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_dictionary['context']['dispatcher']['stores']['ScreenerResultsStore']['results']['rows'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1646465934989,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "mnq4KHKXAJAX",
    "outputId": "9ff532d7-c82c-4185-99a0-4369aacdc8cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows on the Current page : 100\n"
     ]
    }
   ],
   "source": [
    "print('Total Rows on the Current page :',len(parsed_dictionary['context']['dispatcher']['stores']['ScreenerResultsStore']['results']['rows']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6khD1Pn6AJAX"
   },
   "source": [
    "This sub-dictionary is showing all the data displayed on current page.<br>\n",
    "You can do more research and exploration to get different information from the web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1646465934989,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "DcGw1gDRAJAX",
    "outputId": "4b6b5161-94a4-4e42-9559-bb21ac1ae7b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows for the search criteria : 205\n"
     ]
    }
   ],
   "source": [
    "print('Total Rows for the search criteria :',parsed_dictionary['context']['dispatcher']['stores']['ScreenerResultsStore']['results']['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1646465934989,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "I7_WMaROAJAX",
    "outputId": "77de1f3a-7e30-4e4f-fc99-d1901eb01abe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'data': 'ticker', 'content': 'Symbol'},\n",
       " {'data': 'companyshortname', 'content': 'Company Name'},\n",
       " {'data': 'startdatetime', 'content': 'Event Start Date'},\n",
       " {'data': 'startdatetimetype', 'content': 'Event Start Time'},\n",
       " {'data': 'epsestimate', 'content': 'EPS Estimate'},\n",
       " {'data': 'epsactual', 'content': 'Reported EPS'},\n",
       " {'data': 'epssurprisepct', 'content': 'Surprise (%)'},\n",
       " {'data': 'timeZoneShortName', 'content': 'Timezone short name'},\n",
       " {'data': 'gmtOffsetMilliSeconds', 'content': 'GMT Offset'}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Columns\")\n",
    "parsed_dictionary['context']['dispatcher']['stores']['ScreenerResultsStore']['results']['columns']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbgxoTdPAJAX"
   },
   "source": [
    "Putting this in function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1646465934989,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "wyX1dtPy8di_"
   },
   "outputs": [],
   "source": [
    "def get_total_rows(parsed_dictionary):\n",
    "    '''Get the Total Rows for the search criteria & Columns detail''' \n",
    "    total_rows = parsed_dictionary['context']['dispatcher']['stores']['ScreenerResultsStore']['results']['total']\n",
    "    return total_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1646465934990,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "435fS9Vv8di_"
   },
   "outputs": [],
   "source": [
    "def get_page_rows(parsed_dictionary):\n",
    "    \"\"\"Get the Content current page\"\"\"    \n",
    "    data_dictionary = parsed_dictionary['context']['dispatcher']['stores']['ScreenerResultsStore']['results']['rows']\n",
    "    return data_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoJ-lzeNAJAX"
   },
   "source": [
    "### 3.5 Pagination & Compiling the information into python list\n",
    "\n",
    "As we saw in the previous section how to handle `Pagination` using selenium methods, here we'll learn new technique of accessing multiple pages.<br>\n",
    "\n",
    "Most of the times webpage url gets changed runtime depending on the user selection, e.g. In below screen-shot I selected the **Earnings** for **1-March-2022**. You can notice how that information is passed in the url. \n",
    "![](https://i.imgur.com/h5QU99h.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8g-j5xJAJAX"
   },
   "source": [
    "Similarly, when i click next button `offset`& `size` values gets changed in the url.\n",
    "![](https://i.imgur.com/jYa1vq5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5LQ_YkvAJAX"
   },
   "source": [
    "So we can figure out the pattern & structure of the url and how it affects the page navigation.<br> \n",
    "\n",
    "In this case webpage url pattern is mentioned below:<br>\n",
    "- Following values are used for calendar event type \n",
    "`event_types = ['splits','economic','ipo','earnings']`\n",
    "- Date is passed in `yyyy-mm-dd` format\n",
    "- Page number is controlled by `offset` value (for first page `offset=0`)\n",
    "- Maximum numbers of rows in a page is assigned to `size`\n",
    "\n",
    "Based on the above information we can build url runtime and download the page then extract the information, this is how we handle the pagination.<br>\n",
    "\n",
    "Putting all things together in a function. In this function we will pass `event_type` and `date`, then we will calculate the total rows for matching criteria using `get_columns_and_total_rows` function. Maximum rows per page is constant (i.e 100), so we can build iterating summation logic to calculate total number of pages involved for current criteria and extract each page data in the loop.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1646465935147,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "3magUBX48di_"
   },
   "outputs": [],
   "source": [
    "def scrape_all_pages(event_type, date):\n",
    "    \"\"\"Loop through each row and return lists of data dictiionary\"\"\"\n",
    "    YAHOO_CAL_URL = BASE_URL+'/calendar/{}?day={}&offset={}&size={}'\n",
    "    max_rows_per_page = '100' # this indicates max rows per page \n",
    "    page_number = 1\n",
    "    final_data_dictionary = []\n",
    "    \n",
    "    while page_number > 0:\n",
    "        print(\"Pricessing page # {}\".format(page_number))\n",
    "        page_url = str((page_number - 1 ) * int(max_rows_per_page))\n",
    "        scrape_url = YAHOO_CAL_URL.format(event_type, date, page_url, max_rows_per_page)\n",
    "        print(\"Scrape url for page {} is {}\".format(page_number,scrape_url))\n",
    "        page_doc = get_event_page(scrape_url)\n",
    "        parse_dict = get_json_dictionary(page_doc)\n",
    "        if page_number == 1:\n",
    "            total_rows = get_total_rows(parse_dict)        \n",
    "        final_data_dictionary += get_page_rows(parse_dict)\n",
    "        if len(final_data_dictionary) >= total_rows:\n",
    "            page_number = 0\n",
    "            return final_data_dictionary\n",
    "        page_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scLr8_DFAJAY"
   },
   "source": [
    "### 3.6 Save the extracted information to a CSV file\n",
    "\n",
    "In this last section, we will save the data to csv format using `pd.DataFrame()` & `to_csv()` and call everything in a single placeholder function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1646465935147,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "VLfoy_33QWZz"
   },
   "outputs": [],
   "source": [
    "def scrape_yahoo_calendar(event_types, date_param):\n",
    "    \"\"\"Get the list of yahoo finance calendar and write them to CSV file \"\"\"\n",
    "    for event in event_types:\n",
    "        print('Web Scraping for ', event  )\n",
    "        data_dict = scrape_all_pages(event, date_param)\n",
    "        scraped_df = pd.DataFrame(data_dict)\n",
    "        scraped_df.to_csv(event+'_'+date_param+'.csv',index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cUgyY0IAJAY"
   },
   "source": [
    "calling final function `scrape_yahoo_calendar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3446,
     "status": "ok",
     "timestamp": 1646466032802,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "WpvYTWyx8djA",
    "outputId": "18a66183-f658-4ae3-eb62-e726a654da84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web Scraping for  splits\n",
      "Pricessing page # 1\n",
      "Scrape url for page 1 is https://finance.yahoo.com/calendar/splits?day=2022-03-18&offset=0&size=100\n",
      "Web Scraping for  economic\n",
      "Pricessing page # 1\n",
      "Scrape url for page 1 is https://finance.yahoo.com/calendar/economic?day=2022-03-18&offset=0&size=100\n",
      "Web Scraping for  ipo\n",
      "Pricessing page # 1\n",
      "Scrape url for page 1 is https://finance.yahoo.com/calendar/ipo?day=2022-03-18&offset=0&size=100\n",
      "Web Scraping for  earnings\n",
      "Pricessing page # 1\n",
      "Scrape url for page 1 is https://finance.yahoo.com/calendar/earnings?day=2022-03-18&offset=0&size=100\n"
     ]
    }
   ],
   "source": [
    "BASE_URL = 'https://finance.yahoo.com' #Global Variable \n",
    "date_param = '2022-02-28'\n",
    "date_param = '2022-03-18' # no data condition\n",
    "event_types = ['splits','economic','ipo','earnings']\n",
    "scrape_yahoo_calendar(event_types, date_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBjhZrB7AJAY"
   },
   "source": [
    "Total 4 csv files \"event_type_yyyy-mm-dd.csv\" should be available in File $\\rightarrow$ Open Menu, you can download the file or directly open it on browser. Please verify the file content and compare it with the actual information available on the webpage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpW06QKr8djA"
   },
   "source": [
    "**Summary** : This is very useful technique which can be easily replicable. Without writing any customized code we were able to extract the data from multiple types of web pages just by changing one variable (in this case `event_type`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jA-c9qFnAJAY"
   },
   "source": [
    "## References\n",
    "\n",
    "References to links you found useful\n",
    "\n",
    "- https://htmldog.com/guides/html/\n",
    "- https://selenium-python.readthedocs.io/index.html\n",
    "- https://stackoverflow.com/questions/27003423/staleelementreferenceexception-on-python-selenium\n",
    "- https://www.w3schools.com/js/js_json_intro.asp\n",
    "- https://hhsm95.dev/blog/the-importance-of-using-user-agent-to-scraping-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIcjbAgOAJAY"
   },
   "source": [
    "## Future Work\n",
    "\n",
    "Ideas for future work<br>\n",
    "- Automate this process using [AWS Lambda](https://aws.amazon.com/lambda/) to download daily market calendar, crypto-currencies & market news in CSV format.\n",
    "- Move the old files to an Archive folder append date-stamp to the file if required also  delete the Archived files older than 2 weeks.\n",
    "- Process the raw data extracted from third technique using different methods of pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkY7RsOeAJAY"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this Tutorial we Implement following web scraping techniques.\n",
    " - Using requests, BeautifulSoup and HTML tags to extract web page.\n",
    " - Using Selenium to scrape data from dynamically loading websites.\n",
    " - Using embedded Json format data to scrape website .\n",
    "\n",
    "I hope I was able to teach you these webscraping methods and I hope you can use this knowledge to scrape any website.\n",
    "\n",
    "Thank you for reading. Happy coding!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "executionInfo": {
     "elapsed": 2409,
     "status": "ok",
     "timestamp": 1646465942166,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "7aSVLdJ1lrgw",
    "outputId": "1039af28-0280-4ab9-b4d7-c5267d7ac4d5"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"vinodvidhole/yahoo-finance-web-scraper\" on https://jovian.ai/\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/vinodvidhole/yahoo-finance-web-scraper\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/vinodvidhole/yahoo-finance-web-scraper'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project=\"yahoo-finance-web-scraper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJ6o34es8djA"
   },
   "source": [
    "- check grammar , extra space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "DvSMJZeClrgo"
   ],
   "name": "yahoo-finance-web-scraper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
