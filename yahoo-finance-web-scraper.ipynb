{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pF8beWv0lrgc"
   },
   "source": [
    "# Web Scraping Yahoo! Finance using Python\n",
    "\n",
    "A detailed guide for web scraping https://finance.yahoo.com/ using **requests**, **BeautifulSoup**, **Selenium**, **HTML tags** & embedded **JSON** data.\n",
    "\n",
    "![](https://imgur.com/7jMFOcE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FULGhEjlrge"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "**What is Web scraping?**<br>\n",
    "Web scraping is the process of extracting and parsing data from websites in an automated fashion using a computer program. It's a useful technique for creating datasets for research and learning.\n",
    "\n",
    "\n",
    "**Objective**<br>\n",
    "The main objective of this tutorial is to showcase different web scraping methods which can be applied to any web page. \n",
    "This is for educational purposes only. Please read the Terms & Conditions carefully for any website to see whether you can legally use the data. \n",
    "\n",
    "In this project, we will perform web scraping using the following 3 techniques based on the problem statement.\n",
    "* use `requests`, `BeautifulSoup` and `HTML tags` to extract web page\n",
    "* use `Selenium` to scrape data from dynamically loading websites \n",
    "* use embedded `JSON` data to scrape website \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CFXNNENlrgf"
   },
   "source": [
    "**The problem statement**<br>\n",
    "1. Scrape **Stock Market News** (url : https://finance.yahoo.com/topic/stock-market-news/) :<br>\n",
    "    This web page shows the latest **news** related to **stock market**, we will try to extract data from this web page and store it in a `CSV` (comma-separated values) file. The file layout would be as mentioned below.\n",
    "    ```\n",
    "    source,headline,url,content,image\n",
    "    <source of the news>,<news head line>,<news url>,<news content>,<news thumbnail image>\n",
    "    ```\n",
    "\n",
    "2. Scrape **Cryptocurrencies** (url : https://finance.yahoo.com/cryptocurrencies) :<br>\n",
    "    This Yahoo! finance web page shows list of trending **Cryptocurrencies** in tabular format, we will perform the web scraping to retrieve first 10 columns for top 100 **Cryptocurrencies** in `CSV` format.\n",
    "    ```\n",
    "    Symbol,Name,Price (Intraday),Change,% Change,Market Cap,Volume in Currency (Since 0:00 UTC),\n",
    "    Volume in Currency (24Hr),Total Volume All Currencies (24Hr),Circulating Supply\n",
    "    BTC-USD,Bitcoin USD,\"43,312.13\",-947.50,-2.14%,821.76B,27.727B,27.727B,27.727B,18.973M\n",
    "\n",
    "    ```\n",
    "        \n",
    "3. Scrape **Market Events Calendar** (url : https://finance.yahoo.com/calendar) :<br> \n",
    "    This page shows **date-wise market events**, user have the option to select the date and choose any one of the following market events **Earnings**, **Stock Splits**, **Economic Events** & **IPO**. Our aim is to create a script which can be run for any single date and market event which grabs the data and loads in `CSV` format.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Z9UQZf8lrgg"
   },
   "source": [
    "**Prerequisites**\n",
    "* Knowledge of Python\n",
    "* Basic knowledge of HTML although it is not necessary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbMRHIhblrgh"
   },
   "source": [
    "**How to run the Code**<br>\n",
    "You can execute the code using \"Run\" button on the top of this page and selecting **\"Run on Colab\"** or **\"Run Locally\"** \n",
    "<br>\n",
    "<br>\n",
    "**Setup and Tools**<br>\n",
    "<u>Run on Colab :</u> \n",
    "    You will need to provide the Google login to run this notebook on Colab.<br>\n",
    "<u>Run Locally :</u> Download and install [Anaconda](https://www.anaconda.com/) framework, We will be using Jupyter Notebook for writing & executing code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YjeIdedlrgh"
   },
   "source": [
    "**Version control**\n",
    "\n",
    "You can make changes and save your version of the notebook to [Jovian](https://jovian.ai/) by executing following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4953,
     "status": "ok",
     "timestamp": 1646608884871,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "iLLVpn2olrgi"
   },
   "outputs": [],
   "source": [
    "!pip install jovian --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1646608884871,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "iTKJFEYglrgj"
   },
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "executionInfo": {
     "elapsed": 5436,
     "status": "ok",
     "timestamp": 1646608890288,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "AxYOBC1xlrgj",
    "outputId": "048a42bf-e240-4ca8-99c5-d64462b3c6e5"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"vinodvidhole/yahoo-finance-web-scraper\" on https://jovian.ai/\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/vinodvidhole/yahoo-finance-web-scraper\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/vinodvidhole/yahoo-finance-web-scraper'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute this to save new versions of the notebook\n",
    "jovian.commit(project=\"yahoo-finance-web-scraper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SWl2eoMlrgk"
   },
   "source": [
    "## 1. Scrape Stock Market News\n",
    "\n",
    "In this section we will learn a basic Python web scraping technique using `requests`, `BeautifulSoup` and `HTML tags`. The objective here is to perform web scraping of [Yahoo! finance Stock Market News](https://finance.yahoo.com/topic/stock-market-news/)\n",
    "\n",
    "![](https://i.imgur.com/1I0Btau.jpg)\n",
    "\n",
    "Let's kick-start with the first objective. Here's an outline of the steps we'll follow<br>\n",
    "**1.1 Download & Parse web page using `requests` and `BeautifulSoup`**<br>\n",
    "**1.2 Exploring and locating Elements**<br>\n",
    "**1.3 Extract & Compile the information into python list**<br>\n",
    "**1.4 Save the extracted information to a CSV file**<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSis_3UJlrgl"
   },
   "source": [
    "### 1.1 Download & Parse webpage using requests and BeautifulSoup\n",
    "\n",
    "First step is to install [`requests`](https://docs.python-requests.org/en/latest/) & [`beautifulsoup4`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) Libraries using `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 11167,
     "status": "ok",
     "timestamp": 1646608901448,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "zXJKsNyQlrgl"
   },
   "outputs": [],
   "source": [
    "!pip install requests --quiet\n",
    "!pip install beautifulsoup4 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 175,
     "status": "ok",
     "timestamp": 1646608901616,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "L_gdMI8llrgm"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1d4YUeDlrgm"
   },
   "source": [
    "The libraries are installed and imported.<br>\n",
    "\n",
    "To download the page, we can use `requests.get`, which returns a response object. the HTML information of a web page is captured in `response.text`.<br>\n",
    "`response.ok` & [`response.status_code`](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status) can be used for error trapping &  tracking.<br> \n",
    "Finally, we can use `BeautifulSoup` to parse the HTML data. This will return `bs4.BeautifulSoup` object. \n",
    "\n",
    "We can create a function to perform this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1646608901616,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "-FIURHiqlrgm"
   },
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    \"\"\"Download a webpage and return a beautiful soup doc\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if not response.ok:\n",
    "        print('Status code:', response.status_code)\n",
    "        raise Exception('Failed to load page {}'.format(url))\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "susW06Hwlrgn"
   },
   "source": [
    "calling function `get_page` and analyzing the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 737,
     "status": "ok",
     "timestamp": 1646608902349,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "cGWBiRCJlrgn"
   },
   "outputs": [],
   "source": [
    "my_url = 'https://finance.yahoo.com/topic/stock-market-news/' \n",
    "doc = get_page(my_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1646608902349,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "jvwJU_ULlrgn",
    "outputId": "1c77f4bc-f6a4-4b07-e2f7-c8ab3abfc6e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of doc:  <class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "print('Type of doc: ',type(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqEwtKgTlrgn"
   },
   "source": [
    "You can access different properties of HTML web page from doc. Following example will display Title of the web page.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1646608902477,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "S0xvsoyelrgo",
    "outputId": "1e7e65ff-5772-4fde-8877-4ee80216308e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Latest Stock Market News</title>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.find('title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hbs8VJS9lrgo"
   },
   "source": [
    "We can use the function `get_page` to download any web page and parse it using beautiful soup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvSMJZeClrgo"
   },
   "source": [
    "### 1.2 Exploring and locating Elements\n",
    "Now its time to explore the elements to find the required data point from the web page. Web pages are written in a language called HTML (Hyper Text Markup Language).  HTML is a fairly simple language comprised of *tags*  (also called *nodes* or *elements*) e.g. `<a href=\"https://finance.yahoo.com/\" target=\"_blank\">Go to Yahoo! Finance</a>`. An HTML tag has three parts:\n",
    "\n",
    "\n",
    "\n",
    "1. **Name**: (`html`, `head`, `body`, `div`, etc.) Indicates what the tag represents and how a browser should interpret the information inside it.\n",
    "2. **Attributes**: (`href`, `target`, `class`, `id`, etc.) Properties of tag used by the browser to customize how a tag is displayed and decide what happens on user interactions.\n",
    "3. **Children**: A tag can contain some text or other tags or both between the opening and closing segments, e.g., `<div>Some content</div>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAFbXWN2lrgo"
   },
   "source": [
    "Let's inspect the webpage source code by right-clicking and selecting the \"Inspect\" option. First, we need to identify the tag which represents the news listing.\n",
    "\n",
    "![](https://i.imgur.com/pGwXU1J.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fr5ofFpFlrgo"
   },
   "source": [
    "In this case we can see the `<div>` tag having class name `\"Ov(h) Pend(44px) Pstart(25px)\"` is representing news listing. We can apply `find_all` method to grab this information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1646608902478,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "WF_cAVptlrgo"
   },
   "outputs": [],
   "source": [
    "div_tags = doc.find_all('div', {'class': \"Ov(h) Pend(44px) Pstart(25px)\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_Y4L3Kglrgp"
   },
   "source": [
    "Total elements in the `<div>` tag list matching with the numbers of news displaying on the webpage, so we are heading towards the right direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1646608902478,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "FniLi1nElrgp",
    "outputId": "b3190a45-5fec-44e5-a80f-491f23bbf8cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(div_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4acfugiWlrgp"
   },
   "source": [
    "Next step to inspect the individual `<div>` tag and try to find more information. I am using \"Visual Studio Code\", but you can use any tool as simple as notepad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1646608902479,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "paFm23Amlrgp",
    "outputId": "f66b328e-f19a-4d8f-cf76-fb5833592b89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"Ov(h) Pend(44px) Pstart(25px)\"><div class=\"C(#959595) Fz(11px) D(ib) Mb(6px)\">MarketWatch</div><h3 class=\"Mb(5px)\"><a class=\"js-content-viewer wafer-caas Fw(b) Fz(18px) Lh(23px) LineClamp(2,46px) Fz(17px)--sm1024 Lh(19px)--sm1024 LineClamp(2,38px)--sm1024 mega-item-header-link Td(n) C(#0078ff):h C(#000) LineClamp(2,46px) LineClamp(2,38px)--sm1024 not-isInStreamVideoEnabled\" data-uuid=\"a93c15a2-942d-3cb4-9dcf-52e8dc93c2e9\" data-wf-caas-prefetch=\"1\" data-wf-caas-uuid=\"a93c15a2-942d-3cb4-9dcf-52e8dc93c2e9\" href=\"/m/a93c15a2-942d-3cb4-9dcf-52e8dc93c2e9/mullen-automotive-stock.html\"><u class=\"StretchedBox\"></u>Mullen Automotive stock surges again, has now more than doubled since snapping record losing streak</a></h3><p class=\"Fz(14px) Lh(19px) Fz(13px)--sm1024 Lh(17px)--sm1024 LineClamp(2,38px) LineClamp(2,34px)--sm1024 M(0)\">Shares of Mullen Automotive Inc. charged 16.1% higher in very active morning trading Tuesday. The electric vehicle maker's stock has already traded in a range of up 6.6% at the intraday low of $1.13, to up 35.8% at the high of $1.44, as trading volume bulged to 85.7 million shares. On Monday, the stock rocketed 34.9% on volume of 257.9 million shares. The company has not immediately responded to a request to confirm that it has not issued a press release, as per its website, or filed anything wi</p></div>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_tags[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaY4rqHxlrgp"
   },
   "source": [
    "![](https://i.imgur.com/ncnfg0z.png)\n",
    "\n",
    "Luckily, most of the required data points are available in this `<div>`, so we can use `find` method to grab each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1646608902479,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "Hn02jUnUlrgp",
    "outputId": "61cb85f9-1db6-43bf-df7d-817724911115"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:  MarketWatch\n",
      "Head Line : Mullen Automotive stock surges again, has now more than doubled since snapping record losing streak\n"
     ]
    }
   ],
   "source": [
    "print(\"Source: \", div_tags[1].find('div').text)\n",
    "print(\"Head Line : {}\".format(div_tags[1].find('a').text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54ry5o3-lrgp"
   },
   "source": [
    "If any tag is not accessible directly, then you can use methods like `findParent()` or `'findChild()` to point to the required tag.\n",
    "\n",
    "![](https://i.imgur.com/OnOAtT2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 344,
     "status": "ok",
     "timestamp": 1646608902819,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "BYm7cqy2lrgp",
    "outputId": "346f6220-9189-4158-bd6b-3ede2fc5309b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image URL:  https://s.yimg.com/uu/api/res/1.2/KVZNH4zA8LdDg6Rls3UodQ--~B/Zmk9c3RyaW07aD0xMjM7cT04MDt3PTIyMDthcHBpZD15dGFjaHlvbg--/https://s.yimg.com/uu/api/res/1.2/S8SCotpO9m2XQyQUIGU5dw--~B/aD02MzA7dz0xMjAwO2FwcGlkPXl0YWNoeW9u/https://media.zenfs.com/en/marketwatch.com/96d076434b2a015a3010bd2787b8c856.cf.jpg\n"
     ]
    }
   ],
   "source": [
    "print(\"Image URL: \",div_tags[1].findParent().find('img')['src'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_rZ3hP_lrgq"
   },
   "source": [
    "Key Takeout from this exercise is to identify the optimal tag which will provide us required information. Mostly this is straight forward, but sometimes you will have to perform a little more research.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylJlxrEhlrgq"
   },
   "source": [
    "### 1.3 Extract & Compile the information into python list\n",
    "\n",
    "We've identified all the required tags and information. Let's put this together in the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1646608902820,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "Ck_Buvi8lrgr"
   },
   "outputs": [],
   "source": [
    "def get_news_tags(doc):\n",
    "    \"\"\"Get the list of tags containing news information\"\"\"\n",
    "    news_class = \"Ov(h) Pend(44px) Pstart(25px)\" ## class name of div tag \n",
    "    news_list  = doc.find_all('div', {'class': news_class})\n",
    "    return news_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u93jXLLslrgr"
   },
   "source": [
    "sample run of the function `get_news_tags`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1646608902820,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "byS55R-9lrgr"
   },
   "outputs": [],
   "source": [
    "my_news_tags = get_news_tags(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfXnnMASlrgr"
   },
   "source": [
    "we will create one more function, to parse individual `<div>` tags and return the information in dictionary form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1646608902820,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "bsuD1-O4lrgr"
   },
   "outputs": [],
   "source": [
    "BASE_URL = 'https://finance.yahoo.com' #Global Variable \n",
    "\n",
    "def parse_news(news_tag):\n",
    "    \"\"\"Get the news data point and return dictionary\"\"\"\n",
    "    news_source = news_tag.find('div').text #source\n",
    "    news_headline = news_tag.find('a').text #heading\n",
    "    news_url = news_tag.find('a')['href'] #link\n",
    "    news_content = news_tag.find('p').text #content\n",
    "    news_image = news_tag.findParent().find('img')['src'] #thumb image\n",
    "    return { 'source' : news_source,\n",
    "            'headline' : news_headline,\n",
    "            'url' : BASE_URL + news_url,\n",
    "            'content' : news_content,\n",
    "            'image' : news_image\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q88hBshllrgr"
   },
   "source": [
    "Testing the `parse_news` function for first `<div>` tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1646608902820,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "7gO3YYf2lrgr",
    "outputId": "4e22e5f9-a830-4747-ce31-786682e3efbf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'Barrons.com',\n",
       " 'headline': 'After Plunging 13%, Kohl’s Stock Is Holding Steady Today. Thank the Many Suitors.',\n",
       " 'url': 'https://finance.yahoo.com/m/5fc6fadf-5033-3e9d-a493-65b6e4c0a429/after-plunging-13-kohl%E2%80%99s.html',\n",
       " 'content': 'Kohl’s  stock got a reprieve early Tuesday after the department-store chain disclosed late Monday that it had attracted more than 20 potential suitors before ultimately rejecting takeover bids.  The move, coming after yesterday’s 13% plunge, demonstrates how many investors are still hoping for a sale, although it wasn’t enough to keep the stock from sliding again.  Kohl’s (ticker: KSS) said in a regulatory filing that advisor Goldman Sachs had talked to more than 20 interested parties since the start of the year, and that some had gone as far as to sign confidentiality agreements that allowed them to access some of Kohl’s financial data.',\n",
       " 'image': 'https://s.yimg.com/uu/api/res/1.2/tfORcfSINUzHP6M38rxJJA--~B/Zmk9c3RyaW07aD0xMjM7cT04MDt3PTIyMDthcHBpZD15dGFjaHlvbg--/https://s.yimg.com/uu/api/res/1.2/G6KcX0CyEWMQCWKBwskhQw--~B/aD02NDA7dz0xMjgwO2FwcGlkPXl0YWNoeW9u/https://media.zenfs.com/en/Barrons.com/3f06d1f81d4b635221781cf4ebfdf1f7.cf.jpg'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_news(my_news_tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4Jt8PhXlrgr"
   },
   "source": [
    "We can use the `get_news_tags` & `parse_news` functions to pars news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ruE_5Q2lrgs"
   },
   "source": [
    "### 1.4 Save the extracted information to a CSV file\n",
    "\n",
    "This is the last step of this section. We are going to use Python library [`pandas`](https://pandas.pydata.org/docs/) to save the data in CSV format. Install and then import the pandas Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 5885,
     "status": "ok",
     "timestamp": 1646608908701,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "fFiWEPAalrgs"
   },
   "outputs": [],
   "source": [
    "!pip install pandas --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 522,
     "status": "ok",
     "timestamp": 1646608909203,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "zMrbCInvlrgs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JoZJq6GTlrgs"
   },
   "source": [
    "Creating wrapper function which will call previously created helper functions.<br>\n",
    "\n",
    "The `get_page` function will download HTML page, then we can pass the result in `get_news_tags` to identify list of `<div>` tags for news.<br>\n",
    "After that we will use [List Comprehension](https://www.w3schools.com/python/python_lists_comprehension.asp) technique to parse each `<div>` tag using `parse_news`, the output will be in the form of `lists` of `dictionaries`<br>\n",
    "Finally, we will use `DataFrame` method to create pandas [dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) and use `to_csv` method to store required data in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1646608909204,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "8evbvhJklrgs"
   },
   "outputs": [],
   "source": [
    "def scrape_yahoo_news(url, path=None):\n",
    "    \"\"\"Get the yahoo finance market news and write them to CSV file \"\"\"\n",
    "    if path is None:\n",
    "        path = 'stock-market-news.csv'\n",
    "        \n",
    "    print('Requesting html page')\n",
    "    doc = get_page(url)\n",
    "\n",
    "    print('Extracting news tags')\n",
    "    news_list = get_news_tags(doc)\n",
    "\n",
    "    print('Parsing news tags')\n",
    "    news_data = [parse_news(news_tag) for news_tag in news_list]\n",
    "\n",
    "    print('Save the data to a CSV')\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    news_df.to_csv(path, index=None)\n",
    "    \n",
    "    #This return statement is optional, we are doing this just analyze the final output \n",
    "    return news_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGYzOftflrgs"
   },
   "source": [
    "Scraping the news using `scrape_yahoo_news` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 192,
     "status": "ok",
     "timestamp": 1646608909391,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "ZhSeuZ6Pi8MD",
    "outputId": "3c77f53c-62db-42f5-8764-b8bd67eee639"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting html page\n",
      "Extracting news tags\n",
      "Parsing news tags\n",
      "Save the data to a CSV\n"
     ]
    }
   ],
   "source": [
    "YAHOO_NEWS_URL = BASE_URL+'/topic/stock-market-news/'\n",
    "news_df = scrape_yahoo_news(YAHOO_NEWS_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBD6nB50lrgs"
   },
   "source": [
    "The \"stock-market-news.csv\" should be available in File $\\rightarrow$ Open Menu. You can download the file or directly open it on browser. Please verify the file content and compare it with the actual information available on the webpage.\n",
    "\n",
    "You can also check the data by grabbing a few rows from the data frame returned by the `scrape_yahoo_news` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 159,
     "status": "ok",
     "timestamp": 1646608909544,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "1segLgXIlrgs",
    "outputId": "fb2b059c-e12c-49db-88ae-ad1ab61c66c2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>headline</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barrons.com</td>\n",
       "      <td>After Plunging 13%, Kohl’s Stock Is Holding St...</td>\n",
       "      <td>https://finance.yahoo.com/m/5fc6fadf-5033-3e9d...</td>\n",
       "      <td>Kohl’s  stock got a reprieve early Tuesday aft...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/tfORcfSINUzH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MarketWatch</td>\n",
       "      <td>Mullen Automotive stock surges again, has now ...</td>\n",
       "      <td>https://finance.yahoo.com/m/a93c15a2-942d-3cb4...</td>\n",
       "      <td>Shares of Mullen Automotive Inc. charged 16.1%...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/KVZNH4zA8LdD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barrons.com</td>\n",
       "      <td>Betting on Biogen Stock Isn’t Worth the Risk, ...</td>\n",
       "      <td>https://finance.yahoo.com/m/7dc3be93-2f99-39c2...</td>\n",
       "      <td>Stifel analyst Paul Matteis downgraded shares ...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/vIvWMt7YnJF_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>Elon Musk Asks Judge to Block SEC Subpoena Ove...</td>\n",
       "      <td>https://finance.yahoo.com/news/musk-asks-judge...</td>\n",
       "      <td>(Bloomberg) -- Tesla Inc. Chief Executive Offi...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/tJPHZTI1k6SR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Investor's Business Daily</td>\n",
       "      <td>Rivian Stock: EV Maker Hikes Prices Ahead Of E...</td>\n",
       "      <td>https://finance.yahoo.com/m/5f46a16e-09a2-3126...</td>\n",
       "      <td>EV maker Rivian will report fourth-quarter ear...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/L5NKseG7T2NE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      source  \\\n",
       "0                Barrons.com   \n",
       "1                MarketWatch   \n",
       "2                Barrons.com   \n",
       "3                  Bloomberg   \n",
       "4  Investor's Business Daily   \n",
       "\n",
       "                                            headline  \\\n",
       "0  After Plunging 13%, Kohl’s Stock Is Holding St...   \n",
       "1  Mullen Automotive stock surges again, has now ...   \n",
       "2  Betting on Biogen Stock Isn’t Worth the Risk, ...   \n",
       "3  Elon Musk Asks Judge to Block SEC Subpoena Ove...   \n",
       "4  Rivian Stock: EV Maker Hikes Prices Ahead Of E...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://finance.yahoo.com/m/5fc6fadf-5033-3e9d...   \n",
       "1  https://finance.yahoo.com/m/a93c15a2-942d-3cb4...   \n",
       "2  https://finance.yahoo.com/m/7dc3be93-2f99-39c2...   \n",
       "3  https://finance.yahoo.com/news/musk-asks-judge...   \n",
       "4  https://finance.yahoo.com/m/5f46a16e-09a2-3126...   \n",
       "\n",
       "                                             content  \\\n",
       "0  Kohl’s  stock got a reprieve early Tuesday aft...   \n",
       "1  Shares of Mullen Automotive Inc. charged 16.1%...   \n",
       "2  Stifel analyst Paul Matteis downgraded shares ...   \n",
       "3  (Bloomberg) -- Tesla Inc. Chief Executive Offi...   \n",
       "4  EV maker Rivian will report fourth-quarter ear...   \n",
       "\n",
       "                                               image  \n",
       "0  https://s.yimg.com/uu/api/res/1.2/tfORcfSINUzH...  \n",
       "1  https://s.yimg.com/uu/api/res/1.2/KVZNH4zA8LdD...  \n",
       "2  https://s.yimg.com/uu/api/res/1.2/vIvWMt7YnJF_...  \n",
       "3  https://s.yimg.com/uu/api/res/1.2/tJPHZTI1k6SR...  \n",
       "4  https://s.yimg.com/uu/api/res/1.2/L5NKseG7T2NE...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptBKMG_Alrgt"
   },
   "source": [
    "**Summary** : Hopefully I was able to explain this simple but very powerful Python technique to scrape the Yahoo! finance market news. These steps can be used to scrape any web page. You just have to do a little research to identify the required `<tags>` and use relevant python methods to collect the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaOUKkUOlrgt"
   },
   "source": [
    "## 2. Scrape Cryptocurrencies\n",
    "\n",
    "In phase One we were able to scrape the [yahoo market news](https://finance.yahoo.com/topic/stock-market-news/) web page. However, if you've noticed, as we scroll down the webpage more news will appear at the bottom of the page. This is called dynamic page loading. The previous technique is a basic Python method useful to scrape static data, To scrape the dynamically loading data will use a different method called web scraping using **Selenium**. Let's move ahead with this topic. The goal of this section is to extract top listing [Crypto currencies](https://finance.yahoo.com/cryptocurrencies) from Yahoo! finance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuViJR0Elrgt"
   },
   "source": [
    "![](https://i.imgur.com/sF6k0Pk.jpg)\n",
    "\n",
    "\n",
    "Here's an outline of the steps we'll follow<br>\n",
    "**2.1 Introduction of selenium**<br>\n",
    "**2.2 Downloads & Installation**<br>\n",
    "**2.3 Install & Import libraries**<br>\n",
    "**2.4 Create Web Driver**<br>\n",
    "**2.5 Exploring and locating Elements**<br>\n",
    "**2.6 Extract & Compile the information into a python list**<br>\n",
    "**2.7 Save the extracted information to a CSV file**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvUPJgUEQWZo"
   },
   "source": [
    "### 2.1 Introduction of selenium\n",
    "\n",
    "**[Selenium](https://www.selenium.dev/)** is an open-source web-based automation tool. Python language and other languages are used with Selenium for testing as well as web scraping. Here we will use Chrome browser, but you can try on any browser.<br>\n",
    "\n",
    "**Why you should use Selenium?**\n",
    "- Clicking on buttons\n",
    "- Filling forms\n",
    "- Scrolling\n",
    "- Taking a screen-shot\n",
    "- Refreshing the page\n",
    "\n",
    "You can find proper documentation on selenium [here](https://selenium-python.readthedocs.io/)<br>\n",
    "\n",
    "The following methods will help to find elements in a webpage (these methods will return a list):\n",
    "- `find_elements_by_name`\n",
    "- `find_elements_by_xpath`\n",
    "- `find_elements_by_link_text`\n",
    "- `find_elements_by_partial_link_text`\n",
    "- `find_elements_by_tag_name`\n",
    "- `find_elements_by_class_name`\n",
    "- `find_elements_by_css_selector`\n",
    "\n",
    "In this tutorial we will use only `find_elements_by_xpath` and `find_elements_by_tag_name` You can find complete documentation of these methods [here](https://selenium-python.readthedocs.io/locating-elements.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5IXSU3_QWZs"
   },
   "source": [
    "### 2.2 Downloads & Installation \n",
    "\n",
    "Unlike the previous section, here we'll have to do some prep work to implement this method. We will need to install Selenium & proper web browser driver<br>\n",
    "\n",
    "If you are using **Google Colab** platform then execute following code to perform Initial installation. This piece of code `'google.colab' in str(get_ipython())` is used to identify the Google Colab platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35298,
     "status": "ok",
     "timestamp": 1646608944835,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "ot7iEH0QQWZs",
    "outputId": "b6f3b009-28b7-4d91-9da7-8a6b827ae66b"
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Google CoLab Installation')\n",
    "    !apt update --quiet\n",
    "    !apt install chromium-chromedriver --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kj9N6faQWZs"
   },
   "source": [
    "To run it on **Locally** you will need **Webdriver for Chrome** on your machine. You can download it from this link https://chromedriver.chromium.org/downloads and just copy the file in the folder where we will create the python file (No need of installation). But make sure that the driver‘s version matches the Chrome browser version installed on the local machine.\n",
    "\n",
    "![](https://i.imgur.com/FvQ586e.gif)\n",
    "![](https://i.imgur.com/wQbjRIU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGOzu85gQWZs"
   },
   "source": [
    "### 2.3 Install & Import libraries\n",
    "\n",
    "Installation of the required libraries. Please note that there are some platform-specific libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9542,
     "status": "ok",
     "timestamp": 1646608954376,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "WmDBSb_qQWZt",
    "outputId": "33872365-9988-453f-ed9b-dc2fb6840d89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "library Installation\n",
      "Not running on CoLab\n"
     ]
    }
   ],
   "source": [
    "print('library Installation')\n",
    "if 'google.colab' not in str(get_ipython()):\n",
    "    print('Not running on CoLab')\n",
    "    #!pip install webdriver-manager --upgrade --quiet\n",
    "else:\n",
    "    print('Running on CoLab')\n",
    "    \n",
    "!pip install selenium --quiet\n",
    "!pip install pandas --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pn1J-U1PQWZt"
   },
   "source": [
    "Once the Libraries installation is done, next step is to import all the required modules / libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1646608954377,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "PaHNd1hSQWZt",
    "outputId": "a159e25b-7ccd-44af-9bf9-af469657b81c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library Import\n",
      "Not running on CoLab\n",
      "Common Library Import\n"
     ]
    }
   ],
   "source": [
    "print('Library Import')\n",
    "if 'google.colab' not in str(get_ipython()):\n",
    "    print('Not running on CoLab')\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    #from webdriver_manager.chrome import ChromeDriverManager\n",
    "    import os\n",
    "else:\n",
    "    print('Running on CoLab')\n",
    "    \n",
    "print('Common Library Import')\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlfwVrnVQWZt"
   },
   "source": [
    "So all the necessary prep work is done. Let's  move ahead to implement this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ms87q9huQWZu"
   },
   "source": [
    "### 2.4 Create Web Driver\n",
    "\n",
    "In this step first we will create the instance of Chrome WebDriver using `webdriver.Chrome()` method. and then the `driver.get()` method will navigate to a page given by the URL. In this case also there is slight variation based on platform. Also passed `options` parameters for e.g. `--headless` option will load the driver in background. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1646608954377,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "cAn7GekCQWZu",
    "outputId": "1a01cc95-5eeb-4943-c921-3943dc78aedb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on CoLab\n"
     ]
    }
   ],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Running on CoLab')\n",
    "    def get_driver(url):\n",
    "        \"\"\"Return web driver\"\"\"\n",
    "        colab_options = webdriver.ChromeOptions()\n",
    "        colab_options.add_argument('--no-sandbox')\n",
    "        colab_options.add_argument('--disable-dev-shm-usage')\n",
    "        colab_options.add_argument('--headless')\n",
    "        driver = webdriver.Chrome(options=colab_options)\n",
    "        driver.get(url)\n",
    "        return driver\n",
    "else:\n",
    "    print('Not running on CoLab')\n",
    "    def get_driver(url):\n",
    "        \"\"\"Return web driver\"\"\"\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        chrome_options.add_argument('--headless')\n",
    "        #serv = Service(ChromeDriverManager().install())\n",
    "        serv = Service(os.getcwd()+'/chromedriver')\n",
    "        driver = webdriver.Chrome(options=chrome_options, service=serv)\n",
    "        driver.get(url)\n",
    "        return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnCtyw66QWZu"
   },
   "source": [
    "Test run of `get_driver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 4593,
     "status": "ok",
     "timestamp": 1646608958957,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "rd7DJYjaQWZu"
   },
   "outputs": [],
   "source": [
    "driver = get_driver('https://finance.yahoo.com/cryptocurrencies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOF6149XQWZu"
   },
   "source": [
    "### 2.5 Exploring and locating Elements\n",
    "\n",
    "This is almost similar step that we have done in phase 1. We will try to identify relevant information like `<tags>`, `class` , `XPath` etc from the web page. Right-click and select the \"Inspect\" to do further analysis.\n",
    "\n",
    "As the webpage showing cryptocurrency information in the Table form. We can grab the table header by using tag `<th>`, we will use find_elements by TAG to get the table headers. These headers can be used as columns for a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1646608959371,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "zR29b9xH8di9",
    "outputId": "23b77d94-c3dd-4ed8-997a-efed9779befb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbol\n",
      "Price (Intraday)\n"
     ]
    }
   ],
   "source": [
    "header = driver.find_elements(By.TAG_NAME, value= 'th')\n",
    "print(header[0].text)\n",
    "print(header[2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNZ0kwDhQWZv"
   },
   "source": [
    "Creating a helper function to get first 10 columns from header, we have used List comprehension with conditions. You can also check out usage of `enumerate` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 138,
     "status": "ok",
     "timestamp": 1646608959507,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "QTfDpiWrQWZv"
   },
   "outputs": [],
   "source": [
    "def get_table_header(driver):\n",
    "    \"\"\"Return Table columns in list form \"\"\"\n",
    "    header = driver.find_elements(By.TAG_NAME, value= 'th')\n",
    "    header_list = [item.text for index, item in enumerate(header) if index < 10]\n",
    "    return header_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqW7UgNwQWZv"
   },
   "source": [
    "Next we find out number of rows available in a Page, you can see table rows are placed in `<tr>` tag, we can capture the `XPath` by selection `<tr>` tag the Right Click $\\rightarrow$ Copy $\\rightarrow$ Copy XPath.\n",
    "\n",
    "![](https://i.imgur.com/DVAYMzY.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlfJ632vQWZv"
   },
   "source": [
    "So we get the  XPath value as `//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr[1]`, Let's use this with `find_element()` & `By.XPATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1646608959774,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "U3FVWjftQWZv",
    "outputId": "439acb74-631d-496e-cf3b-a3da959a6a13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BTC-USD\\nBitcoin USD 38,484.46 -551.90 -1.41% 730.344B 31.284B 31.284B 31.284B 18.978M'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt=driver.find_element(By.XPATH, value='//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr[1]').text\n",
    "txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6JcCdb1Trrp"
   },
   "source": [
    "Above `XPath` points to first row, we can get rid of row number part from XPath and use it with `find_elements` to get hold of all the available rows. Let's implement this in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1646608959898,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "dd9kJccjToHK"
   },
   "outputs": [],
   "source": [
    "def get_table_rows(driver):\n",
    "    \"\"\"Get number of rows available on the page \"\"\"\n",
    "    tablerows = len(driver.find_elements(By.XPATH, value='//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr'))\n",
    "    return tablerows    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 142,
     "status": "ok",
     "timestamp": 1646608960038,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "0v1phXVMRn2E",
    "outputId": "290bc4de-5360-4252-f5c2-54cc09dced85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "print(get_table_rows(driver))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A45Ekufq8di9"
   },
   "source": [
    "Similarly, we can take the XPath for any column value.\n",
    "\n",
    "![](https://i.imgur.com/aT3I3Ur.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0dHoGoVVzTv"
   },
   "source": [
    "This is the XPAth for a column `//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr[1]/td[2]`.<br>\n",
    "If you noticed the number after `tr` & `td` represents the `row_number` and `column_number`, we can check this with `find_element()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 367,
     "status": "ok",
     "timestamp": 1646608960404,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "1MNZ9bGilrgt",
    "outputId": "f5e1a1ae-485c-4350-8507-a3eed58b9a20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bitcoin USD'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.find_element(By.XPATH, value='//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr[1]/td[2]').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2KhgeZAW7d8"
   },
   "source": [
    "So we can change the `row_number` & `column_number` in `XPath` and loop it through row count and column count to get all the available column values. Let's generalize this and put it in a function. We will get the data for one row at a time and return column values in the form of a dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1646608960404,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "pEorS2flYikH"
   },
   "outputs": [],
   "source": [
    "def parse_table_rows(rownum, driver, header_list):\n",
    "    \"\"\"get the data for one row at a time and return column value in the form of dictionary\"\"\"\n",
    "    row_dictionary = {}\n",
    "    #time.sleep(1/3)\n",
    "    for index , item in enumerate(header_list):\n",
    "        time.sleep(1/20)\n",
    "        column_xpath = '//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr[{}]/td[{}]'.format(rownum, index+1)\n",
    "        row_dictionary[item] = driver.find_element(By.XPATH, value=column_xpath).text\n",
    "    return row_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96fzN44rXu-u"
   },
   "source": [
    "The Yahoo! Finance web page shows only 25 Cryptocurrencies per page and user will have to click `Next` button to load next set of crypto currencies. This is called **Pagination**. This is the main reason we are implementing selenium methods to handle events like pagination. you can perform multiple events like clicking, scrolling , refreshing etc. on a webpage using selenium methods.\n",
    "\n",
    "Now we will grab the `XPath` of `Next` button, find the element using `find_element` method, and after that we can perform click action using `.click()` method \n",
    "\n",
    "![](https://i.imgur.com/tCxQKfR.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 126,
     "status": "ok",
     "timestamp": 1646608960528,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "s45SMjxligLN"
   },
   "outputs": [],
   "source": [
    "button_element = driver.find_element(By.XPATH, value = '//*[@id=\"scr-res-table\"]/div[2]/button[3]')\n",
    "button_element.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osU8bvFYeIgu"
   },
   "source": [
    "In this section we have learned how to get required data points, and perform events on webpage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 130,
     "status": "ok",
     "timestamp": 1646608960657,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "J0CKtbmTf5aF"
   },
   "outputs": [],
   "source": [
    "driver.quit() #terminating driver from test runs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdpxUXQCd5LH"
   },
   "source": [
    "### 2.6 Extract & Compile the information into python list\n",
    "\n",
    "Let's put all the pieces in the puzzle, we will pass the integer `total_crypto` i.e. numbers of rows to be scraped (in this case 100 rows) in the function. Parse each row from the page and append the data in the `List` till the total parsed row count reach to `total_crypto`. In addition, we will perform `Next` button click if we are at the last row of the table. \n",
    "\n",
    "**Please Note** : Here to identify the `Next` button element we have used [WebDriverWait](https://www.selenium.dev/selenium/docs/api/java/org/openqa/selenium/support/ui/WebDriverWait.html) class instead of using `find_element()` method. In this technique we can pass some wait-time before grabbing the element. This type of implementation is done to avoid the [`StaleElementReferenceException`](https://stackoverflow.com/questions/27003423/staleelementreferenceexception-on-python-selenium).\n",
    "\n",
    "Code Sample:\n",
    "```\n",
    "element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"scr-res-table\"]/div[2]/button[3]')))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1646608960657,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "FgH4XUG9SNAD"
   },
   "outputs": [],
   "source": [
    "def parse_multiple_pages(driver, total_crypto):\n",
    "    \"\"\"Loop through each row, perform Next button click at the end of page \n",
    "    return total_crypto numbers of rows \n",
    "    \"\"\"\n",
    "    table_data = []\n",
    "    page_num = 1\n",
    "    is_scraping = True\n",
    "    header_list = get_table_header(driver)\n",
    "\n",
    "    while is_scraping:\n",
    "        table_rows = get_table_rows(driver)\n",
    "        print('Found {} rows on Page : {}'.format(table_rows, page_num))\n",
    "        print('Parsing Page : {}'.format(page_num))\n",
    "        table_data += [parse_table_rows(i, driver, header_list) for i in range (1, table_rows + 1)]\n",
    "        total_count = len(table_data)\n",
    "        print('Total rows scraped : {}'.format(total_count))\n",
    "        if total_count >= total_crypto:\n",
    "            print('Done Parsing..')\n",
    "            is_scraping = False\n",
    "        else:    \n",
    "            print('Clicking Next Button')\n",
    "            element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"scr-res-table\"]/div[2]/button[3]')))\n",
    "            element.click() \n",
    "            page_num += 1\n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZL49rJqd5aa"
   },
   "source": [
    "### 2.7 Save the extracted information to a CSV file.\n",
    "\n",
    "This is the last step of this section, we are creating a last function which will be the placeholder for all helper functions and at the and we will save the data in CSV format using `pd.to_csv` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1646608960657,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "a8AxKJLrYikH"
   },
   "outputs": [],
   "source": [
    "def scrape_yahoo_crypto(url, total_crypto, path=None):\n",
    "    \"\"\"Get the list of yahoo finance crypto-currencies and write them to CSV file \"\"\"\n",
    "    if path is None:\n",
    "        path = 'crypto-currencies.csv'\n",
    "    print('Creating driver')\n",
    "    driver = get_driver(url)    \n",
    "    table_data = parse_multiple_pages(driver, total_crypto)\n",
    "    driver.close()\n",
    "    print('Save the data to a CSV')\n",
    "    table_df = pd.DataFrame(table_data)\n",
    "    #print(table_df)\n",
    "    table_df.to_csv(path, index=None)\n",
    "    #This return statement is optional, we are doing this just analyze the final output \n",
    "    return table_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJc4JNBaSNAD"
   },
   "source": [
    "Time to scrape some cryptos!!! , we will scrape top 100 cryptos in Yahoo! Finance webpage by calling `scrape_yahoo_crypto` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 90871,
     "status": "ok",
     "timestamp": 1646609051527,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "1fEtHtL2jXxR",
    "outputId": "7a159beb-3cf1-40d5-b02c-33d5793ff853"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating driver\n",
      "Found 25 rows on Page : 1\n",
      "Parsing Page : 1\n",
      "Total rows scraped : 25\n",
      "Clicking Next Button\n",
      "Found 25 rows on Page : 2\n",
      "Parsing Page : 2\n",
      "Total rows scraped : 50\n",
      "Clicking Next Button\n",
      "Found 25 rows on Page : 3\n",
      "Parsing Page : 3\n",
      "Total rows scraped : 75\n",
      "Clicking Next Button\n",
      "Found 25 rows on Page : 4\n",
      "Parsing Page : 4\n",
      "Total rows scraped : 100\n",
      "Done Parsing..\n",
      "Save the data to a CSV\n"
     ]
    }
   ],
   "source": [
    "YAHOO_FINANCE_URL = BASE_URL+'/cryptocurrencies'\n",
    "TOTAL_CRYPTO = 100\n",
    "crypto_df = scrape_yahoo_crypto(YAHOO_FINANCE_URL, TOTAL_CRYPTO,'crypto-currencies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tf7cgOSVYikI"
   },
   "source": [
    "The \"crypto-currencies.csv\" should be available in File $\\rightarrow$ Open Menu. You can download the file or directly open it on browser. Please verify the file content and compare it with the actual information available on the webpage.\n",
    "\n",
    "You can also check the data by grabbing a few rows from the data frame returned by the `scrape_yahoo_crypto` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1646609051527,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "4-fahUk4SNAE",
    "outputId": "52809182-57ce-4ffc-a0ed-dd5c563877d2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Name</th>\n",
       "      <th>Price (Intraday)</th>\n",
       "      <th>Change</th>\n",
       "      <th>% Change</th>\n",
       "      <th>Market Cap</th>\n",
       "      <th>Volume in Currency (Since 0:00 UTC)</th>\n",
       "      <th>Volume in Currency (24Hr)</th>\n",
       "      <th>Total Volume All Currencies (24Hr)</th>\n",
       "      <th>Circulating Supply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BTC-USD</td>\n",
       "      <td>Bitcoin USD</td>\n",
       "      <td>38,484.46</td>\n",
       "      <td>-551.90</td>\n",
       "      <td>-1.41%</td>\n",
       "      <td>730.344B</td>\n",
       "      <td>31.284B</td>\n",
       "      <td>31.284B</td>\n",
       "      <td>31.284B</td>\n",
       "      <td>18.978M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ETH-USD</td>\n",
       "      <td>Ethereum USD</td>\n",
       "      <td>2,546.27</td>\n",
       "      <td>-68.98</td>\n",
       "      <td>-2.64%</td>\n",
       "      <td>305.237B</td>\n",
       "      <td>15.611B</td>\n",
       "      <td>15.611B</td>\n",
       "      <td>15.611B</td>\n",
       "      <td>119.876M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USDT-USD</td>\n",
       "      <td>Tether USD</td>\n",
       "      <td>1.0002</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>-0.01%</td>\n",
       "      <td>80.042B</td>\n",
       "      <td>64.294B</td>\n",
       "      <td>64.294B</td>\n",
       "      <td>64.294B</td>\n",
       "      <td>80.03B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BNB-USD</td>\n",
       "      <td>Binance Coin USD</td>\n",
       "      <td>379.93</td>\n",
       "      <td>-2.26</td>\n",
       "      <td>-0.59%</td>\n",
       "      <td>62.733B</td>\n",
       "      <td>2.064B</td>\n",
       "      <td>2.064B</td>\n",
       "      <td>2.064B</td>\n",
       "      <td>165.117M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USDC-USD</td>\n",
       "      <td>USD Coin USD</td>\n",
       "      <td>0.999509</td>\n",
       "      <td>-0.000524</td>\n",
       "      <td>-0.05%</td>\n",
       "      <td>52.556B</td>\n",
       "      <td>4.453B</td>\n",
       "      <td>4.453B</td>\n",
       "      <td>4.453B</td>\n",
       "      <td>52.582B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Symbol              Name Price (Intraday)     Change % Change Market Cap  \\\n",
       "0   BTC-USD       Bitcoin USD        38,484.46    -551.90   -1.41%   730.344B   \n",
       "1   ETH-USD      Ethereum USD         2,546.27     -68.98   -2.64%   305.237B   \n",
       "2  USDT-USD        Tether USD           1.0002    -0.0001   -0.01%    80.042B   \n",
       "3   BNB-USD  Binance Coin USD           379.93      -2.26   -0.59%    62.733B   \n",
       "4  USDC-USD      USD Coin USD         0.999509  -0.000524   -0.05%    52.556B   \n",
       "\n",
       "  Volume in Currency (Since 0:00 UTC) Volume in Currency (24Hr)  \\\n",
       "0                             31.284B                   31.284B   \n",
       "1                             15.611B                   15.611B   \n",
       "2                             64.294B                   64.294B   \n",
       "3                              2.064B                    2.064B   \n",
       "4                              4.453B                    4.453B   \n",
       "\n",
       "  Total Volume All Currencies (24Hr) Circulating Supply  \n",
       "0                            31.284B            18.978M  \n",
       "1                            15.611B           119.876M  \n",
       "2                            64.294B             80.03B  \n",
       "3                             2.064B           165.117M  \n",
       "4                             4.453B            52.582B  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crypto_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ion8RYbZ8di_"
   },
   "source": [
    "**Summary** : Hope you've enjoyed this tutorial. Selenium enables us to perform multiple actions on the web browser, which is really very handy for scraping different types of data from any webpage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-klaoqb8di_"
   },
   "source": [
    "## 3. Scrape Market Events Calendar\n",
    "\n",
    "This is the final segment of the tutorial in this section, we will learn how to extract embedded [JSON](https://www.w3schools.com/js/js_json_intro.asp) formatted data which can be easily converted to Python dictionary. Problem statement for section is to scrape date-wise market events from [Yahoo! finance](https://finance.yahoo.com/calendar)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EsDeulcAJAV"
   },
   "source": [
    "![](https://i.imgur.com/bKQoAjs.png)\n",
    "\n",
    "Here's an outline of the steps we'll follow<br>\n",
    "**3.1 Install & Import libraries**<br>\n",
    "**3.2 Download & Parse web page**<br>\n",
    "**3.3 Get Embedded Json data**<br>\n",
    "**3.4 Locating Json Keys**<br>\n",
    "**3.5 Pagination & Compiling the information into a python list**<br>\n",
    "**3.6 Save the extracted information to a CSV file**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30uQai53AJAV"
   },
   "source": [
    "### 3.1 Install & Import libraries\n",
    "\n",
    "First step to install and import Python Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "5h5BUiRoAJAV"
   },
   "outputs": [],
   "source": [
    "!pip install requests --quiet\n",
    "!pip install beautifulsoup4 --quiet\n",
    "!pip install pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "2m7QniEX8di_"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HV8zDkvJAJAV"
   },
   "source": [
    "### 3.2 Download & Parse web page\n",
    "\n",
    "This is exactly the same step that we've performed to download webpage in section 1.1, Here we have used [custom header](https://docs.python-requests.org/en/master/user/quickstart/#custom-headers) in `requests.get()`\n",
    "\n",
    "Most of the things are explained in section 1.1, creating the helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "qFhyZG558di_"
   },
   "outputs": [],
   "source": [
    "def get_event_page(scraper_url):\n",
    "    \"\"\"Download a webpage and return a beautiful soup doc\"\"\"\n",
    "    headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
    "                  \"(KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19582\"\n",
    "    }\n",
    "    response = requests.get(scraper_url, headers=headers)\n",
    "    if not response.ok:\n",
    "        print('Status code:', response.status_code)\n",
    "        raise Exception('Failed to fetch web page ' + scraper_url)\n",
    "    # Construct a beautiful soup document\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "qBYaeOWDAJAV"
   },
   "outputs": [],
   "source": [
    "doc = get_event_page('https://finance.yahoo.com/calendar/earnings?from=2022-02-27&to=2022-03-05&day=2022-02-28')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7TGB4SZAJAV"
   },
   "source": [
    "### 3.3 Get Embedded Json data\n",
    "\n",
    "\n",
    "In this step we will locate the Jason formated data, Open the web page and do Right Click $\\rightarrow$ View Page Source, If you scroll down to source page you will notice the [Json](https://www.w3schools.com/whatis/whatis_json.asp) formated data. Apparently this information is `<script>` tag which contains the following text `/* -- Data -- */`.\n",
    "\n",
    "![](https://i.imgur.com/2xlpNbw.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcePCvsCAJAV"
   },
   "source": [
    "We will use [Regular expressions](https://docs.python.org/3/library/re.html) to get text  inside `<script>` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "YkAMhCnRAJAW"
   },
   "outputs": [],
   "source": [
    "pattern = re.compile(r'\\s--\\sData\\s--\\s')\n",
    "#script_data = doc.find('script', text=pattern).text\n",
    "script_data = doc.find('script', text=pattern).contents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_t2zpCGAJAW"
   },
   "source": [
    "Further, the `Json` formated string has the first key as `context` and it ends at 12 characters from the end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "AecrY-WkAJAW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(function (root) {\n",
      "/* -- Data -- */\n",
      "root.App || (root.App = {});\n",
      "root.App.now = 1646754698535;\n",
      "root.App.main = {\"context\":{\"dispatcher\":{\"stores\":{\"P\n",
      "odal\":{\"strings\":1},\"tdv2-wafer-header\":{\"strings\":1},\"yahoodotcom-layout\":{\"strings\":1}}},\"options\":{\"defaultBundle\":\"td-app-finance\"}}}};\n",
      "}(this));\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(script_data[:150])\n",
    "print(script_data[-150:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiYPaUpPAJAW"
   },
   "source": [
    "So we can grab the Json string using Python slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "-WYEbgQWAJAW"
   },
   "outputs": [],
   "source": [
    "start  = script_data.find('context')-2\n",
    "json_text  = script_data[start:-12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "605zLEjAAJAW"
   },
   "source": [
    "Using `json.loads()`method to convert Jason string into Python Dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "ShAdpupKAJAW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_dictionary = json.loads(json_text)\n",
    "type(parsed_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUjp0BqkAJAW"
   },
   "source": [
    "Creating a function using above information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "rgcsfbSx8di_"
   },
   "outputs": [],
   "source": [
    "def get_json_dictionary(doc):\n",
    "    \"\"\"Get Json formated data in the form of Python Dictionary\"\"\"\n",
    "    pattern = re.compile(r'\\s--\\sData\\s--\\s')\n",
    "    script_data = doc.find('script', text=pattern).text\n",
    "    script_data = doc.find('script', text=pattern).contents[0]\n",
    "    \n",
    "    start  = script_data.find('context')-2\n",
    "    json_text  = script_data[start:-12]\n",
    "    \n",
    "    parsed_dictionary = json.loads(json_text)\n",
    "    return parsed_dictionary    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIolGWTkAJAW"
   },
   "source": [
    "### 3.4 Locating Json Keys\n",
    "\n",
    "So basically the Json text is multi level nested dictionaries, and some keys are used to store all the meta data displayed on the webpage. In this section we will identify the keys for the data we are trying to scrape.\n",
    "\n",
    "We'll need some `Json Formatter` tool to navigate through multiple keys, I am using online tool https://jsonblob.com/. However, you can choose any tool.\n",
    "\n",
    "We will write the Json text into `my_json_file.json` file, then grab the file content and paste it to the left panel of https://jsonblob.com/. The JSON Blob it will do nice formatting. We can easily navigate through each Keys and search any item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "SUWFo_MG8di_"
   },
   "outputs": [],
   "source": [
    "with open('my_json_file.json', 'w') as file:\n",
    "    file.write(json_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8FwV_y1AJAW"
   },
   "source": [
    "Next step is to find the Required Key location. Let's search the company name `3D Systems Corporation` displayed in the webpage in the [JSON Blob](https://jsonblob.com/) formatter.\n",
    "![](https://i.imgur.com/Iv8b7vl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XklVfiv-AJAW"
   },
   "source": [
    "![](https://i.imgur.com/jpJYOCy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKY1688xAJAX"
   },
   "source": [
    "You can see the table data is stored in the `rows` key, and we can track down the parent keys as shown in the above screen, checkout the content of `row` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "00fa9Sx9AJAX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ticker': 'DDD',\n",
       "  'companyshortname': '3D Systems Corporation',\n",
       "  'startdatetime': '2022-02-28T16:05:00.000Z',\n",
       "  'startdatetimetype': 'TAS',\n",
       "  'epsestimate': 0.03,\n",
       "  'epsactual': 0.09,\n",
       "  'epssurprisepct': 181.25,\n",
       "  'timeZoneShortName': 'EST',\n",
       "  'gmtOffsetMilliSeconds': -18000000,\n",
       "  'quoteType': 'EQUITY'},\n",
       " {'ticker': 'WEIIF',\n",
       "  'companyshortname': 'Wolverine Energy and Infrastructure Inc.',\n",
       "  'startdatetime': '2022-02-28T16:35:00.000Z',\n",
       "  'startdatetimetype': 'TAS',\n",
       "  'epsestimate': -0.03,\n",
       "  'epsactual': -0.09,\n",
       "  'epssurprisepct': -180.65,\n",
       "  'timeZoneShortName': 'EST',\n",
       "  'gmtOffsetMilliSeconds': -18000000,\n",
       "  'quoteType': 'EQUITY'},\n",
       " {'ticker': 'GCP',\n",
       "  'companyshortname': 'GCP Applied Technologies Inc.',\n",
       "  'startdatetime': '2022-02-28T19:00:00.000Z',\n",
       "  'startdatetimetype': 'TAS',\n",
       "  'epsestimate': 0.18,\n",
       "  'epsactual': 0.12,\n",
       "  'epssurprisepct': -33.33,\n",
       "  'timeZoneShortName': 'EST',\n",
       "  'gmtOffsetMilliSeconds': -18000000,\n",
       "  'quoteType': 'EQUITY'}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_dictionary['context']['dispatcher']['stores']['ScreenerResultsStore']['results']['rows'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "mnq4KHKXAJAX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows on the Current page : 100\n"
     ]
    }
   ],
   "source": [
    "print('Total Rows on the Current page :',len(parsed_dictionary['context']['dispatcher']['stores']['ScreenerResultsStore']['results']['rows']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6khD1Pn6AJAX"
   },
   "source": [
    "This sub-dictionary shows all the data displayed on the current page.<br>\n",
    "You can do more research and exploration to get different information from the web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "DcGw1gDRAJAX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows for the search criteria : 215\n"
     ]
    }
   ],
   "source": [
    "print('Total Rows for the search criteria :',parsed_dictionary['context']['dispatcher']['stores']['ScreenerResultsStore']['results']['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "I7_WMaROAJAX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'data': 'ticker', 'content': 'Symbol'},\n",
       " {'data': 'companyshortname', 'content': 'Company Name'},\n",
       " {'data': 'startdatetime', 'content': 'Event Start Date'},\n",
       " {'data': 'startdatetimetype', 'content': 'Event Start Time'},\n",
       " {'data': 'epsestimate', 'content': 'EPS Estimate'},\n",
       " {'data': 'epsactual', 'content': 'Reported EPS'},\n",
       " {'data': 'epssurprisepct', 'content': 'Surprise (%)'},\n",
       " {'data': 'timeZoneShortName', 'content': 'Timezone short name'},\n",
       " {'data': 'gmtOffsetMilliSeconds', 'content': 'GMT Offset'}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Columns\")\n",
    "parsed_dictionary['context']['dispatcher']['stores']['ScreenerResultsStore']['results']['columns']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbgxoTdPAJAX"
   },
   "source": [
    "Putting this in a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "wyX1dtPy8di_"
   },
   "outputs": [],
   "source": [
    "def get_total_rows(parsed_dictionary):\n",
    "    '''Get the Total Rows for the search criteria & Columns detail''' \n",
    "    total_rows = parsed_dictionary['context']['dispatcher']['stores']['ScreenerResultsStore']['results']['total']\n",
    "    return total_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "435fS9Vv8di_"
   },
   "outputs": [],
   "source": [
    "def get_page_rows(parsed_dictionary):\n",
    "    \"\"\"Get the Content current page\"\"\"    \n",
    "    data_dictionary = parsed_dictionary['context']['dispatcher']['stores']['ScreenerResultsStore']['results']['rows']\n",
    "    return data_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoJ-lzeNAJAX"
   },
   "source": [
    "### 3.5 Pagination & Compiling the information into python list\n",
    "\n",
    "As we saw in the previous section on how to handle `Pagination` using selenium methods, here we'll learn a new technique for accessing multiple pages.<br>\n",
    "\n",
    "Most of the times webpage url gets changed at runtime depending on the user selection, e.g. In the below screen-shot, I selected the **Earnings** for **1-March-2022**. You can notice how that information is passed in the URL. \n",
    "![](https://i.imgur.com/h5QU99h.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8g-j5xJAJAX"
   },
   "source": [
    "Similarly, when i click next button, `offset`& `size` values gets changed in the url.\n",
    "![](https://i.imgur.com/jYa1vq5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5LQ_YkvAJAX"
   },
   "source": [
    "So we can figure out the pattern & structure of the url and how it affects page navigation.<br> \n",
    "\n",
    "In this case webpage url pattern is mentioned below:<br>\n",
    "- The following values are used for calendar event types \n",
    "`event_types = ['splits','economic','ipo','earnings']`\n",
    "- Date passed in `yyyy-mm-dd` format\n",
    "- Page number is controlled by `offset` value (for first page `offset=0`)\n",
    "- Maximum number of rows in a page is assigned to `size`\n",
    "\n",
    "Based on the above information, we can build the URL at runtime and download the page, then extract the information. This is how we handle pagination.<br>\n",
    "\n",
    "Putting all things together in a function. In this function we will pass `event_type` and `date`, then we will calculate the total rows for matching criteria using `get_columns_and_total_rows` function. Maximum rows per page are constant (i.e. 100), so we can build iterating summation logic to calculate the total number of pages involved for the current criteria and extract each page data in the loop.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "3magUBX48di_"
   },
   "outputs": [],
   "source": [
    "def scrape_all_pages(event_type, date):\n",
    "    \"\"\"Loop through each row and return lists of data dictiionary\"\"\"\n",
    "    YAHOO_CAL_URL = BASE_URL+'/calendar/{}?day={}&offset={}&size={}'\n",
    "    max_rows_per_page = '100' # this indicates max rows per page \n",
    "    page_number = 1\n",
    "    final_data_dictionary = []\n",
    "    \n",
    "    while page_number > 0:\n",
    "        print(\"Processing page # {}\".format(page_number))\n",
    "        page_url = str((page_number - 1 ) * int(max_rows_per_page))\n",
    "        scrape_url = YAHOO_CAL_URL.format(event_type, date, page_url, max_rows_per_page)\n",
    "        print(\"Scrape url for page {} is {}\".format(page_number,scrape_url))\n",
    "        page_doc = get_event_page(scrape_url)\n",
    "        parse_dict = get_json_dictionary(page_doc)\n",
    "        if page_number == 1:\n",
    "            total_rows = get_total_rows(parse_dict)        \n",
    "        final_data_dictionary += get_page_rows(parse_dict)\n",
    "        if len(final_data_dictionary) >= total_rows:\n",
    "            page_number = 0\n",
    "            return final_data_dictionary\n",
    "        page_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scLr8_DFAJAY"
   },
   "source": [
    "### 3.6 Save the extracted information to a CSV file.\n",
    "\n",
    "In this last section, we will save the data to csv format using `pd.DataFrame()` & `to_csv()` and call everything in a single placeholder function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "VLfoy_33QWZz"
   },
   "outputs": [],
   "source": [
    "def scrape_yahoo_calendar(event_types, date_param):\n",
    "    \"\"\"Get the list of yahoo finance calendar and write them to CSV file \"\"\"\n",
    "    for event in event_types:\n",
    "        data_dict = {}\n",
    "        print('Web Scraping for ', event  )\n",
    "        data_dict = scrape_all_pages(event, date_param)\n",
    "        if len(data_dict) > 0:\n",
    "            scraped_df = pd.DataFrame(data_dict)\n",
    "            scraped_df.to_csv(event+'_'+date_param+'.csv',index=False)\n",
    "            print(\"checking few rows.. for event : {} & date : {}\".format(event, date_param))\n",
    "            display(scraped_df.head())\n",
    "        else:\n",
    "            print(\"No data found for event : {} & date : {}\".format(event, date_param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cUgyY0IAJAY"
   },
   "source": [
    "calling the final function `scrape_yahoo_calendar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "WpvYTWyx8djA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web Scraping for  splits\n",
      "Processing page # 1\n",
      "Scrape url for page 1 is https://finance.yahoo.com/calendar/splits?day=2022-02-28&offset=0&size=100\n",
      "checking few rows.. for event : splits & date : 2022-02-28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>companyshortname</th>\n",
       "      <th>startdatetime</th>\n",
       "      <th>optionable</th>\n",
       "      <th>old_share_worth</th>\n",
       "      <th>share_worth</th>\n",
       "      <th>gmtOffsetMilliSeconds</th>\n",
       "      <th>quoteType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSRT</td>\n",
       "      <td>MassRoots Inc</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>False</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RI4.SG</td>\n",
       "      <td>POET Technologies Inc</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POETF</td>\n",
       "      <td>POET Technologies Inc</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TDRK</td>\n",
       "      <td>Tiderock Companies Inc</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>543272.BO</td>\n",
       "      <td>Easy Trip Planners Ltd</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker        companyshortname             startdatetime  optionable  \\\n",
       "0       MSRT           MassRoots Inc  2022-02-28T05:00:00.000Z       False   \n",
       "1     RI4.SG   POET Technologies Inc  2022-02-28T05:00:00.000Z       False   \n",
       "2      POETF   POET Technologies Inc  2022-02-28T05:00:00.000Z       False   \n",
       "3       TDRK  Tiderock Companies Inc  2022-02-28T05:00:00.000Z       False   \n",
       "4  543272.BO  Easy Trip Planners Ltd  2022-02-28T05:00:00.000Z       False   \n",
       "\n",
       "   old_share_worth  share_worth  gmtOffsetMilliSeconds quoteType  \n",
       "0              300            1                      0    EQUITY  \n",
       "1               10            1                      0    EQUITY  \n",
       "2               10            1                      0    EQUITY  \n",
       "3                1            4                      0    EQUITY  \n",
       "4                1            2                      0    EQUITY  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web Scraping for  economic\n",
      "Processing page # 1\n",
      "Scrape url for page 1 is https://finance.yahoo.com/calendar/economic?day=2022-02-28&offset=0&size=100\n",
      "checking few rows.. for event : economic & date : 2022-02-28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>econ_release</th>\n",
       "      <th>country_code</th>\n",
       "      <th>startdatetime</th>\n",
       "      <th>period</th>\n",
       "      <th>after_release_actual</th>\n",
       "      <th>consensus_estimate</th>\n",
       "      <th>prior_release_actual</th>\n",
       "      <th>originally_reported_actual</th>\n",
       "      <th>gmtOffsetMilliSeconds</th>\n",
       "      <th>quoteType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Construction Orders YY*</td>\n",
       "      <td>JP</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>Jan</td>\n",
       "      <td>11</td>\n",
       "      <td>None</td>\n",
       "      <td>4.8</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Housing Starts YY*</td>\n",
       "      <td>JP</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>Jan</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.7</td>\n",
       "      <td>4.2</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Retail Sales YY*</td>\n",
       "      <td>NL</td>\n",
       "      <td>2022-02-28T05:30:00.000Z</td>\n",
       "      <td>Jan</td>\n",
       "      <td>15.8</td>\n",
       "      <td>None</td>\n",
       "      <td>7.6</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pvt Sector Credit Ext.*</td>\n",
       "      <td>ZA</td>\n",
       "      <td>2022-02-28T06:00:00.000Z</td>\n",
       "      <td>Jan</td>\n",
       "      <td>3.12</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.58</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M3 Money Supply YY*</td>\n",
       "      <td>ZA</td>\n",
       "      <td>2022-02-28T06:00:00.000Z</td>\n",
       "      <td>Jan</td>\n",
       "      <td>5.65</td>\n",
       "      <td>5.85</td>\n",
       "      <td>5.71</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              econ_release country_code             startdatetime period  \\\n",
       "0  Construction Orders YY*           JP  2022-02-28T05:00:00.000Z    Jan   \n",
       "1       Housing Starts YY*           JP  2022-02-28T05:00:00.000Z    Jan   \n",
       "2         Retail Sales YY*           NL  2022-02-28T05:30:00.000Z    Jan   \n",
       "3  Pvt Sector Credit Ext.*           ZA  2022-02-28T06:00:00.000Z    Jan   \n",
       "4      M3 Money Supply YY*           ZA  2022-02-28T06:00:00.000Z    Jan   \n",
       "\n",
       "  after_release_actual consensus_estimate prior_release_actual  \\\n",
       "0                   11               None                  4.8   \n",
       "1                  2.1                1.7                  4.2   \n",
       "2                 15.8               None                  7.6   \n",
       "3                 3.12               2.85                 2.58   \n",
       "4                 5.65               5.85                 5.71   \n",
       "\n",
       "  originally_reported_actual  gmtOffsetMilliSeconds quoteType  \n",
       "0                       None                      0    EQUITY  \n",
       "1                       None                      0    EQUITY  \n",
       "2                        7.7                      0    EQUITY  \n",
       "3                       None                      0    EQUITY  \n",
       "4                       None                      0    EQUITY  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web Scraping for  ipo\n",
      "Processing page # 1\n",
      "Scrape url for page 1 is https://finance.yahoo.com/calendar/ipo?day=2022-02-28&offset=0&size=100\n",
      "checking few rows.. for event : ipo & date : 2022-02-28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>companyshortname</th>\n",
       "      <th>exchange_short_name</th>\n",
       "      <th>filingdate</th>\n",
       "      <th>startdatetime</th>\n",
       "      <th>amendeddate</th>\n",
       "      <th>pricefrom</th>\n",
       "      <th>priceto</th>\n",
       "      <th>offerprice</th>\n",
       "      <th>currencyname</th>\n",
       "      <th>shares</th>\n",
       "      <th>dealtype</th>\n",
       "      <th>gmtOffsetMilliSeconds</th>\n",
       "      <th>quoteType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VBOC</td>\n",
       "      <td>Viscogliosi Brothers Acquisition Corp Common S...</td>\n",
       "      <td>Nasdaq</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Expected</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CXAC</td>\n",
       "      <td>C5 Acquisition Corporation</td>\n",
       "      <td>NYSE</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>Expected</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COSM</td>\n",
       "      <td>Cosmos Holdings Inc. Common Stock</td>\n",
       "      <td>Nasdaq</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Expected</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VBOCW</td>\n",
       "      <td>Viscogliosi Brothers Acquisition Corp Warrant</td>\n",
       "      <td>Nasdaq</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Expected</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SCRMW</td>\n",
       "      <td>Screaming Eagle Acquisition Corp. Warrant</td>\n",
       "      <td>Nasdaq</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Expected</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker                                   companyshortname  \\\n",
       "0   VBOC  Viscogliosi Brothers Acquisition Corp Common S...   \n",
       "1   CXAC                         C5 Acquisition Corporation   \n",
       "2   COSM                  Cosmos Holdings Inc. Common Stock   \n",
       "3  VBOCW      Viscogliosi Brothers Acquisition Corp Warrant   \n",
       "4  SCRMW          Screaming Eagle Acquisition Corp. Warrant   \n",
       "\n",
       "  exchange_short_name filingdate             startdatetime amendeddate  \\\n",
       "0              Nasdaq       None  2022-02-28T05:00:00.000Z        None   \n",
       "1                NYSE       None  2022-02-28T05:00:00.000Z        None   \n",
       "2              Nasdaq       None  2022-02-28T05:00:00.000Z        None   \n",
       "3              Nasdaq       None  2022-02-28T05:00:00.000Z        None   \n",
       "4              Nasdaq       None  2022-02-28T05:00:00.000Z        None   \n",
       "\n",
       "   pricefrom  priceto  offerprice currencyname  shares  dealtype  \\\n",
       "0        NaN      NaN         NaN          USD     NaN  Expected   \n",
       "1        NaN      NaN         NaN                  NaN  Expected   \n",
       "2        NaN      NaN         NaN          USD     NaN  Expected   \n",
       "3        NaN      NaN         NaN          USD     NaN  Expected   \n",
       "4        NaN      NaN         NaN          USD     NaN  Expected   \n",
       "\n",
       "   gmtOffsetMilliSeconds quoteType  \n",
       "0                    0.0    EQUITY  \n",
       "1                    0.0    EQUITY  \n",
       "2                    0.0    EQUITY  \n",
       "3                    0.0    EQUITY  \n",
       "4                    0.0    EQUITY  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web Scraping for  earnings\n",
      "Processing page # 1\n",
      "Scrape url for page 1 is https://finance.yahoo.com/calendar/earnings?day=2022-02-28&offset=0&size=100\n",
      "Processing page # 2\n",
      "Scrape url for page 2 is https://finance.yahoo.com/calendar/earnings?day=2022-02-28&offset=100&size=100\n",
      "Processing page # 3\n",
      "Scrape url for page 3 is https://finance.yahoo.com/calendar/earnings?day=2022-02-28&offset=200&size=100\n",
      "checking few rows.. for event : earnings & date : 2022-02-28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>companyshortname</th>\n",
       "      <th>startdatetime</th>\n",
       "      <th>startdatetimetype</th>\n",
       "      <th>epsestimate</th>\n",
       "      <th>epsactual</th>\n",
       "      <th>epssurprisepct</th>\n",
       "      <th>timeZoneShortName</th>\n",
       "      <th>gmtOffsetMilliSeconds</th>\n",
       "      <th>quoteType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DDD</td>\n",
       "      <td>3D Systems Corporation</td>\n",
       "      <td>2022-02-28T16:05:00.000Z</td>\n",
       "      <td>TAS</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.09</td>\n",
       "      <td>181.25</td>\n",
       "      <td>EST</td>\n",
       "      <td>-18000000</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WEIIF</td>\n",
       "      <td>Wolverine Energy and Infrastructure Inc.</td>\n",
       "      <td>2022-02-28T16:35:00.000Z</td>\n",
       "      <td>TAS</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-180.65</td>\n",
       "      <td>EST</td>\n",
       "      <td>-18000000</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GCP</td>\n",
       "      <td>GCP Applied Technologies Inc.</td>\n",
       "      <td>2022-02-28T19:00:00.000Z</td>\n",
       "      <td>TAS</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-33.33</td>\n",
       "      <td>EST</td>\n",
       "      <td>-18000000</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ATRA</td>\n",
       "      <td>Atara Biotherapeutics, Inc.</td>\n",
       "      <td>2022-02-28T16:05:00.000Z</td>\n",
       "      <td>TAS</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>-39.94</td>\n",
       "      <td>EST</td>\n",
       "      <td>-18000000</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAON</td>\n",
       "      <td>AAON, Inc.</td>\n",
       "      <td>2022-02-28T16:01:00.000Z</td>\n",
       "      <td>TAS</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-35.02</td>\n",
       "      <td>EST</td>\n",
       "      <td>-18000000</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker                          companyshortname             startdatetime  \\\n",
       "0    DDD                    3D Systems Corporation  2022-02-28T16:05:00.000Z   \n",
       "1  WEIIF  Wolverine Energy and Infrastructure Inc.  2022-02-28T16:35:00.000Z   \n",
       "2    GCP             GCP Applied Technologies Inc.  2022-02-28T19:00:00.000Z   \n",
       "3   ATRA               Atara Biotherapeutics, Inc.  2022-02-28T16:05:00.000Z   \n",
       "4   AAON                                AAON, Inc.  2022-02-28T16:01:00.000Z   \n",
       "\n",
       "  startdatetimetype  epsestimate  epsactual  epssurprisepct timeZoneShortName  \\\n",
       "0               TAS         0.03       0.09          181.25               EST   \n",
       "1               TAS        -0.03      -0.09         -180.65               EST   \n",
       "2               TAS         0.18       0.12          -33.33               EST   \n",
       "3               TAS        -0.69      -0.96          -39.94               EST   \n",
       "4               TAS         0.28       0.18          -35.02               EST   \n",
       "\n",
       "   gmtOffsetMilliSeconds quoteType  \n",
       "0              -18000000    EQUITY  \n",
       "1              -18000000    EQUITY  \n",
       "2              -18000000    EQUITY  \n",
       "3              -18000000    EQUITY  \n",
       "4              -18000000    EQUITY  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BASE_URL = 'https://finance.yahoo.com' #Global Variable \n",
    "#date_param = '2022-03-18' # no data condition\n",
    "date_param = '2022-02-28'\n",
    "event_types = ['splits','economic','ipo','earnings']\n",
    "scrape_yahoo_calendar(event_types, date_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBjhZrB7AJAY"
   },
   "source": [
    "Total 4 csv files \"event_type_yyyy-mm-dd.csv\" should be available in File $\\rightarrow$ Open Menu. You can download the file or directly open it in a browser. Please verify the file content and compare it with the actual information available on the webpage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpW06QKr8djA"
   },
   "source": [
    "**Summary** : This is a very useful technique which can be easily replicable. Without writing any customized code, we were able to extract the data from multiple types of web pages just by changing one variable (in this case `event_type`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jA-c9qFnAJAY"
   },
   "source": [
    "## References\n",
    "\n",
    "References to some useful links.\n",
    "\n",
    "- https://htmldog.com/guides/html/\n",
    "- https://selenium-python.readthedocs.io/index.html\n",
    "- https://stackoverflow.com/questions/27003423/staleelementreferenceexception-on-python-selenium\n",
    "- https://www.w3schools.com/js/js_json_intro.asp\n",
    "- https://hhsm95.dev/blog/the-importance-of-using-user-agent-to-scraping-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIcjbAgOAJAY"
   },
   "source": [
    "## Future Work\n",
    "\n",
    "Ideas for future work<br>\n",
    "- Automate this process using [AWS Lambda](https://aws.amazon.com/lambda/) to download daily market calendar, crypto-currencies & market news in CSV format.\n",
    "- Move the old files to an Archive folder append date-stamp to the file if required, also  delete the Archived files older than 2 weeks.\n",
    "- Process the raw data extracted from third technique using different methods of pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkY7RsOeAJAY"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial we implement the following web scraping techniques.\n",
    " - Use requests, BeautifulSoup and HTML tags to extract web page.\n",
    " - Use Selenium to scrape data from dynamically loading websites.\n",
    " - Use embedded JSON data to scrape website.\n",
    "\n",
    "I hope I was able to teach you these webscraping methods and I hope you can use this knowledge to scrape any website.\n",
    "\n",
    "Thank you for reading. Happy coding!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "7aSVLdJ1lrgw"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"vinodvidhole/yahoo-finance-web-scraper\" on https://jovian.ai/\u001b[0m\n",
      "[jovian] Uploading additional files...\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/vinodvidhole/yahoo-finance-web-scraper\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/vinodvidhole/yahoo-finance-web-scraper'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project=\"yahoo-finance-web-scraper\",files=['stock-market-news.csv','crypto-currencies.csv','splits_2022-02-28.csv','economic_2022-02-28.csv','ipo_2022-02-28.csv','earnings_2022-02-28.csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2huL77IORxre"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"vinodvidhole/yahoo-finance-web-scraper\" on https://jovian.ai/\u001b[0m\n",
      "[jovian] Uploading additional files...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m[jovian] Error: File upload failed: (HTTP 400) Invalid filename or extension (chrome_headless_lambda_layer.sh)\u001b[0m\n",
      "\u001b[31m[jovian] Error: File upload failed: (HTTP 400) Invalid filename or extension (README.md)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Committed successfully! https://jovian.ai/vinodvidhole/yahoo-finance-web-scraper\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/vinodvidhole/yahoo-finance-web-scraper'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project=\"yahoo-finance-web-scraper\",files=['yahooscraper.py','lambda_function.py','chrome_headless_lambda_layer.sh','README.md'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "DvSMJZeClrgo"
   ],
   "name": "yahoo-finance-web-scraper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
