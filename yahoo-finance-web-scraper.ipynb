{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VUU_94gDRYWm"
   },
   "outputs": [],
   "source": [
    "# Jovian Commit Essentials\n",
    "# Please retain and execute this cell without modifying the contents for `jovian.commit` to work\n",
    "!pip install jovian -q\n",
    "import jovian\n",
    "jovian.set_project('yahoo-finance-web-scraper')\n",
    "jovian.set_colab_id('1BRmuZI_aFQWd62cRys6QUgaxAnGrLT6U')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pF8beWv0lrgc"
   },
   "source": [
    "# Web Scraping Yahoo! Finance using multiple techniques in Python\n",
    "\n",
    "A detailed guide for scraping https://finance.yahoo.com/ using ***requests***, ***BeautifulSoup***, ***selenium***, ***HTML tags*** & existing available data in ***json*** format.\n",
    "\n",
    "![](https://imgur.com/7jMFOcE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FULGhEjlrge"
   },
   "source": [
    "**What is Web scraping?**\n",
    "\n",
    "Web scraping is the process of extracting and parsing data from websites in an automated fashion using a computer program. It's a useful technique for creating datasets for research and learning.\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "The main objective of this tutorial is to showcase different web scraping methods which can be applied to any web page. \n",
    "This is for educational purposes only. Please read the Terms & Conditions carefully for any website whether you can legally use the data. \n",
    "\n",
    "In this Project we will perform web scraping using following 3 techniques based on the problem statement.\n",
    "* use `BeautifulSoup` and `HTML tags` to extract web page\n",
    "* use `selenium` to scrape data from dynamically loading websites \n",
    "* scrape data using existing data available in `json` format\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CFXNNENlrgf"
   },
   "source": [
    "**The problem statement**\n",
    "\n",
    "1. Scrape **Stock Market News** (url : https://finance.yahoo.com/topic/stock-market-news/) :<br>\n",
    "    This web page shows latest **news** related to **stock market**, we will try to extract data from this web page and store it in `CSV` (comma-separated values) file. The file layout would be as mentioned below.\n",
    "    ```\n",
    "    source,headline,url,content,image\n",
    "    <source of the news>,<news head line>,<news url>,<news content>,<news thumbnail image>\n",
    "    ```\n",
    "\n",
    "## TODO change this ##\n",
    "2. Scrape **Trending Tickers** (url : https://finance.yahoo.com/trending-tickers) :<br>\n",
    "    This yahoo finance web page is showing list of trending **Tickers** in tabular format, we will perform the web scraping to retrieve first 8 columns for all available **Tickers** in `CSV` format.\n",
    "    ```\n",
    "    Symbol,Name,Last Price,Market Time,Change,% Change,Volume,Market Cap\n",
    "    COKE,\"Coca-Cola Consolidated, Inc.\",446.66,4:00PM EST,-136.98,-23.47%,\"100,345\",4.187B\n",
    "    ```\n",
    "        \n",
    "3. Scrape **Market Events Calendar** (url : https://finance.yahoo.com/calendar) :<br> \n",
    "    This page is showing **date-wise market events**, user have option to select the date and choose any one of the following market event **Earnings**, **Stock Splits**, **Economic Events** & **IPO**. Our aim is to create script which can be run for any single date and market event which grabs the data and load it in `CSV` format. If there is no data found then just create file with column headers.\n",
    "    \n",
    "\n",
    "\n",
    "**Future Work**\n",
    "\n",
    "Automate this process to get daily calendar , trending tickers & news in CSV files\n",
    "- create daily 6 files \n",
    "- move the old files in to archive folder with time stamp \n",
    "- delete older files files older than 2 weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Z9UQZf8lrgg"
   },
   "source": [
    "**Prerequisites**\n",
    "\n",
    "* Knowledge of Python\n",
    "* Basic knowledge of HTML although it is not necessary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbMRHIhblrgh"
   },
   "source": [
    "**How to run the Code**\n",
    "\n",
    "You can execute the code using \"Run\" button on the top of this page and selecting **\"Run on Colab\"** or **\"Run Locally\"** \n",
    "<br>\n",
    "<br>\n",
    "**Setup and Tools**\n",
    "\n",
    "<u>Run on Colab :</u> \n",
    "    You will need to provide the Google login to run this notebook on Colab.<br>\n",
    "\n",
    "<u>Run Locally :</u> Download and install [Anaconda](https://www.anaconda.com/) framework, We will be using Jupyter Notebook for writing the & executing code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YjeIdedlrgh"
   },
   "source": [
    "**Code Re-usability & Version control**\n",
    "\n",
    "You can make changes and save your version of the notebook to [Jovian](https://jovian.ai/) by executing following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iLLVpn2olrgi"
   },
   "outputs": [],
   "source": [
    "!pip install jovian --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iTKJFEYglrgj"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (window.IPython && IPython.notebook.kernel) IPython.notebook.kernel.execute('jovian.utils.jupyter.get_notebook_name_saved = lambda: \"' + IPython.notebook.notebook_name + '\"')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AxYOBC1xlrgj"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"vinodvidhole/yahoo-finance-web-scraper\" on https://jovian.ai/\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/vinodvidhole/yahoo-finance-web-scraper\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/vinodvidhole/yahoo-finance-web-scraper'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute this to save new versions of the notebook\n",
    "jovian.commit(project=\"yahoo-finance-web-scraper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SWl2eoMlrgk"
   },
   "source": [
    "## 1. Scrape Stock Market News \n",
    "\n",
    "## TODO Add image ?\n",
    "Lets kick start with the first objective. Here's an outline of the steps we'll follow<br>\n",
    "\n",
    "**1.1 Download & Parse webpage using `requests` and `BeautifulSoup`**<br>\n",
    "**1.2 Exploring and locating Elements**<br>\n",
    "**1.3 Extract & Compile the information into python list**<br>\n",
    "**1.4 Save the extracted information to a CSV file**<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSis_3UJlrgl"
   },
   "source": [
    "### 1.1 Download & Parse webpage using requests and BeautifulSoup\n",
    "\n",
    "First step is to install [`requests`](https://docs.python-requests.org/en/latest/) & [`beautifulsoup4`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) Libraries using `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zXJKsNyQlrgl"
   },
   "outputs": [],
   "source": [
    "!pip install requests --upgrade --quiet\n",
    "!pip install beautifulsoup4 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "L_gdMI8llrgm"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1d4YUeDlrgm"
   },
   "source": [
    "The library is now installed and imported.<br>\n",
    "\n",
    "To download the page, we can use `requests.get`, which returns a response object. the HTML information of web page is captured in `response.text`.<br>\n",
    "`response.ok` & [`response.status_code`](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status) can be used for error trapping &  tracking.<br> \n",
    "Finally we can use `BeautifulSoup` to parse the HTML data, this will return `bs4.BeautifulSoup` object  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rD6E93Trlrgm"
   },
   "source": [
    "Lets create a function to perform this step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-FIURHiqlrgm"
   },
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    \"\"\"Download a webpage and return a beautiful soup doc\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if not response.ok:\n",
    "        print('Status code:', response.status_code)\n",
    "        raise Exception('Failed to load page {}'.format(url))\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "susW06Hwlrgn"
   },
   "source": [
    "calling function `get_page` and analyze the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cGWBiRCJlrgn"
   },
   "outputs": [],
   "source": [
    "my_url = 'https://finance.yahoo.com/topic/stock-market-news/' #Global variable \n",
    "doc = get_page(my_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jvwJU_ULlrgn",
    "outputId": "a8117e45-6ae0-4f24-9748-c6693a3a24eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of doc:  <class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "print('Type of doc: ',type(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqEwtKgTlrgn"
   },
   "source": [
    "You can access different properties of HTML web page from doc, following example will display Title of the web page.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S0xvsoyelrgo",
    "outputId": "811e7de4-9fed-4226-8f23-50864df3750b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Latest Stock Market News</title>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.find('title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hbs8VJS9lrgo"
   },
   "source": [
    "**Summary** : We can now use the function `get_page` to download any web page and parse it using beautiful soup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvSMJZeClrgo"
   },
   "source": [
    "### 1.2 Exploring and locating Elements\n",
    "Now its time to explore the elements to find the required data point from the web page. Web pages are written in a language called HTML (Hyper Text Markup Language).  HTML is a fairly simple language comprised of *tags*  (also called *nodes* or *elements*) e.g. `<a href=\"https://finance.yahoo.com/\" target=\"_blank\">Go to Yahoo! Finance</a>`. An HTML tag has three parts:\n",
    "\n",
    "\n",
    "\n",
    "1. **Name**: (`html`, `head`, `body`, `div`, etc.) Indicates what the tag represents and how a browser should interpret the information inside it.\n",
    "2. **Attributes**: (`href`, `target`, `class`, `id`, etc.) Properties of tag used by the browser to customize how a tag is displayed and decide what happens on user interactions.\n",
    "3. **Children**: A tag can contain some text or other tags or both between the opening and closing segments, e.g., `<div>Some content</div>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAFbXWN2lrgo"
   },
   "source": [
    "Now lets inspect the webpage source code by right-click and select the \"Inspect\" option. First we need to identify the tag which represents the news listing.\n",
    "\n",
    "## TODO FIX BELOW "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C97r1w_5lrgo"
   },
   "source": [
    "![](https://media.giphy.com/media/RQpW64jdiQG8LNCGsZ/giphy.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fr5ofFpFlrgo"
   },
   "source": [
    "In this case we can see the `<div>` tag having class name `\"Ov(h) Pend(44px) Pstart(25px)\"` is representing news listing, we can apply `find_all` method to grab this information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "WF_cAVptlrgo"
   },
   "outputs": [],
   "source": [
    "div_tags = doc.find_all('div', {'class': \"Ov(h) Pend(44px) Pstart(25px)\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_Y4L3Kglrgp"
   },
   "source": [
    "Total elements in the `<div>` tag list is matching with the numbers of news displaying in the webpage , so we are heading towards right direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FniLi1nElrgp",
    "outputId": "422dca88-a49d-43fc-9af4-0dea6462ec83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(div_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4acfugiWlrgp"
   },
   "source": [
    "Next step to inspect the individual `<div>` tag and try to find more information. I am using \"Visual Studio Code\", but you can use any tool as simple as notepad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "paFm23Amlrgp",
    "outputId": "f82b9dc3-761c-49a4-d3ee-775a02382071"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"Ov(h) Pend(44px) Pstart(25px)\"><div class=\"C(#959595) Fz(11px) D(ib) Mb(6px)\">News Direct</div><h3 class=\"Mb(5px)\"><a class=\"js-content-viewer wafer-caas Fw(b) Fz(18px) Lh(23px) LineClamp(2,46px) Fz(17px)--sm1024 Lh(19px)--sm1024 LineClamp(2,38px)--sm1024 mega-item-header-link Td(n) C(#0078ff):h C(#000) LineClamp(2,46px) LineClamp(2,38px)--sm1024 not-isInStreamVideoEnabled\" data-uuid=\"bd999a4f-1a4b-3beb-b963-8035e1ef51fb\" data-wf-caas-prefetch=\"1\" data-wf-caas-uuid=\"bd999a4f-1a4b-3beb-b963-8035e1ef51fb\" href=\"/news/freshworks-alums-launch-growfin-targeting-081500142.html\"><u class=\"StretchedBox\"></u>Freshworks alums launch Growfin targeting the $125T global B2B payments market with a collaboration-first approach</a></h3><p class=\"Fz(14px) Lh(19px) Fz(13px)--sm1024 Lh(17px)--sm1024 LineClamp(2,38px) LineClamp(2,34px)--sm1024 M(0)\">Growfin’s AI-powered platform enables businesses to improve collection efficiency and forecast better with real-time cash flow visibility and predictability.</p></div>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_tags[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaY4rqHxlrgp"
   },
   "source": [
    "![](https://i.imgur.com/ncnfg0z.png)\n",
    "\n",
    "Luckily most of the required data points are available in this `<div>`, so we can use `find` method to grab each items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hn02jUnUlrgp",
    "outputId": "d01e1ad5-4796-46f3-a1b8-0bcc605f0322"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:  News Direct\n",
      "Head Line : Freshworks alums launch Growfin targeting the $125T global B2B payments market with a collaboration-first approach\n"
     ]
    }
   ],
   "source": [
    "print(\"Source: \", div_tags[1].find('div').text)\n",
    "print(\"Head Line : {}\".format(div_tags[1].find('a').text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54ry5o3-lrgp"
   },
   "source": [
    "If any tag is not accessible directly, then you can use methods like `findParent()` or `'findChild()` to point to the required tag.\n",
    "\n",
    "![](https://i.imgur.com/OnOAtT2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BYm7cqy2lrgp",
    "outputId": "0560b1f3-e726-4b00-e356-ddaa6b0a0760"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image URL:  https://s.yimg.com/uu/api/res/1.2/rI2S.krfLjVNOh9OLdo3Uw--~B/Zmk9c3RyaW07aD0xMjM7cT04MDt3PTIyMDthcHBpZD15dGFjaHlvbg--/https://s.yimg.com/uu/api/res/1.2/XY6GVxhFwq2iy2RRUgpZpg--~B/aD0yMjYwO3c9Mjg4MDthcHBpZD15dGFjaHlvbg--/https://media.zenfs.com/en/news_direct/eeb8da85066380e365e04c019fc88413.cf.jpg\n"
     ]
    }
   ],
   "source": [
    "print(\"Image URL: \",div_tags[1].findParent().find('img')['src'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_rZ3hP_lrgq"
   },
   "source": [
    "**Summary** : Key Takeout from this exercise is to identify the optimal tag which will provide us required information. Sometimes its straight forward, sometimes you will have to perform little more research.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylJlxrEhlrgq"
   },
   "source": [
    "### 1.3 Extract & Compile the information into python list\n",
    "\n",
    "Now we've identified all required tags and information, Let's put this together in the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Ck_Buvi8lrgr"
   },
   "outputs": [],
   "source": [
    "def get_news_tags(doc):\n",
    "    \"\"\"Get the list of tags containing news information\"\"\"\n",
    "    news_class = \"Ov(h) Pend(44px) Pstart(25px)\" ## class name of div tag \n",
    "    news_list  = doc.find_all('div', {'class': news_class})\n",
    "    return news_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u93jXLLslrgr"
   },
   "source": [
    "sample run of the function `get_news_tags`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "byS55R-9lrgr"
   },
   "outputs": [],
   "source": [
    "my_news_tags = get_news_tags(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfXnnMASlrgr"
   },
   "source": [
    "we will create one more function, to parse individual `<div>` tag and return the information in dictionary form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "bsuD1-O4lrgr"
   },
   "outputs": [],
   "source": [
    "BASE_URL = 'https://finance.yahoo.com' #Global Variable \n",
    "\n",
    "def parse_news(news_tag):\n",
    "    news_source = news_tag.find('div').text #source\n",
    "    news_headline = news_tag.find('a').text #heading\n",
    "    news_url = news_tag.find('a')['href'] #link\n",
    "    news_content = news_tag.find('p').text #content\n",
    "    news_image = news_tag.findParent().find('img')['src'] #thumb image\n",
    "    return { 'source' : news_source,\n",
    "            'headline' : news_headline,\n",
    "            'url' : BASE_URL + news_url,\n",
    "            'content' : news_content,\n",
    "            'image' : news_image\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q88hBshllrgr"
   },
   "source": [
    "Lets test this `parse_news` function on first `<div>` tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7gO3YYf2lrgr",
    "outputId": "a0ec470a-696f-4273-8e91-e7ec371c8e39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'Bloomberg',\n",
       " 'headline': 'U.S. Futures Rise as Europe Stocks Pare Drop: Markets Wrap',\n",
       " 'url': 'https://finance.yahoo.com/news/stocks-drop-bonds-oil-surge-223608217.html',\n",
       " 'content': '(Bloomberg) -- U.S. futures rose and European stocks wiped early losses, signaling the market selloff is easing after Tuesday’s slide.Most Read from BloombergChina Spy Think Tank Advising Xi Predicts Russia Sanctions Will BackfireTeen Who Tracked Elon Musk’s Jet Is Now Chasing Russian TycoonsMicrosoft Says Son of CEO Satya Nadella Has Died at 26Russia Steps Up Aerial Campaign Against Cities: Ukraine UpdateBiden Assails Putin, Pledges Inflation Fight in State of UnionContracts on U.S. gauges adva',\n",
       " 'image': 'https://s.yimg.com/uu/api/res/1.2/vYIFOtsagzMYkLUMw2g8fQ--~B/Zmk9c3RyaW07aD0xMjM7cT04MDt3PTIyMDthcHBpZD15dGFjaHlvbg--/https://s.yimg.com/uu/api/res/1.2/d5ZdqYi2ON5wPGY03c68CQ--~B/aD0xMzM0O3c9MjAwMDthcHBpZD15dGFjaHlvbg--/https://media.zenfs.com/en/bloomberg_markets_842/0affa85e6d71d331fefdd71d01fb4b1b.cf.jpg'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_news(my_news_tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4Jt8PhXlrgr"
   },
   "source": [
    "**Summary** : We can use the `get_news_tags` & `parse_news` functions to pars news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ruE_5Q2lrgs"
   },
   "source": [
    "### 1.4 Save the extracted information to a CSV file\n",
    "\n",
    "This is the last step of this section, We are going to use Python library [`pandas`](https://pandas.pydata.org/docs/) to save the data in CSV format. Install and then Import the pandas Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "fFiWEPAalrgs"
   },
   "outputs": [],
   "source": [
    "!pip install pandas --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "zMrbCInvlrgs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JoZJq6GTlrgs"
   },
   "source": [
    "Now we will create one final function, in this function we will use all previously created helper functions.<br>\n",
    "The `get_page` function will download HTML page,then we can pass the result in `get_news_tags` to identify list of `<div>` tags for news.<br>\n",
    "After that we will use [List Comprehension](https://www.w3schools.com/python/python_lists_comprehension.asp) technique to pars each `<div>` tag using `parse_news`, the output will be in the form of `lists` of `dictionaries`<br>\n",
    "Finally we will use `DataFrame` method to create pandas [dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) and use `to_csv` method to store required data in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "8evbvhJklrgs"
   },
   "outputs": [],
   "source": [
    "def scrape_yahoo_news(url, path=None):\n",
    "    \"\"\"Get the yahoo finance market news and write them to CSV file \"\"\"\n",
    "    if path is None:\n",
    "        path = 'stock-market-news.csv'\n",
    "        \n",
    "    print('Requesting html page')\n",
    "    doc = get_page(url)\n",
    "\n",
    "    print('Extracting news tags')\n",
    "    news_list = get_news_tags(doc)\n",
    "\n",
    "    print('Parsing news tags')\n",
    "    news_data = [parse_news(news_tag) for news_tag in news_list]\n",
    "\n",
    "    print('Save the data to a CSV')\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    news_df.to_csv('stock-market-news.csv', index=None)\n",
    "    \n",
    "    #This return statement is optional, we are doing this just analyze the final output \n",
    "    return news_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGYzOftflrgs"
   },
   "source": [
    "It's time to test the `scrape_yahoo_news` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0UC91K0dlrgs",
    "outputId": "4e5c2e01-77be-4d06-f851-40b15f8a3b9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting html page\n",
      "Extracting news tags\n",
      "Parsing news tags\n",
      "Save the data to a CSV\n"
     ]
    }
   ],
   "source": [
    "YAHOO_NEWS_URL = BASE_URL+'/topic/stock-market-news/'\n",
    "news_df = scrape_yahoo_news(YAHOO_NEWS_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBD6nB50lrgs"
   },
   "source": [
    "The \"stock-market-news.csv\" should be available in File --> Open Menu, you can download the file or directly open it on browser. Please verify the file content and compare it with the actual information available on the webpage.\n",
    "\n",
    "You can also check the data by grabbing few rows form the data frame returned by the `scrape_yahoo_news` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "1segLgXIlrgs",
    "outputId": "f0b74462-783c-475c-9b94-32c04d708b31"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>headline</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>U.S. Futures Rise as Europe Stocks Pare Drop: ...</td>\n",
       "      <td>https://finance.yahoo.com/news/stocks-drop-bon...</td>\n",
       "      <td>(Bloomberg) -- U.S. futures rose and European ...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/vYIFOtsagzMY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>News Direct</td>\n",
       "      <td>Freshworks alums launch Growfin targeting the ...</td>\n",
       "      <td>https://finance.yahoo.com/news/freshworks-alum...</td>\n",
       "      <td>Growfin’s AI-powered platform enables business...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/rI2S.krfLjVN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>OPEC+ Sits on Market Sideline as Russian Invas...</td>\n",
       "      <td>https://finance.yahoo.com/news/opec-sits-marke...</td>\n",
       "      <td>(Bloomberg) -- OPEC+ ministers gather on Wedne...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/1dLNVZTPb81j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reuters</td>\n",
       "      <td>World's major companies lag on climate, some m...</td>\n",
       "      <td>https://finance.yahoo.com/news/worlds-major-co...</td>\n",
       "      <td>The corporate world remains far from being ali...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/MLsYBX7UYX2T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business Insider</td>\n",
       "      <td>State of the Union: Joe Biden ignores Congress...</td>\n",
       "      <td>https://finance.yahoo.com/news/state-union-joe...</td>\n",
       "      <td>President Barack Obama, his former boss, calle...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/nsUkhGK.mIYo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             source                                           headline  \\\n",
       "0         Bloomberg  U.S. Futures Rise as Europe Stocks Pare Drop: ...   \n",
       "1       News Direct  Freshworks alums launch Growfin targeting the ...   \n",
       "2         Bloomberg  OPEC+ Sits on Market Sideline as Russian Invas...   \n",
       "3           Reuters  World's major companies lag on climate, some m...   \n",
       "4  Business Insider  State of the Union: Joe Biden ignores Congress...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://finance.yahoo.com/news/stocks-drop-bon...   \n",
       "1  https://finance.yahoo.com/news/freshworks-alum...   \n",
       "2  https://finance.yahoo.com/news/opec-sits-marke...   \n",
       "3  https://finance.yahoo.com/news/worlds-major-co...   \n",
       "4  https://finance.yahoo.com/news/state-union-joe...   \n",
       "\n",
       "                                             content  \\\n",
       "0  (Bloomberg) -- U.S. futures rose and European ...   \n",
       "1  Growfin’s AI-powered platform enables business...   \n",
       "2  (Bloomberg) -- OPEC+ ministers gather on Wedne...   \n",
       "3  The corporate world remains far from being ali...   \n",
       "4  President Barack Obama, his former boss, calle...   \n",
       "\n",
       "                                               image  \n",
       "0  https://s.yimg.com/uu/api/res/1.2/vYIFOtsagzMY...  \n",
       "1  https://s.yimg.com/uu/api/res/1.2/rI2S.krfLjVN...  \n",
       "2  https://s.yimg.com/uu/api/res/1.2/1dLNVZTPb81j...  \n",
       "3  https://s.yimg.com/uu/api/res/1.2/MLsYBX7UYX2T...  \n",
       "4  https://s.yimg.com/uu/api/res/1.2/nsUkhGK.mIYo...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptBKMG_Alrgt"
   },
   "source": [
    "**Summary** : Hopefully I was able to explain this simple but very powerful Python technique to scrape the yahoo finance market news. These steps can be used to scrape any web page, you just have to little research ti identify required <tags> and use relevant python methods to collect the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YiML_iMllrgt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaOUKkUOlrgt"
   },
   "source": [
    "## 2. Scrape **Trending Tickers**\n",
    "\n",
    "In phase One we were able to scrape the [yahoo market news](https://finance.yahoo.com/topic/stock-market-news/) web page. However If you've noticed, as we scroll down the web page more news will appear at the bottom of the page. This is called dynamic page loading. Previous technique is a basic Python method useful to scrape static data, To scrape the dynamically loading data will required a special method that we are going to discussion in this phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuViJR0Elrgt"
   },
   "source": [
    "## TODO Add image ?\n",
    "\n",
    "about selenium \n",
    "functions\n",
    "tell them about xpath \n",
    "\n",
    "https://www.browserstack.com/guide/find-element-by-xpath-in-selenium\n",
    "\n",
    "giude to find xpath -> right click copy x path chk below guide \n",
    "https://analyticsindiamag.com/comprehensive-guide-to-web-scraping-with-selenium/\n",
    "\n",
    "\n",
    "fist explain hoe to use diffetent methods \n",
    "then you can write the code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3PvzjWjlrgt"
   },
   "source": [
    "###  prereq\n",
    "\n",
    "for local enviornmant \n",
    "- !pip install webdriver-manager --upgrade --quiet\n",
    "- download required chromdriver and place it in the project path \n",
    "\n",
    "for colab use other method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1MNZ9bGilrgt",
    "outputId": "24b4c04e-56f5-4675-f6bc-c4634af0cc68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation\n",
      "Not running on CoLab\n",
      "Common Installation\n"
     ]
    }
   ],
   "source": [
    "print('Installation')\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Running on CoLab')\n",
    "    !apt update --quiet\n",
    "    !apt install chromium-chromedriver --quiet\n",
    "else:\n",
    "    print('Not running on CoLab')\n",
    "    !pip install webdriver-manager --upgrade --quiet\n",
    "print('Common Installation')    \n",
    "!pip install selenium --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Axsgec6Tlrgt",
    "outputId": "48052a2a-dfda-4782-9b9c-6c00bc357115"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library Import\n",
      "Not running on CoLab\n",
      "Common Library Import\n"
     ]
    }
   ],
   "source": [
    "print('Library Import')\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Running on CoLab')\n",
    "else:\n",
    "    print('Not running on CoLab')\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "print('Common Library Import')\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5MQcamTlrgt",
    "outputId": "970a47ff-edac-4f23-8de8-059674130568"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on CoLab\n"
     ]
    }
   ],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Running on CoLab')\n",
    "    def get_driver():\n",
    "        colab_options = webdriver.ChromeOptions()\n",
    "        colab_options.add_argument('--no-sandbox')\n",
    "        colab_options.add_argument('--disable-dev-shm-usage')\n",
    "        colab_options.add_argument('--headless')\n",
    "        driver = webdriver.Chrome(options=colab_options)\n",
    "        driver.get(YAHOO_FINANCE_URL)\n",
    "        return driver\n",
    "else:\n",
    "    print('Not running on CoLab')\n",
    "    def get_driver():\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        chrome_options.add_argument('--headless')\n",
    "        serv = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(options=chrome_options, service=serv)\n",
    "        driver.get(YAHOO_FINANCE_URL)\n",
    "        return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S6p8idwllrgu"
   },
   "outputs": [],
   "source": [
    "##old code remove this \n",
    "'''\n",
    "def get_tickers(driver):\n",
    "    TABLE_CLASS = \"W(100%)\"  \n",
    "    driver.get(YAHOO_FINANCE_URL)\n",
    "    tablerows = len(driver.find_elements(By.XPATH, value=\"//table[@class= '{}']/tbody/tr\".format(TABLE_CLASS)))\n",
    "    return tablerows\n",
    "'''   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxmsZmKJlrgu"
   },
   "outputs": [],
   "source": [
    "##old code remove this \n",
    "'''\n",
    "def parse_ticker(rownum, table_driver):\n",
    "    Symbol = table_driver.find_element(By.XPATH, value=\"//tr[{}]/td[1]\".format(rownum)).text\n",
    "    Name = table_driver.find_element(By.XPATH, value=\"//tr[{}]/td[2]\".format(rownum)).text\n",
    "    LastPrice = table_driver.find_element(By.XPATH, value=\"//tr[{}]/td[3]\".format(rownum)).text\n",
    "    MarketTime = table_driver.find_element(By.XPATH, value=\"//tr[{}]/td[4]\".format(rownum)).text\n",
    "    Change = table_driver.find_element(By.XPATH, value=\"//tr[{}]/td[5]\".format(rownum)).text\n",
    "    PercentChange = table_driver.find_element(By.XPATH, value=\"//tr[{}]/td[6]\".format(rownum)).text\t\n",
    "    Volume = table_driver.find_element(By.XPATH, value=\"//tr[{}]/td[7]\".format(rownum)).text\n",
    "    MarketCap = table_driver.find_element(By.XPATH, value=\"//tr[{}]/td[8]\".format(rownum)).text\t\n",
    "\n",
    "    return {\n",
    "    'Symbol': Symbol,\n",
    "    'Name': Name,\n",
    "    'LastPrice': LastPrice,\n",
    "    'MarketTime': MarketTime,\n",
    "    'Change': Change,\n",
    "    'PercentChange': PercentChange,\n",
    "    'Volume': Volume,\n",
    "    'MarketCap': MarketCap\n",
    "    }\n",
    " '''   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_rows(driver):\n",
    "    TABLE_CLASS = \"W(100%)\"  \n",
    "    tablerows = len(driver.find_elements(By.XPATH, value=\"//table[@class= '{}']/tbody/tr\".format(TABLE_CLASS)))\n",
    "    return tablerows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_header(driver):\n",
    "    header = driver.find_elements(By.TAG_NAME, value= 'th')\n",
    "    header_list = [item.text for index, item in enumerate(header) if index < 10]\n",
    "    return header_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use below to explain \n",
    "```\n",
    "rownum = 1\n",
    "colnum = 3\n",
    "driver.find_element(By.XPATH, value=\"//tr[{}]/td[{}]\".format(rownum,colnum)).text\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_table_rows(rownum, driver, header_list):\n",
    "    row_dictionary = {}\n",
    "    for index , item in enumerate(header_list):\n",
    "        row_dictionary[item] = driver.find_element(By.XPATH, value=\"//tr[{}]/td[{}]\".format(rownum, index+1)).text\n",
    "    return row_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 98.0.4758\n",
      "Get LATEST chromedriver version for 98.0.4758 google-chrome\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating driver\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Driver [/Users/vinoddhole/.wdm/drivers/chromedriver/mac64/98.0.4758.102/chromedriver] found in cache\n"
     ]
    }
   ],
   "source": [
    "YAHOO_FINANCE_URL = 'https://finance.yahoo.com/cryptocurrencies'#'https://finance.yahoo.com/trending-tickers'\n",
    "\n",
    "print('Creating driver')\n",
    "driver = get_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_list = get_table_header(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Table row count for Page : 1\n",
      "Parsing Page : 1\n",
      "Clicking Next Button\n",
      "Getting Table row count for Page : 2\n",
      "Parsing Page : 2\n",
      "Clicking Next Button\n",
      "Getting Table row count for Page : 3\n",
      "Parsing Page : 3\n"
     ]
    },
    {
     "ename": "StaleElementReferenceException",
     "evalue": "Message: stale element reference: element is not attached to the page document\n  (Session info: headless chrome=98.0.4758.109)\nStacktrace:\n0   chromedriver                        0x0000000100c94ee9 chromedriver + 5013225\n1   chromedriver                        0x0000000100c201d3 chromedriver + 4534739\n2   chromedriver                        0x00000001007f6a68 chromedriver + 170600\n3   chromedriver                        0x00000001007f98b1 chromedriver + 182449\n4   chromedriver                        0x00000001007f96d1 chromedriver + 181969\n5   chromedriver                        0x00000001007f996c chromedriver + 182636\n6   chromedriver                        0x0000000100825b59 chromedriver + 363353\n7   chromedriver                        0x00000001008487e2 chromedriver + 505826\n8   chromedriver                        0x000000010081fbf5 chromedriver + 338933\n9   chromedriver                        0x00000001008488ee chromedriver + 506094\n10  chromedriver                        0x000000010085b074 chromedriver + 581748\n11  chromedriver                        0x00000001008486d3 chromedriver + 505555\n12  chromedriver                        0x000000010081e76e chromedriver + 333678\n13  chromedriver                        0x000000010081f745 chromedriver + 337733\n14  chromedriver                        0x0000000100c50efe chromedriver + 4734718\n15  chromedriver                        0x0000000100c6aa19 chromedriver + 4839961\n16  chromedriver                        0x0000000100c701c8 chromedriver + 4862408\n17  chromedriver                        0x0000000100c6b3aa chromedriver + 4842410\n18  chromedriver                        0x0000000100c45a01 chromedriver + 4688385\n19  chromedriver                        0x0000000100c86538 chromedriver + 4953400\n20  chromedriver                        0x0000000100c866c1 chromedriver + 4953793\n21  chromedriver                        0x0000000100c9c225 chromedriver + 5042725\n22  libsystem_pthread.dylib             0x00007fff2065a8fc _pthread_start + 224\n23  libsystem_pthread.dylib             0x00007fff20656443 thread_start + 15\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStaleElementReferenceException\u001b[0m            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m table_rows \u001b[38;5;241m=\u001b[39m get_table_rows(driver)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParsing Page : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(page))\n\u001b[0;32m---> 10\u001b[0m table_data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [parse_table_rows(i, driver, header_list) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (\u001b[38;5;241m1\u001b[39m, table_rows \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClicking Next Button\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m next_button \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mXPATH, value \u001b[38;5;241m=\u001b[39m next_button_class)    \n",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m table_rows \u001b[38;5;241m=\u001b[39m get_table_rows(driver)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParsing Page : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(page))\n\u001b[0;32m---> 10\u001b[0m table_data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[43mparse_table_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader_list\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (\u001b[38;5;241m1\u001b[39m, table_rows \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClicking Next Button\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m next_button \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mXPATH, value \u001b[38;5;241m=\u001b[39m next_button_class)    \n",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36mparse_table_rows\u001b[0;34m(rownum, driver, header_list)\u001b[0m\n\u001b[1;32m      2\u001b[0m row_dictionary \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index , item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(header_list):\n\u001b[0;32m----> 4\u001b[0m     row_dictionary[item] \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m//tr[\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m]/td[\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrownum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m row_dictionary\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/selenium/webdriver/remote/webelement.py:77\u001b[0m, in \u001b[0;36mWebElement.text\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;124;03m\"\"\"The text of the element.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET_ELEMENT_TEXT\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/selenium/webdriver/remote/webelement.py:710\u001b[0m, in \u001b[0;36mWebElement._execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    708\u001b[0m     params \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    709\u001b[0m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id\n\u001b[0;32m--> 710\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py:425\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    423\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m     response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(\n\u001b[1;32m    427\u001b[0m         response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/selenium/webdriver/remote/errorhandler.py:247\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    245\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mStaleElementReferenceException\u001b[0m: Message: stale element reference: element is not attached to the page document\n  (Session info: headless chrome=98.0.4758.109)\nStacktrace:\n0   chromedriver                        0x0000000100c94ee9 chromedriver + 5013225\n1   chromedriver                        0x0000000100c201d3 chromedriver + 4534739\n2   chromedriver                        0x00000001007f6a68 chromedriver + 170600\n3   chromedriver                        0x00000001007f98b1 chromedriver + 182449\n4   chromedriver                        0x00000001007f96d1 chromedriver + 181969\n5   chromedriver                        0x00000001007f996c chromedriver + 182636\n6   chromedriver                        0x0000000100825b59 chromedriver + 363353\n7   chromedriver                        0x00000001008487e2 chromedriver + 505826\n8   chromedriver                        0x000000010081fbf5 chromedriver + 338933\n9   chromedriver                        0x00000001008488ee chromedriver + 506094\n10  chromedriver                        0x000000010085b074 chromedriver + 581748\n11  chromedriver                        0x00000001008486d3 chromedriver + 505555\n12  chromedriver                        0x000000010081e76e chromedriver + 333678\n13  chromedriver                        0x000000010081f745 chromedriver + 337733\n14  chromedriver                        0x0000000100c50efe chromedriver + 4734718\n15  chromedriver                        0x0000000100c6aa19 chromedriver + 4839961\n16  chromedriver                        0x0000000100c701c8 chromedriver + 4862408\n17  chromedriver                        0x0000000100c6b3aa chromedriver + 4842410\n18  chromedriver                        0x0000000100c45a01 chromedriver + 4688385\n19  chromedriver                        0x0000000100c86538 chromedriver + 4953400\n20  chromedriver                        0x0000000100c866c1 chromedriver + 4953793\n21  chromedriver                        0x0000000100c9c225 chromedriver + 5042725\n22  libsystem_pthread.dylib             0x00007fff2065a8fc _pthread_start + 224\n23  libsystem_pthread.dylib             0x00007fff20656443 thread_start + 15\n"
     ]
    }
   ],
   "source": [
    "TOTAL_PAGES_TO_SCRAPE = 10\n",
    "table_data = []\n",
    "next_button_class = '//*[@id=\"scr-res-table\"]/div[2]/button[3]'\n",
    "for page in range(1, TOTAL_PAGES_TO_SCRAPE + 1):\n",
    "    \n",
    "    print('Getting Table row count for Page : {}'.format(page))\n",
    "    table_rows = get_table_rows(driver)\n",
    "    \n",
    "    print('Parsing Page : {}'.format(page))\n",
    "    table_data += [parse_table_rows(i, driver, header_list) for i in range (1, table_rows + 1)]\n",
    "    \n",
    "    print('Clicking Next Button')\n",
    "    next_button = driver.find_elements(By.XPATH, value = next_button_class)    \n",
    "    next_button[0].click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(table_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Save the data to a CSV')\n",
    "table_df = pd.DataFrame(table_data)\n",
    "#print(table_df)\n",
    "table_df.to_csv('cryptocurrencies.csv', index=None)\n",
    "#### add timer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tc6XWwf5lrgu"
   },
   "outputs": [],
   "source": [
    "header = driver.find_elements(By.TAG_NAME, value= 'th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "FQABvh2klrgu",
    "outputId": "444acc22-2074-4d68-8c7d-587abb0802f5"
   },
   "outputs": [],
   "source": [
    "header[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "LuEM88Sclrgv",
    "outputId": "bdea7d87-c323-4d46-8075-224c6ed3df25"
   },
   "outputs": [],
   "source": [
    "rownum=2\n",
    "txt=driver.find_element(By.XPATH, value=\"//tr[{}]/td[2]\".format(rownum)).text\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2dK17gZRYW4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMYdI4n3RYW4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JUaqMYcUlrgv",
    "outputId": "8ba812d1-7248-49bb-f20d-fcabac3ebd4c"
   },
   "outputs": [],
   "source": [
    "##OLD CODE REMOVE \n",
    "'''\n",
    "YAHOO_FINANCE_URL = 'https://finance.yahoo.com/trending-tickers'\n",
    "\n",
    "print('Creating driver')\n",
    "driver = get_driver()\n",
    "\n",
    "print('Fetching the page')\n",
    "table_rows = get_tickers(driver)\n",
    "\n",
    "print(f'Found {table_rows} Tickers')\n",
    "\n",
    "print('Parsing Trending tickers')\n",
    "ticker_data = [parse_ticker(i, driver) for i in range (1, table_rows + 1)]\n",
    "\n",
    "print('Save the data to a CSV')\n",
    "tickers_df = pd.DataFrame(ticker_data)\n",
    "#print(tickers_df)\n",
    "tickers_df.to_csv('trending-tickers.csv', index=None)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wetnFwuiRYW4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5qDIT-slrgv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQw0QRuGlrgv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5I5QtnERlrgv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYxdL1q6lrgv"
   },
   "source": [
    "**Installation**\n",
    "\n",
    "Anaconda: Download and install it from this link https://www.anaconda.com/ . We will be using Jupyter Notebook for writing the code\n",
    "Chromedriver — Webdriver for Chrome: Download it from this link https://chromedriver.chromium.org/downloads. No need of installing, just copy the file in the folder where we will create the python file. But before downloading, confirm that the driver‘s version matches that of the Chrome browser installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7aSVLdJ1lrgw"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"vinodvidhole/yahoo-finance-web-scraper\" on https://jovian.ai\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/vinodvidhole/yahoo-finance-web-scraper\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/vinodvidhole/yahoo-finance-web-scraper'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project=\"yahoo-finance-web-scraper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXXLySqvlrgw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWvWtKV9lrgw"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z9MF3oRzlrgw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBAYOrPylrgw"
   },
   "source": [
    "future \n",
    "fix timezone in market events "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74uMwVpolrgw"
   },
   "source": [
    "-do to\n",
    "\n",
    "-check above notes \n",
    "\n",
    "-testing - done normal, zero rows \n",
    "\n",
    "-comments \n",
    "\n",
    "-print statements & function doc strings  \n",
    "\n",
    "-code clean up *** is applicable \n",
    "\n",
    "-documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMk7mH11lrgw"
   },
   "source": [
    "## reference\n",
    "    https://htmldog.com/guides/html/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9EQk85jlrgw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-bSdX-nlrgw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "DvSMJZeClrgo"
   ],
   "name": "yahoo-finance-web-scraper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
