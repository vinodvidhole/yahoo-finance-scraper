{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pF8beWv0lrgc"
   },
   "source": [
    "# Web Scraping Yahoo! Finance using Python\n",
    "\n",
    "A detailed guide for scraping https://finance.yahoo.com/ using **requests**, **BeautifulSoup**, **Selenium**, **HTML tags** & existing available data in **json** format.\n",
    "\n",
    "![](https://imgur.com/7jMFOcE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FULGhEjlrge"
   },
   "source": [
    "**What is Web scraping?**<br>\n",
    "Web scraping is the process of extracting and parsing data from websites in an automated fashion using a computer program. It's a useful technique for creating datasets for research and learning.\n",
    "\n",
    "**Introduction**<br>\n",
    "The main objective of this tutorial is to showcase different web scraping methods which can be applied to any web page. \n",
    "This is for educational purposes only. Please read the Terms & Conditions carefully for any website whether you can legally use the data. \n",
    "\n",
    "In this Project we will perform web scraping using following 3 techniques based on the problem statement.\n",
    "* use `requests`, `BeautifulSoup` and `HTML tags` to extract web page\n",
    "* use `Selenium` to scrape data from dynamically loading websites \n",
    "* scrape data using existing data available in `json` format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CFXNNENlrgf"
   },
   "source": [
    "**The problem statement**<br>\n",
    "1. Scrape **Stock Market News** (url : https://finance.yahoo.com/topic/stock-market-news/) :<br>\n",
    "    This web page shows latest **news** related to **stock market**, we will try to extract data from this web page and store it in `CSV` (comma-separated values) file. The file layout would be as mentioned below.\n",
    "    ```\n",
    "    source,headline,url,content,image\n",
    "    <source of the news>,<news head line>,<news url>,<news content>,<news thumbnail image>\n",
    "    ```\n",
    "\n",
    "2. Scrape **Cryptocurrencies** (url : https://finance.yahoo.com/cryptocurrencies) :<br>\n",
    "    This yahoo finance web page is showing list of trending **Cryptocurrencies** in tabular format, we will perform the web scraping to retrieve first 10 columns for top 100 **Cryptocurrencies** in `CSV` format.\n",
    "    ```\n",
    "    Symbol,Name,Price (Intraday),Change,% Change,Market Cap,Volume in Currency (Since 0:00 UTC),\n",
    "    Volume in Currency (24Hr),Total Volume All Currencies (24Hr),Circulating Supply\n",
    "    BTC-USD,Bitcoin USD,\"43,312.13\",-947.50,-2.14%,821.76B,27.727B,27.727B,27.727B,18.973M\n",
    "\n",
    "    ```\n",
    "        \n",
    "3. Scrape **Market Events Calendar** (url : https://finance.yahoo.com/calendar) :<br> \n",
    "    This page is showing **date-wise market events**, user have option to select the date and choose any one of the following market event **Earnings**, **Stock Splits**, **Economic Events** & **IPO**. Our aim is to create script which can be run for any single date and market event which grabs the data and load it in `CSV` format. If there is no data found then just create file with column headers.<br>\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Z9UQZf8lrgg"
   },
   "source": [
    "**Prerequisites**\n",
    "* Knowledge of Python\n",
    "* Basic knowledge of HTML although it is not necessary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbMRHIhblrgh"
   },
   "source": [
    "**How to run the Code**<br>\n",
    "You can execute the code using \"Run\" button on the top of this page and selecting **\"Run on Colab\"** or **\"Run Locally\"** \n",
    "<br>\n",
    "<br>\n",
    "**Setup and Tools**<br>\n",
    "<u>Run on Colab :</u> \n",
    "    You will need to provide the Google login to run this notebook on Colab.<br>\n",
    "<u>Run Locally :</u> Download and install [Anaconda](https://www.anaconda.com/) framework, We will be using Jupyter Notebook for writing the & executing code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YjeIdedlrgh"
   },
   "source": [
    "**Code Re-usability & Version control**\n",
    "\n",
    "You can make changes and save your version of the notebook to [Jovian](https://jovian.ai/) by executing following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4060,
     "status": "ok",
     "timestamp": 1646369793553,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "iLLVpn2olrgi"
   },
   "outputs": [],
   "source": [
    "!pip install jovian --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1646369795600,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "iTKJFEYglrgj"
   },
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "executionInfo": {
     "elapsed": 6876,
     "status": "ok",
     "timestamp": 1646369803815,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "AxYOBC1xlrgj",
    "outputId": "382cd097-c1d4-4a9a-dc56-07a322b5cf98"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"vinodvidhole/yahoo-finance-web-scraper\" on https://jovian.ai/\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/vinodvidhole/yahoo-finance-web-scraper\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/vinodvidhole/yahoo-finance-web-scraper'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute this to save new versions of the notebook\n",
    "jovian.commit(project=\"yahoo-finance-web-scraper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SWl2eoMlrgk"
   },
   "source": [
    "## <u>1. Scrape Stock Market News</u>\n",
    "\n",
    "In this section we will learn basic Python web scraping technique using `requests`, `BeautifulSoup` and `HTML tags`. The objective here is to perform web scraping of [Yahoo! finance Stock Market News](https://finance.yahoo.com/topic/stock-market-news/)\n",
    "\n",
    "![](https://i.imgur.com/1I0Btau.jpg)\n",
    "\n",
    "Lets kick start with the first objective. Here's an outline of the steps we'll follow<br>\n",
    "**1.1 Download & Parse webpage using `requests` and `BeautifulSoup`**<br>\n",
    "**1.2 Exploring and locating Elements**<br>\n",
    "**1.3 Extract & Compile the information into python list**<br>\n",
    "**1.4 Save the extracted information to a CSV file**<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSis_3UJlrgl"
   },
   "source": [
    "### 1.1 Download & Parse webpage using requests and BeautifulSoup\n",
    "\n",
    "First step is to install [`requests`](https://docs.python-requests.org/en/latest/) & [`beautifulsoup4`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) Libraries using `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6822,
     "status": "ok",
     "timestamp": 1646369851256,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "zXJKsNyQlrgl",
    "outputId": "711cc86d-5b33-469a-bfb9-b0ce8b9d36cd"
   },
   "outputs": [],
   "source": [
    "!pip install requests --quiet\n",
    "!pip install beautifulsoup4 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1646369868216,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "L_gdMI8llrgm"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1d4YUeDlrgm"
   },
   "source": [
    "The library is now installed and imported.<br>\n",
    "\n",
    "To download the page, we can use `requests.get`, which returns a response object. the HTML information of web page is captured in `response.text`.<br>\n",
    "`response.ok` & [`response.status_code`](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status) can be used for error trapping &  tracking.<br> \n",
    "Finally we can use `BeautifulSoup` to parse the HTML data, this will return `bs4.BeautifulSoup` object  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rD6E93Trlrgm"
   },
   "source": [
    "Lets create a function to perform this step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 120,
     "status": "ok",
     "timestamp": 1646369873187,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "-FIURHiqlrgm"
   },
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    \"\"\"Download a webpage and return a beautiful soup doc\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if not response.ok:\n",
    "        print('Status code:', response.status_code)\n",
    "        raise Exception('Failed to load page {}'.format(url))\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "susW06Hwlrgn"
   },
   "source": [
    "calling function `get_page` and analyze the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 786,
     "status": "ok",
     "timestamp": 1646369876720,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "cGWBiRCJlrgn"
   },
   "outputs": [],
   "source": [
    "my_url = 'https://finance.yahoo.com/topic/stock-market-news/' \n",
    "doc = get_page(my_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 140,
     "status": "ok",
     "timestamp": 1646369881503,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "jvwJU_ULlrgn",
    "outputId": "fedf6353-17b6-4829-b2db-d31b413e504f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of doc:  <class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "print('Type of doc: ',type(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqEwtKgTlrgn"
   },
   "source": [
    "You can access different properties of HTML web page from doc, following example will display Title of the web page.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 137,
     "status": "ok",
     "timestamp": 1646369883415,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "S0xvsoyelrgo",
    "outputId": "b6fbe348-fb4f-4f62-fecb-439fd2a316bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Latest Stock Market News</title>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.find('title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hbs8VJS9lrgo"
   },
   "source": [
    "**Summary** : We can now use the function `get_page` to download any web page and parse it using beautiful soup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvSMJZeClrgo"
   },
   "source": [
    "### 1.2 Exploring and locating Elements\n",
    "Now its time to explore the elements to find the required data point from the web page. Web pages are written in a language called HTML (Hyper Text Markup Language).  HTML is a fairly simple language comprised of *tags*  (also called *nodes* or *elements*) e.g. `<a href=\"https://finance.yahoo.com/\" target=\"_blank\">Go to Yahoo! Finance</a>`. An HTML tag has three parts:\n",
    "\n",
    "\n",
    "\n",
    "1. **Name**: (`html`, `head`, `body`, `div`, etc.) Indicates what the tag represents and how a browser should interpret the information inside it.\n",
    "2. **Attributes**: (`href`, `target`, `class`, `id`, etc.) Properties of tag used by the browser to customize how a tag is displayed and decide what happens on user interactions.\n",
    "3. **Children**: A tag can contain some text or other tags or both between the opening and closing segments, e.g., `<div>Some content</div>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAFbXWN2lrgo"
   },
   "source": [
    "Now lets inspect the webpage source code by right-click and select the \"Inspect\" option. First we need to identify the tag which represents the news listing.\n",
    "\n",
    "## TODO FIX BELOW "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C97r1w_5lrgo"
   },
   "source": [
    "![](https://media.giphy.com/media/RQpW64jdiQG8LNCGsZ/giphy.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fr5ofFpFlrgo"
   },
   "source": [
    "In this case we can see the `<div>` tag having class name `\"Ov(h) Pend(44px) Pstart(25px)\"` is representing news listing, we can apply `find_all` method to grab this information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 129,
     "status": "ok",
     "timestamp": 1646369891150,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "WF_cAVptlrgo"
   },
   "outputs": [],
   "source": [
    "div_tags = doc.find_all('div', {'class': \"Ov(h) Pend(44px) Pstart(25px)\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_Y4L3Kglrgp"
   },
   "source": [
    "Total elements in the `<div>` tag list is matching with the numbers of news displaying in the webpage , so we are heading towards right direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1646369892537,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "FniLi1nElrgp",
    "outputId": "28a6b6c8-e63f-4900-f125-cf255784d2c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(div_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4acfugiWlrgp"
   },
   "source": [
    "Next step to inspect the individual `<div>` tag and try to find more information. I am using \"Visual Studio Code\", but you can use any tool as simple as notepad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1646369894617,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "paFm23Amlrgp",
    "outputId": "1aa51562-730a-4246-f312-13100b46c11f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"Ov(h) Pend(44px) Pstart(25px)\"><div class=\"C(#959595) Fz(11px) D(ib) Mb(6px)\">The Wall Street Journal</div><h3 class=\"Mb(5px)\"><a class=\"js-content-viewer wafer-caas Fw(b) Fz(18px) Lh(23px) LineClamp(2,46px) Fz(17px)--sm1024 Lh(19px)--sm1024 LineClamp(2,38px)--sm1024 mega-item-header-link Td(n) C(#0078ff):h C(#000) LineClamp(2,46px) LineClamp(2,38px)--sm1024 not-isInStreamVideoEnabled\" data-uuid=\"ee272bf0-c96d-37f1-9a6f-3a6ba8c49627\" data-wf-caas-prefetch=\"1\" data-wf-caas-uuid=\"ee272bf0-c96d-37f1-9a6f-3a6ba8c49627\" href=\"/m/ee272bf0-c96d-37f1-9a6f-3a6ba8c49627/global-markets-fall-on.html\"><u class=\"StretchedBox\"></u>Global Markets Fall on Concerns About Ukraine</a></h3><p class=\"Fz(14px) Lh(19px) Fz(13px)--sm1024 Lh(17px)--sm1024 LineClamp(2,38px) LineClamp(2,34px)--sm1024 M(0)\">U.S. equity futures retreated and global stock indexes fell sharply as investor concerns mounted about Russia’s intensifying military campaign in Ukraine.</p></div>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_tags[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaY4rqHxlrgp"
   },
   "source": [
    "![](https://i.imgur.com/ncnfg0z.png)\n",
    "\n",
    "Luckily most of the required data points are available in this `<div>`, so we can use `find` method to grab each items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1646369897636,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "Hn02jUnUlrgp",
    "outputId": "1cb5714a-393e-49ac-f6a3-86362e8d5eac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:  The Wall Street Journal\n",
      "Head Line : Global Markets Fall on Concerns About Ukraine\n"
     ]
    }
   ],
   "source": [
    "print(\"Source: \", div_tags[1].find('div').text)\n",
    "print(\"Head Line : {}\".format(div_tags[1].find('a').text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54ry5o3-lrgp"
   },
   "source": [
    "If any tag is not accessible directly, then you can use methods like `findParent()` or `'findChild()` to point to the required tag.\n",
    "\n",
    "![](https://i.imgur.com/OnOAtT2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 146,
     "status": "ok",
     "timestamp": 1646369901782,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "BYm7cqy2lrgp",
    "outputId": "12668476-6e78-4904-cf81-f0e4d12d89c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image URL:  https://s.yimg.com/uu/api/res/1.2/4TyVOk8HrS2hx9x82CtLvA--~B/Zmk9c3RyaW07aD0xMjM7cT04MDt3PTIyMDthcHBpZD15dGFjaHlvbg--/https://s.yimg.com/uu/api/res/1.2/fg.v9Cy62kl7bcZJnG3wAg--~B/aD02NDA7dz0xMjgwO2FwcGlkPXl0YWNoeW9u/https://media.zenfs.com/en/wsj.com/cfb9c36c3cde1f8fbb1ed8b6ffa115cc.cf.jpg\n"
     ]
    }
   ],
   "source": [
    "print(\"Image URL: \",div_tags[1].findParent().find('img')['src'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_rZ3hP_lrgq"
   },
   "source": [
    "**Summary** : Key Takeout from this exercise is to identify the optimal tag which will provide us required information. Sometimes its straight forward, sometimes you will have to perform little more research.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylJlxrEhlrgq"
   },
   "source": [
    "### 1.3 Extract & Compile the information into python list\n",
    "\n",
    "Now we've identified all required tags and information, Let's put this together in the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 120,
     "status": "ok",
     "timestamp": 1646369907854,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "Ck_Buvi8lrgr"
   },
   "outputs": [],
   "source": [
    "def get_news_tags(doc):\n",
    "    \"\"\"Get the list of tags containing news information\"\"\"\n",
    "    news_class = \"Ov(h) Pend(44px) Pstart(25px)\" ## class name of div tag \n",
    "    news_list  = doc.find_all('div', {'class': news_class})\n",
    "    return news_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u93jXLLslrgr"
   },
   "source": [
    "sample run of the function `get_news_tags`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 132,
     "status": "ok",
     "timestamp": 1646369909760,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "byS55R-9lrgr"
   },
   "outputs": [],
   "source": [
    "my_news_tags = get_news_tags(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfXnnMASlrgr"
   },
   "source": [
    "we will create one more function, to parse individual `<div>` tag and return the information in dictionary form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1646369912164,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "bsuD1-O4lrgr"
   },
   "outputs": [],
   "source": [
    "BASE_URL = 'https://finance.yahoo.com' #Global Variable \n",
    "\n",
    "def parse_news(news_tag):\n",
    "    \"\"\"Get the news data point and return dictionary\"\"\"\n",
    "    news_source = news_tag.find('div').text #source\n",
    "    news_headline = news_tag.find('a').text #heading\n",
    "    news_url = news_tag.find('a')['href'] #link\n",
    "    news_content = news_tag.find('p').text #content\n",
    "    news_image = news_tag.findParent().find('img')['src'] #thumb image\n",
    "    return { 'source' : news_source,\n",
    "            'headline' : news_headline,\n",
    "            'url' : BASE_URL + news_url,\n",
    "            'content' : news_content,\n",
    "            'image' : news_image\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q88hBshllrgr"
   },
   "source": [
    "Lets test this `parse_news` function on first `<div>` tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 161,
     "status": "ok",
     "timestamp": 1646369914109,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "7gO3YYf2lrgr",
    "outputId": "78e2d63a-7812-4304-cb2b-7a225bd3a702"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': \"Investor's Business Daily\",\n",
       " 'headline': 'Dow Jones Futures Fall, Slash Losses On Ukraine Nuclear Power Plant News',\n",
       " 'url': 'https://finance.yahoo.com/m/3b583f8b-c283-37bd-a4cc-ddc97c4e9e21/dow-jones-futures-fall-slash.html',\n",
       " 'content': 'Futures dived, pared losses amid reports of a fire at a huge Ukraine nuclear power plant facility, under attack by Russian troops.',\n",
       " 'image': 'https://s.yimg.com/uu/api/res/1.2/VswMf765SI4ghJOXjD8Qmw--~B/Zmk9c3RyaW07aD0xMjM7cT04MDt3PTIyMDthcHBpZD15dGFjaHlvbg--/https://s.yimg.com/uu/api/res/1.2/oIMjIc5ViQ2cGM.EWTasaw--~B/aD01NjM7dz0xMDAwO2FwcGlkPXl0YWNoeW9u/https://media.zenfs.com/en/ibd.com/f408c2a91866eb8647765667fffecbd1.cf.jpg'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_news(my_news_tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4Jt8PhXlrgr"
   },
   "source": [
    "**Summary** : We can use the `get_news_tags` & `parse_news` functions to pars news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ruE_5Q2lrgs"
   },
   "source": [
    "### 1.4 Save the extracted information to a CSV file\n",
    "\n",
    "This is the last step of this section, We are going to use Python library [`pandas`](https://pandas.pydata.org/docs/) to save the data in CSV format. Install and then Import the pandas Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 3683,
     "status": "ok",
     "timestamp": 1646369923932,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "fFiWEPAalrgs"
   },
   "outputs": [],
   "source": [
    "!pip install pandas --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 640,
     "status": "ok",
     "timestamp": 1646369927647,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "zMrbCInvlrgs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JoZJq6GTlrgs"
   },
   "source": [
    "Now we will create one final function, in this function we will use all previously created helper functions.<br>\n",
    "The `get_page` function will download HTML page,then we can pass the result in `get_news_tags` to identify list of `<div>` tags for news.<br>\n",
    "After that we will use [List Comprehension](https://www.w3schools.com/python/python_lists_comprehension.asp) technique to pars each `<div>` tag using `parse_news`, the output will be in the form of `lists` of `dictionaries`<br>\n",
    "Finally we will use `DataFrame` method to create pandas [dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) and use `to_csv` method to store required data in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 139,
     "status": "ok",
     "timestamp": 1646369931432,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "8evbvhJklrgs"
   },
   "outputs": [],
   "source": [
    "def scrape_yahoo_news(url, path=None):\n",
    "    \"\"\"Get the yahoo finance market news and write them to CSV file \"\"\"\n",
    "    if path is None:\n",
    "        path = 'stock-market-news.csv'\n",
    "        \n",
    "    print('Requesting html page')\n",
    "    doc = get_page(url)\n",
    "\n",
    "    print('Extracting news tags')\n",
    "    news_list = get_news_tags(doc)\n",
    "\n",
    "    print('Parsing news tags')\n",
    "    news_data = [parse_news(news_tag) for news_tag in news_list]\n",
    "\n",
    "    print('Save the data to a CSV')\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    news_df.to_csv(path, index=None)\n",
    "    \n",
    "    #This return statement is optional, we are doing this just analyze the final output \n",
    "    return news_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGYzOftflrgs"
   },
   "source": [
    "It's time to test the `scrape_yahoo_news` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 421,
     "status": "ok",
     "timestamp": 1646369935719,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "ZhSeuZ6Pi8MD",
    "outputId": "f24fbb34-47f4-45b5-e3b6-3bdea42f712b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting html page\n",
      "Extracting news tags\n",
      "Parsing news tags\n",
      "Save the data to a CSV\n"
     ]
    }
   ],
   "source": [
    "YAHOO_NEWS_URL = BASE_URL+'/topic/stock-market-news/'\n",
    "news_df = scrape_yahoo_news(YAHOO_NEWS_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBD6nB50lrgs"
   },
   "source": [
    "The \"stock-market-news.csv\" should be available in File --> Open Menu, you can download the file or directly open it on browser. Please verify the file content and compare it with the actual information available on the webpage.\n",
    "\n",
    "You can also check the data by grabbing few rows form the data frame returned by the `scrape_yahoo_news` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1646369942971,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "1segLgXIlrgs",
    "outputId": "fc26edc3-dfa9-455e-99b6-d5ca3d6c4ce3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>headline</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Investor's Business Daily</td>\n",
       "      <td>Dow Jones Futures Fall, Slash Losses On Ukrain...</td>\n",
       "      <td>https://finance.yahoo.com/m/3b583f8b-c283-37bd...</td>\n",
       "      <td>Futures dived, pared losses amid reports of a ...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/VswMf765SI4g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Wall Street Journal</td>\n",
       "      <td>Global Markets Fall on Concerns About Ukraine</td>\n",
       "      <td>https://finance.yahoo.com/m/ee272bf0-c96d-37f1...</td>\n",
       "      <td>U.S. equity futures retreated and global stock...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/4TyVOk8HrS2h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>Stocks, Futures Fall on Ukraine as Havens Adva...</td>\n",
       "      <td>https://finance.yahoo.com/news/stocks-set-fall...</td>\n",
       "      <td>(Bloomberg) -- Stocks fell Friday along with E...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/QwRNlbQ30i3R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>Russia Keeps Stock Trading Closed in Nation’s ...</td>\n",
       "      <td>https://finance.yahoo.com/news/russia-keeps-st...</td>\n",
       "      <td>(Bloomberg) -- The Russian stock market will b...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/RP4AFJF2q6ZS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MarketWatch</td>\n",
       "      <td>Asian markets slide as Russia shells Ukraine n...</td>\n",
       "      <td>https://finance.yahoo.com/m/8ce89ec8-6b7e-3cee...</td>\n",
       "      <td>Shares were lower Friday in Asia after another...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/hvqgq.IXadCi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      source  \\\n",
       "0  Investor's Business Daily   \n",
       "1    The Wall Street Journal   \n",
       "2                  Bloomberg   \n",
       "3                  Bloomberg   \n",
       "4                MarketWatch   \n",
       "\n",
       "                                            headline  \\\n",
       "0  Dow Jones Futures Fall, Slash Losses On Ukrain...   \n",
       "1      Global Markets Fall on Concerns About Ukraine   \n",
       "2  Stocks, Futures Fall on Ukraine as Havens Adva...   \n",
       "3  Russia Keeps Stock Trading Closed in Nation’s ...   \n",
       "4  Asian markets slide as Russia shells Ukraine n...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://finance.yahoo.com/m/3b583f8b-c283-37bd...   \n",
       "1  https://finance.yahoo.com/m/ee272bf0-c96d-37f1...   \n",
       "2  https://finance.yahoo.com/news/stocks-set-fall...   \n",
       "3  https://finance.yahoo.com/news/russia-keeps-st...   \n",
       "4  https://finance.yahoo.com/m/8ce89ec8-6b7e-3cee...   \n",
       "\n",
       "                                             content  \\\n",
       "0  Futures dived, pared losses amid reports of a ...   \n",
       "1  U.S. equity futures retreated and global stock...   \n",
       "2  (Bloomberg) -- Stocks fell Friday along with E...   \n",
       "3  (Bloomberg) -- The Russian stock market will b...   \n",
       "4  Shares were lower Friday in Asia after another...   \n",
       "\n",
       "                                               image  \n",
       "0  https://s.yimg.com/uu/api/res/1.2/VswMf765SI4g...  \n",
       "1  https://s.yimg.com/uu/api/res/1.2/4TyVOk8HrS2h...  \n",
       "2  https://s.yimg.com/uu/api/res/1.2/QwRNlbQ30i3R...  \n",
       "3  https://s.yimg.com/uu/api/res/1.2/RP4AFJF2q6ZS...  \n",
       "4  https://s.yimg.com/uu/api/res/1.2/hvqgq.IXadCi...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptBKMG_Alrgt"
   },
   "source": [
    "**Summary** : Hopefully I was able to explain this simple but very powerful Python technique to scrape the yahoo finance market news. These steps can be used to scrape any web page, you just have to little research to identify required <tags> and use relevant python methods to collect the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaOUKkUOlrgt"
   },
   "source": [
    "## <u>2. Scrape Cryptocurrencies</u>\n",
    "\n",
    "In phase One we were able to scrape the [yahoo market news](https://finance.yahoo.com/topic/stock-market-news/) web page. However If you've noticed, as we scroll down the web page more news will appear at the bottom of the page. This is called dynamic page loading. Previous technique is a basic Python method useful to scrape static data, To scrape the dynamically loading data will use a different method called webs craping using **Selenium**. Lets move ahead with this topic. The goal of this section is extract top listing [Crypto currencies](https://finance.yahoo.com/cryptocurrencies) from Yahoo! finance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuViJR0Elrgt"
   },
   "source": [
    "![](https://i.imgur.com/sF6k0Pk.jpg)\n",
    "\n",
    "\n",
    "Here's an outline of the steps we'll follow<br>\n",
    "**2.1 Introduction of selenium**<br>\n",
    "**2.2 Downloads & Installation**<br>\n",
    "**2.3 Install & Import libraries**<br>\n",
    "**2.4 Create Web Driver**<br>\n",
    "**2.5 Exploring and locating Elements**<br>\n",
    "**2.6 Extract & Compile the information into python list**<br>\n",
    "**2.7 Save the extracted information to a CSV file**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvUPJgUEQWZo"
   },
   "source": [
    "### 2.1 Introduction of selenium\n",
    "\n",
    "**[Selenium](https://www.selenium.dev/)** is an open-source web-based automation tool. Python language and other languages are used with Selenium for testing as well as web scraping. Here we will use Chrome browser, but you can try on any browser.<br>\n",
    "\n",
    "**Why you should use Selenium?**\n",
    "- Clicking on buttons\n",
    "- Filling forms\n",
    "- Scrolling\n",
    "- Taking a screen-shot\n",
    "- Refreshing the page\n",
    "\n",
    "You can find proper documentation on selenium [here](https://selenium-python.readthedocs.io/)<br>\n",
    "\n",
    "Following methods will help to find elements in a webpage (these methods will return a list):\n",
    "- `find_elements_by_name`\n",
    "- `find_elements_by_xpath`\n",
    "- `find_elements_by_link_text`\n",
    "- `find_elements_by_partial_link_text`\n",
    "- `find_elements_by_tag_name`\n",
    "- `find_elements_by_class_name`\n",
    "- `find_elements_by_css_selector`\n",
    "\n",
    "In this tutorial we will use only `find_elements_by_xpath` and `find_elements_by_tag_name` You can find complete documentation of these methods [here](https://selenium-python.readthedocs.io/locating-elements.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5IXSU3_QWZs"
   },
   "source": [
    "### 2.2 Downloads & Installation \n",
    "\n",
    "Unlike previous section, here we'll have to do some prep work to implement this method. We will need to install Selenium & proper web browser driver<br>\n",
    "\n",
    "If you are using **Google Colab** platform then execute following code to perform Initial installation. This piece of code `'google.colab' in str(get_ipython())` is used to identify the Google Colab platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28255,
     "status": "ok",
     "timestamp": 1646369981169,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "ot7iEH0QQWZs",
    "outputId": "10da7e6f-a73f-4a55-9f1c-bfb00de8a93f"
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Google CoLab Installation')\n",
    "    !apt update --quiet\n",
    "    !apt install chromium-chromedriver --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kj9N6faQWZs"
   },
   "source": [
    "To run it on **Locally** you will need **Webdriver for Chrome** in the machine, This installation will be performed automatically while loading the driver. But in case auto installation failed then you may need to manually download the Webdriver for Chrome.<br> \n",
    "You can download it from this link https://chromedriver.chromium.org/downloads and just copy the file in the folder where we will create the python file (No need of installation). But make sure that the driver‘s version matches that of the Chrome browser installed on the local machine.\n",
    "\n",
    "## TODO ADD image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGOzu85gQWZs"
   },
   "source": [
    "### 2.3 Install & Import libraries\n",
    "\n",
    "Now lets Install the required libraries. Please note that there are some platform specific libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10549,
     "status": "ok",
     "timestamp": 1646370005782,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "WmDBSb_qQWZt",
    "outputId": "8f24d3aa-df12-4a23-83fe-4964190960ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "library Installation\n",
      "Not running on CoLab\n"
     ]
    }
   ],
   "source": [
    "print('library Installation')\n",
    "if 'google.colab' not in str(get_ipython()):\n",
    "    print('Not running on CoLab')\n",
    "    !pip install webdriver-manager --upgrade --quiet\n",
    "else:\n",
    "    print('Running on CoLab')\n",
    "    \n",
    "!pip install selenium --quiet\n",
    "!pip install pandas --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pn1J-U1PQWZt"
   },
   "source": [
    "Once the Libraries installation is done, next step is to import all the required modules / libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1646370011574,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "PaHNd1hSQWZt",
    "outputId": "c36bc922-80a8-401d-edda-240e22e1af7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library Import\n",
      "Not running on CoLab\n",
      "Common Library Import\n"
     ]
    }
   ],
   "source": [
    "print('Library Import')\n",
    "if 'google.colab' not in str(get_ipython()):\n",
    "    print('Not running on CoLab')\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "else:\n",
    "    print('Running on CoLab')\n",
    "\n",
    "print('Common Library Import')\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlfwVrnVQWZt"
   },
   "source": [
    "So all the necessary prep work is done, lets move ahead to implement this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ms87q9huQWZu"
   },
   "source": [
    "### 2.4 Create Web Driver\n",
    "\n",
    "In this step first we will create the instance of Chrome WebDriver using `webdriver.Chrome()` method. and then the `driver.get()` method will navigate to a page given by the URL. In this case also there is slight variation based on platform, Also passed `options` parameters for e.g. `--headless` option will load the driver in background. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 136,
     "status": "ok",
     "timestamp": 1646370017083,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "cAn7GekCQWZu",
    "outputId": "8d40d20b-20d6-4abc-f29e-5a328cbd7696"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on CoLab\n"
     ]
    }
   ],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Running on CoLab')\n",
    "    def get_driver(url):\n",
    "        \"\"\"Return web driver\"\"\"\n",
    "        colab_options = webdriver.ChromeOptions()\n",
    "        colab_options.add_argument('--no-sandbox')\n",
    "        colab_options.add_argument('--disable-dev-shm-usage')\n",
    "        colab_options.add_argument('--headless')\n",
    "        driver = webdriver.Chrome(options=colab_options)\n",
    "        driver.get(url)\n",
    "        return driver\n",
    "else:\n",
    "    print('Not running on CoLab')\n",
    "    def get_driver(url):\n",
    "        \"\"\"Return web driver\"\"\"\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        chrome_options.add_argument('--headless')\n",
    "        serv = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(options=chrome_options, service=serv)\n",
    "        driver.get(url)\n",
    "        return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnCtyw66QWZu"
   },
   "source": [
    "lets run the function `get_driver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 3452,
     "status": "ok",
     "timestamp": 1646370023139,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "rd7DJYjaQWZu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 98.0.4758\n",
      "Get LATEST chromedriver version for 98.0.4758 google-chrome\n",
      "Driver [/Users/vinoddhole/.wdm/drivers/chromedriver/mac64/98.0.4758.102/chromedriver] found in cache\n"
     ]
    }
   ],
   "source": [
    "driver = get_driver('https://finance.yahoo.com/cryptocurrencies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOF6149XQWZu"
   },
   "source": [
    "### 2.5 Exploring and locating Elements\n",
    "\n",
    "This is almost similar step that we have done in phase 1, We will try to identify relevant information like `<tags>`, `class` , `XPath` etc from the web page. So lets do right-click and select the \"Inspect\" to do further analysis.\n",
    "\n",
    "As the webpage is showing cryptocurrency information in the Table form. We can grab the table header by using tag `<th>`, we will use find_elements by TAG to get the table headers. These headers will be used columns for CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 137,
     "status": "ok",
     "timestamp": 1646370027159,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "zR29b9xH8di9",
    "outputId": "ec096d13-b2c1-4498-b901-87494795e507"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbol\n",
      "Price (Intraday)\n"
     ]
    }
   ],
   "source": [
    "header = driver.find_elements(By.TAG_NAME, value= 'th')\n",
    "print(header[0].text)\n",
    "print(header[2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNZ0kwDhQWZv"
   },
   "source": [
    "Now we will create a helper function to get first 10 columns from header, we have used List comprehension with conditions. you can also check out usage of `enumerate` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 141,
     "status": "ok",
     "timestamp": 1646370030846,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "QTfDpiWrQWZv"
   },
   "outputs": [],
   "source": [
    "def get_table_header(driver):\n",
    "    \"\"\"Return Table columns in list form \"\"\"\n",
    "    header = driver.find_elements(By.TAG_NAME, value= 'th')\n",
    "    header_list = [item.text for index, item in enumerate(header) if index < 10]\n",
    "    return header_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqW7UgNwQWZv"
   },
   "source": [
    "Next we find out number of rows available in a Page, you can see table rows are placed in `<tr>` tag, we can capture the `XPath` by selection `<tr>` tag the Right Click -> Copy -> Copy XPath.\n",
    "\n",
    "## TODO IMAGE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlfJ632vQWZv"
   },
   "source": [
    "So we get the  XPath value as `//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr[1]`, lets use this with `find_element()` & `By.XPATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1646370037585,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "U3FVWjftQWZv",
    "outputId": "6d395270-6a44-4375-d9ff-31a7a8246887"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BTC-USD\\nBitcoin USD 41,363.87 -2,001.70 -4.62% 784.834B 27.205B 27.205B 27.205B 18.974M'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt=driver.find_element(By.XPATH, value='//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr[1]').text\n",
    "txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6JcCdb1Trrp"
   },
   "source": [
    "Above `XPath` points to first row, we can get rid of row number part from XPath and use it with `find_elements` to get hold of all the available rows. Lets implement this in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 131,
     "status": "ok",
     "timestamp": 1646370040402,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "dd9kJccjToHK"
   },
   "outputs": [],
   "source": [
    "def get_table_rows(driver):\n",
    "    \"\"\"Get number of rows available on the page \"\"\"\n",
    "    tablerows = len(driver.find_elements(By.XPATH, value='//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr'))\n",
    "    return tablerows    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 127,
     "status": "ok",
     "timestamp": 1646370043434,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "0v1phXVMRn2E",
    "outputId": "718de3c5-f3fa-4312-cca9-3a3b7210353e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "print(get_table_rows(driver))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A45Ekufq8di9"
   },
   "source": [
    "Similarly we can take the XPath for any column value.\n",
    "## TODO IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0dHoGoVVzTv"
   },
   "source": [
    "This is the XPAth for a column `//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr[1]/td[2]`.<br>\n",
    "If you noticed the the number after `tr` & `td` represents the `row_number` and `column_number`, lets check this with `find_element()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 133,
     "status": "ok",
     "timestamp": 1646370049170,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "1MNZ9bGilrgt",
    "outputId": "1e6d3f32-9685-4202-a905-5d6e16bb4081"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bitcoin USD'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.find_element(By.XPATH, value='//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr[1]/td[2]').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2KhgeZAW7d8"
   },
   "source": [
    "So we can change the `row_number` & `column_number` in `XPath` and loop it through row count and column count to get all the available column values. Lets generalize this and put it in a function. We will get the data for one row at a time and return column value in the form of dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 147,
     "status": "ok",
     "timestamp": 1646370055542,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "pEorS2flYikH"
   },
   "outputs": [],
   "source": [
    "def parse_table_rows(rownum, driver, header_list):\n",
    "    \"\"\"get the data for one row at a time and return column value in the form of dictionary\"\"\"\n",
    "    row_dictionary = {}\n",
    "    time.sleep(1/3)\n",
    "    for index , item in enumerate(header_list):\n",
    "        column_xpath = '//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr[{}]/td[{}]'.format(rownum, index+1)\n",
    "        row_dictionary[item] = driver.find_element(By.XPATH, value=column_xpath).text\n",
    "    return row_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96fzN44rXu-u"
   },
   "source": [
    "The Yahoo! Finance web page is showing only 25 Cryptocurrencies per page and user will have click `Next` button to load next sets of crypto currencies. This is called as **Pagination**. This is the main reason we are implementing selenium method to handle the events like pagination. you can perform multiple events like clicking, scrolling , refreshing etc. on a webpage using selenium methods.\n",
    "\n",
    "Now we will grab the `XPath` of `Next` button, find the element using `find_element` method and after that we can perform click action using `.click()` method \n",
    "\n",
    "## TODO IMAGE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 297,
     "status": "ok",
     "timestamp": 1646370059775,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "s45SMjxligLN"
   },
   "outputs": [],
   "source": [
    "button_element = driver.find_element(By.XPATH, value = '//*[@id=\"scr-res-table\"]/div[2]/button[3]')\n",
    "button_element.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osU8bvFYeIgu"
   },
   "source": [
    "In this section we have learned how to get required data points, and perform events on webpage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit() #terminating driver from test runs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdpxUXQCd5LH"
   },
   "source": [
    "### 2.6 Extract & Compile the information into python list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rL_yJMXSNAD"
   },
   "source": [
    "Lets put all the pieces in the puzzle, we will pass the integer `total_crypto` i.e. numbers of rows to be scraped (in this case 100 rows) in the function. Parse each row from the page and append the data in the `List` till the total parsed row count reach to `total_crypto`. In addition we will perform `Next` button click if we are at the last row of the table. \n",
    "\n",
    "**Please Note** : Here to identify the `Next` button element we have used [WebDriverWait](https://www.selenium.dev/selenium/docs/api/java/org/openqa/selenium/support/ui/WebDriverWait.html) class instead of using `find_element()` method. In this technique we can pass some wait-time before grabbing the element. This type of implememtation is done to avoid the [`StaleElementReferenceException`](https://stackoverflow.com/questions/27003423/staleelementreferenceexception-on-python-selenium).\n",
    "\n",
    "Code Sample:\n",
    "```\n",
    "element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"scr-res-table\"]/div[2]/button[3]')))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 138,
     "status": "ok",
     "timestamp": 1646370065413,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "FgH4XUG9SNAD"
   },
   "outputs": [],
   "source": [
    "def parse_multiple_pages(driver, total_crypto):\n",
    "    \"\"\"Loop through each row, perform Next button click at the end of page \n",
    "    return total_crypto numbers of rows \n",
    "    \"\"\"\n",
    "    table_data = []\n",
    "    page_num = 1\n",
    "    is_scraping = True\n",
    "    header_list = get_table_header(driver)\n",
    "\n",
    "    while is_scraping:\n",
    "        table_rows = get_table_rows(driver)\n",
    "        print('Found {} rows on Page : {}'.format(table_rows, page_num))\n",
    "        print('Parsing Page : {}'.format(page_num))\n",
    "        table_data += [parse_table_rows(i, driver, header_list) for i in range (1, table_rows + 1)]\n",
    "        total_count = len(table_data)\n",
    "        print('Total rows scraped : {}'.format(total_count))\n",
    "        if total_count >= total_crypto:\n",
    "            print('Done Parsing..')\n",
    "            is_scraping = False\n",
    "        else:    \n",
    "            print('Clicking Next Button')\n",
    "            element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"scr-res-table\"]/div[2]/button[3]')))\n",
    "            element.click() \n",
    "            page_num += 1\n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZL49rJqd5aa"
   },
   "source": [
    "### 2.7 Save the extracted information to a CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wG_ZTSEoigLN"
   },
   "source": [
    "This is the last step of this section, we are creating a last function which will be the placeholder for all helper functions and at the and we will save the data in CSV format using `pd.to_csv` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 127,
     "status": "ok",
     "timestamp": 1646370186271,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "a8AxKJLrYikH"
   },
   "outputs": [],
   "source": [
    "def scrape_yahoo_crypto(url, total_crypto, path=None):\n",
    "    \"\"\"Get the list of yahoo finance crypto-currencies and write them to CSV file \"\"\"\n",
    "    if path is None:\n",
    "        path = 'crypto-currencies.csv'\n",
    "    print('Creating driver')\n",
    "    driver = get_driver(url)    \n",
    "    table_data = parse_multiple_pages(driver, total_crypto)\n",
    "    driver.close()\n",
    "    print('Save the data to a CSV')\n",
    "    table_df = pd.DataFrame(table_data)\n",
    "    #print(table_df)\n",
    "    table_df.to_csv(path, index=None)\n",
    "    #This return statement is optional, we are doing this just analyze the final output \n",
    "    return table_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJc4JNBaSNAD"
   },
   "source": [
    "Now its time to scrape some cryoptos!!! , we will scrape top 100 cryptos in Yahoo! Finance webpage by calling `scrape_yahoo_crypto` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67717,
     "status": "ok",
     "timestamp": 1646370167609,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "1fEtHtL2jXxR",
    "outputId": "075b33be-5808-495c-fa68-6506b0c71f0c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 98.0.4758\n",
      "Get LATEST chromedriver version for 98.0.4758 google-chrome\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating driver\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Driver [/Users/vinoddhole/.wdm/drivers/chromedriver/mac64/98.0.4758.102/chromedriver] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25 rows on Page : 1\n",
      "Parsing Page : 1\n",
      "Total rows scraped : 25\n",
      "Clicking Next Button\n",
      "Found 25 rows on Page : 2\n",
      "Parsing Page : 2\n",
      "Total rows scraped : 50\n",
      "Clicking Next Button\n",
      "Found 25 rows on Page : 3\n",
      "Parsing Page : 3\n",
      "Total rows scraped : 75\n",
      "Clicking Next Button\n",
      "Found 25 rows on Page : 4\n",
      "Parsing Page : 4\n",
      "Total rows scraped : 100\n",
      "Done Parsing..\n",
      "Save the data to a CSV\n"
     ]
    }
   ],
   "source": [
    "YAHOO_FINANCE_URL = BASE_URL+'/cryptocurrencies'\n",
    "TOTAL_CRYPTO = 100\n",
    "crypto_df = scrape_yahoo_crypto(YAHOO_FINANCE_URL, TOTAL_CRYPTO,'crypto-currencies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tf7cgOSVYikI"
   },
   "source": [
    "The \"crypto-currencies.csv\" should be available in File --> Open Menu, you can download the file or directly open it on browser. Please verify the file content and compare it with the actual information available on the webpage.\n",
    "\n",
    "You can also check the data by grabbing few rows form the data frame returned by the `scrape_yahoo_crypto` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 140,
     "status": "ok",
     "timestamp": 1646370189062,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "4-fahUk4SNAE",
    "outputId": "66df0ffb-6ef2-489f-f2bb-5212a4db218f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Name</th>\n",
       "      <th>Price (Intraday)</th>\n",
       "      <th>Change</th>\n",
       "      <th>% Change</th>\n",
       "      <th>Market Cap</th>\n",
       "      <th>Volume in Currency (Since 0:00 UTC)</th>\n",
       "      <th>Volume in Currency (24Hr)</th>\n",
       "      <th>Total Volume All Currencies (24Hr)</th>\n",
       "      <th>Circulating Supply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BTC-USD</td>\n",
       "      <td>Bitcoin USD</td>\n",
       "      <td>41,363.87</td>\n",
       "      <td>-2,001.70</td>\n",
       "      <td>-4.62%</td>\n",
       "      <td>784.834B</td>\n",
       "      <td>27.205B</td>\n",
       "      <td>27.205B</td>\n",
       "      <td>27.205B</td>\n",
       "      <td>18.974M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ETH-USD</td>\n",
       "      <td>Ethereum USD</td>\n",
       "      <td>2,724.60</td>\n",
       "      <td>-175.64</td>\n",
       "      <td>-6.06%</td>\n",
       "      <td>326.45B</td>\n",
       "      <td>14.237B</td>\n",
       "      <td>14.237B</td>\n",
       "      <td>14.237B</td>\n",
       "      <td>119.816M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USDT-USD</td>\n",
       "      <td>Tether USD</td>\n",
       "      <td>1.0002</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>-0.02%</td>\n",
       "      <td>79.531B</td>\n",
       "      <td>60.765B</td>\n",
       "      <td>60.765B</td>\n",
       "      <td>60.765B</td>\n",
       "      <td>79.513B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BNB-USD</td>\n",
       "      <td>Binance Coin USD</td>\n",
       "      <td>393.70</td>\n",
       "      <td>-12.69</td>\n",
       "      <td>-3.12%</td>\n",
       "      <td>65.006B</td>\n",
       "      <td>1.65B</td>\n",
       "      <td>1.65B</td>\n",
       "      <td>1.65B</td>\n",
       "      <td>165.117M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USDC-USD</td>\n",
       "      <td>USD Coin USD</td>\n",
       "      <td>0.999893</td>\n",
       "      <td>+0.000154</td>\n",
       "      <td>+0.02%</td>\n",
       "      <td>53.298B</td>\n",
       "      <td>4.21B</td>\n",
       "      <td>4.21B</td>\n",
       "      <td>4.21B</td>\n",
       "      <td>53.304B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Symbol              Name Price (Intraday)     Change % Change Market Cap  \\\n",
       "0   BTC-USD       Bitcoin USD        41,363.87  -2,001.70   -4.62%   784.834B   \n",
       "1   ETH-USD      Ethereum USD         2,724.60    -175.64   -6.06%    326.45B   \n",
       "2  USDT-USD        Tether USD           1.0002    -0.0002   -0.02%    79.531B   \n",
       "3   BNB-USD  Binance Coin USD           393.70     -12.69   -3.12%    65.006B   \n",
       "4  USDC-USD      USD Coin USD         0.999893  +0.000154   +0.02%    53.298B   \n",
       "\n",
       "  Volume in Currency (Since 0:00 UTC) Volume in Currency (24Hr)  \\\n",
       "0                             27.205B                   27.205B   \n",
       "1                             14.237B                   14.237B   \n",
       "2                             60.765B                   60.765B   \n",
       "3                               1.65B                     1.65B   \n",
       "4                               4.21B                     4.21B   \n",
       "\n",
       "  Total Volume All Currencies (24Hr) Circulating Supply  \n",
       "0                            27.205B            18.974M  \n",
       "1                            14.237B           119.816M  \n",
       "2                            60.765B            79.513B  \n",
       "3                              1.65B           165.117M  \n",
       "4                              4.21B            53.304B  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crypto_df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ion8RYbZ8di_"
   },
   "source": [
    "**Summary** : Hope you've enjoyed this tutorial. Selenium enables us to perform multiple actions on the web browser which is really very handy to scrape different type of data from any webpage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-klaoqb8di_"
   },
   "source": [
    "## <u>3. Scrape Market Events Calendar</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "2m7QniEX8di_"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "qFhyZG558di_"
   },
   "outputs": [],
   "source": [
    "def get_event_page(scraper_url):\n",
    "    \"\"\"add something\"\"\"\n",
    "    headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
    "                  \"(KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19582\"\n",
    "    }\n",
    "    response = requests.get(scraper_url, headers=headers)\n",
    "    if not response.ok:\n",
    "        print('Status code:', response.status_code)\n",
    "        raise Exception('Failed to fetch web page ' + scraper_url)\n",
    "    \n",
    "    # Construct a beautiful soup document\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "rgcsfbSx8di_"
   },
   "outputs": [],
   "source": [
    "def get_json_dictionary(doc):\n",
    "    \"\"\"Add\"\"\"\n",
    "    ## Explain \n",
    "    pattern = re.compile(r'\\s--\\sData\\s--\\s')\n",
    "    script_data = doc.find('script', text=pattern).text\n",
    "    \n",
    "    ## Explain\n",
    "    start  = script_data.find('context')-2\n",
    "    json_dictionary  = script_data[start:-12]\n",
    "    \n",
    "    ##explain \n",
    "    parsed_dictionary = json.loads(json_dictionary)\n",
    "    \n",
    "    return parsed_dictionary, json_dictionary    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "SUWFo_MG8di_"
   },
   "outputs": [],
   "source": [
    "# this function is useful during analysis \n",
    "def create_json_file(file_name,json_dictionary):\n",
    "    \"\"\"Add\"\"\"\n",
    "    with open(file_name+'.json', 'w') as file:\n",
    "        file.write(json_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "wyX1dtPy8di_"
   },
   "outputs": [],
   "source": [
    "def get_columns_and_total_rows(parsed_dictionary):\n",
    "    #explain \n",
    "    total_rows = parsed_dictionary['context']['dispatcher']['stores']['ScreenerResultsStore']['results']['total']\n",
    "    column_dictionary = parsed_dictionary['context']['dispatcher']['stores']['ScreenerResultsStore']['results']['columns']\n",
    "    return total_rows, column_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "435fS9Vv8di_"
   },
   "outputs": [],
   "source": [
    "def get_page_rows(parsed_dictionary):\n",
    "    data_dictionary = parsed_dictionary['context']['dispatcher']['stores']['ScreenerResultsStore']['results']['rows']\n",
    "    return len(data_dictionary), data_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "3magUBX48di_"
   },
   "outputs": [],
   "source": [
    "def scrape_all_pages(event_type, date):\n",
    "    YAHOO_CAL_URL = BASE_URL+'/calendar/{}?day={}&offset={}&size={}'\n",
    "    page_size = '100' # this indicates max rows per page \n",
    "    page_number = 1\n",
    "    pagewise_rows = 0\n",
    "    final_data_dictionary = []\n",
    "    \n",
    "    while page_number > 0:\n",
    "        #Explain : starting page 0, next page must be multiple of page_size i.e. 100\n",
    "        print(\"Pricessing page # {}\".format(page_number))\n",
    "        page_url = str((page_number - 1 ) * int(page_size))\n",
    "        scrape_url = YAHOO_CAL_URL.format(event_type, date, page_url, page_size)\n",
    "        print(\"Scrape url for page {} is {}\".format(page_number,scrape_url))\n",
    "        page_doc = get_event_page(scrape_url)\n",
    "        parse_dict, json_dict = get_json_dictionary(page_doc)\n",
    "        total_row, column_dict = get_columns_and_total_rows(parse_dict)        \n",
    "        page_rows , data_dict = get_page_rows(parse_dict)\n",
    "        print(\"total rows for page {} : {}\".format(page_number,page_rows))\n",
    "        pagewise_rows += page_rows\n",
    "        final_data_dictionary += data_dict\n",
    "        if pagewise_rows >= total_row:\n",
    "            page_number = 0\n",
    "            return final_data_dictionary, column_dict\n",
    "        page_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "97BmN84k8di_"
   },
   "outputs": [],
   "source": [
    "def get_dataframe_layout(column_dictionary):\n",
    "    \"\"\"Add\"\"\"\n",
    "    csv_columns = []\n",
    "    csv_columns_header = []\n",
    "    for i in range(len(column_dictionary)):\n",
    "        csv_columns.append(column_dictionary[i]['data'])\n",
    "        csv_columns_header.append(column_dictionary[i]['content'])\n",
    "    return csv_columns, csv_columns_header  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "QqMdZu-f8di_"
   },
   "outputs": [],
   "source": [
    "def create_csv(data_dictionary, filter_columns, filter_columns_header, file_name):\n",
    "    \"\"\"\"\"\"\n",
    "    if len(data_dictionary) > 0:\n",
    "        scraped_df = pd.DataFrame(data_dictionary)\n",
    "        scraped_df.to_csv(file_name,columns=filter_columns,header=filter_columns_header,index=False)\n",
    "    else:\n",
    "        scraped_df=pd.DataFrame(columns=filter_columns_header)\n",
    "        scraped_df.to_csv(file_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "VLfoy_33QWZz"
   },
   "outputs": [],
   "source": [
    "def scrape_yahoo_calendar(event_type, date_param, path=None):\n",
    "    \"\"\"Get the list of yahoo finance calendar and write them to CSV file \"\"\"\n",
    "    if path is None:\n",
    "        path = event_type+'_'+date_param+'.csv'\n",
    "    \n",
    "    data_dict, column_dict = scrape_all_pages(event_type, date_param)\n",
    "    csv_columnm , csv_header = get_dataframe_layout(column_dict)\n",
    "    create_csv(data_dict, csv_columnm, csv_header, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "WpvYTWyx8djA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pricessing page # 1\n",
      "Scrape url for page 1 is https://finance.yahoo.com/calendar/earnings?day=2022-02-28&offset=0&size=100\n",
      "total rows for page 1 : 100\n",
      "Pricessing page # 2\n",
      "Scrape url for page 2 is https://finance.yahoo.com/calendar/earnings?day=2022-02-28&offset=100&size=100\n",
      "total rows for page 2 : 100\n",
      "Pricessing page # 3\n",
      "Scrape url for page 3 is https://finance.yahoo.com/calendar/earnings?day=2022-02-28&offset=200&size=100\n",
      "total rows for page 3 : 16\n"
     ]
    }
   ],
   "source": [
    "BASE_URL = 'https://finance.yahoo.com' #Global Variable \n",
    "\n",
    "## create dictionary to identify webpage and technique to use \n",
    "event_type = 'splits'\n",
    "event_type = 'economic'\n",
    "event_type = 'ipo'\n",
    "event_type = 'earnings'\n",
    "# create a function to get default date if date is not passed \n",
    "date_param = '2022-02-28'\n",
    "\n",
    "\n",
    "scrape_yahoo_calendar(event_type, date_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xpW06QKr8djA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "7aSVLdJ1lrgw"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"vinodvidhole/yahoo-finance-web-scraper\" on https://jovian.ai/\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/vinodvidhole/yahoo-finance-web-scraper\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/vinodvidhole/yahoo-finance-web-scraper'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project=\"yahoo-finance-web-scraper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9MF3oRzlrgw"
   },
   "source": [
    "## References and Future Work\n",
    "\n",
    "Summary of what we did\n",
    "\n",
    "- ?\n",
    "- ?\n",
    "\n",
    "\n",
    "References to links you found useful\n",
    "\n",
    "-  https://htmldog.com/guides/html/\n",
    "- ?\n",
    " \n",
    "**Ideas for future work**<br>\n",
    "- Automate this process using [AWS Lambda](https://aws.amazon.com/lambda/) to download daily market calendar, crypto-currencies & market news in CSV format.\n",
    "- Move the old files in to an Archive folder and append date-stamp to the archive files.\n",
    "- Delete the files older than 2 weeks.\n",
    "- process the raw data extracted from third technique using different methods of pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-69KlW_J8djA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJ6o34es8djA"
   },
   "source": [
    "check lets word\n",
    "check in grammar\n",
    "functions comments "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "DvSMJZeClrgo"
   ],
   "name": "yahoo-finance-web-scraper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
