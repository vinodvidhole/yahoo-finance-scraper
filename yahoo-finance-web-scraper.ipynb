{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pF8beWv0lrgc"
   },
   "source": [
    "# Web Scraping Yahoo! Finance using Python\n",
    "\n",
    "A detailed guide for web scraping https://finance.yahoo.com/ using **Requests**, **BeautifulSoup**, **Selenium**, **HTML tags** & embedded **JSON** data.\n",
    "\n",
    "![](https://i.imgur.com/V1bzyMs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FULGhEjlrge"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "**What is Web scraping?**<br>\n",
    "Web scraping is the process of extracting and parsing data from websites in an automated fashion using a computer program. It's a useful technique for creating datasets for research and learning.\n",
    "\n",
    "\n",
    "**Objective**<br>\n",
    "The main objective of this tutorial is to showcase different web scraping methods which can be applied to any web page. \n",
    "This is for educational purposes only. Please read the Terms & Conditions carefully for any website to see whether you can legally use the data. \n",
    "\n",
    "In this project, we will perform web scraping using the following 3 techniques based on the problem statement.\n",
    "* use `Requests`, `BeautifulSoup` and `HTML tags` to extract web page\n",
    "* use `Selenium` to scrape data from dynamically loading websites \n",
    "* use embedded `JSON` data to scrape website \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CFXNNENlrgf"
   },
   "source": [
    "**The problem statement**<br>\n",
    "1. Web Scraping **Stock Market News** (url : https://finance.yahoo.com/topic/stock-market-news/)<br>\n",
    "    This web page shows the latest **news** related to **stock market**, we will try to extract data from this web page and store it in a `CSV` (comma-separated values) file. The file layout would be as mentioned below.\n",
    "![](https://i.imgur.com/5I5SVFA.png)\n",
    "![](https://i.imgur.com/TnIWrXq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Web Scraping **Cryptocurrencies** (url : https://finance.yahoo.com/cryptocurrencies)<br>\n",
    "    This Yahoo! finance web page shows list of trending **Cryptocurrencies** in tabular format, we will perform the web scraping to retrieve first 10 columns for top 100 **Cryptocurrencies** in following `CSV` format.\n",
    "![](https://i.imgur.com/VInpTU8.png)\n",
    "![](https://i.imgur.com/KhPbg7S.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            \n",
    "3. Web Scraping **Market Events Calendar** (url : https://finance.yahoo.com/calendar)<br> \n",
    "    This page shows **date-wise market events**, user have the option to select the date and choose any one of the following market events **Earnings**, **Stock Splits**, **Economic Events** & **IPO**. Our aim is to create a script which can be run for any single date and market event which grabs the data and loads in `CSV` format as shown below.\n",
    "![](https://i.imgur.com/6ECn0AG.png)\n",
    "![](https://i.imgur.com/OAYlZ9I.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Z9UQZf8lrgg"
   },
   "source": [
    "**Prerequisites**\n",
    "* Knowledge of Python\n",
    "* Basic knowledge of HTML although it is not necessary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbMRHIhblrgh"
   },
   "source": [
    "**How to run the Code**<br>\n",
    "You can execute the code using \"Run\" button on the top of this page and selecting **\"Run on Colab\"** or **\"Run Locally\"** \n",
    "<br>\n",
    "<br>\n",
    "**Setup and Tools**<br>\n",
    "<u>Run on Colab :</u> \n",
    "    You will need to provide the Google login to run this notebook on Colab.<br>\n",
    "<u>Run Locally :</u> Download and install [Anaconda](https://www.anaconda.com/) framework, We will be using Jupyter Notebook for writing & executing code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YjeIdedlrgh"
   },
   "source": [
    "**Version control**\n",
    "\n",
    "You can make changes and save your version of the notebook to [Jovian](https://jovian.ai/) by executing following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4953,
     "status": "ok",
     "timestamp": 1646608884871,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "iLLVpn2olrgi"
   },
   "outputs": [],
   "source": [
    "!pip install jovian --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1646608884871,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "iTKJFEYglrgj"
   },
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "executionInfo": {
     "elapsed": 5436,
     "status": "ok",
     "timestamp": 1646608890288,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "AxYOBC1xlrgj",
    "outputId": "048a42bf-e240-4ca8-99c5-d64462b3c6e5"
   },
   "outputs": [],
   "source": [
    "# Execute this to save new versions of the notebook\n",
    "jovian.commit(project=\"yahoo-finance-web-scraper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SWl2eoMlrgk"
   },
   "source": [
    "## 1. Web Scraping Stock Market News\n",
    "\n",
    "In this section we will learn a basic Python web scraping technique using `Requests`, `BeautifulSoup` and `HTML tags`. The objective here is to perform web scraping of [Yahoo! finance Stock Market News](https://finance.yahoo.com/topic/stock-market-news/)\n",
    "\n",
    "![](https://i.imgur.com/1I0Btau.jpg)\n",
    "\n",
    "Let's kick-start with the first objective. Here's an outline of the steps we'll follow<br>\n",
    "**1.1 Download & Parse web page using `Requests` and `BeautifulSoup`**<br>\n",
    "**1.2 Exploring and locating Elements**<br>\n",
    "**1.3 Extract & Compile the information into python list**<br>\n",
    "**1.4 Save the extracted information to a CSV file**<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSis_3UJlrgl"
   },
   "source": [
    "### 1.1 Download & Parse webpage using Requests and BeautifulSoup\n",
    "\n",
    "First step is to install [`requests`](https://docs.python-requests.org/en/latest/) & [`beautifulsoup4`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) Libraries using `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 11167,
     "status": "ok",
     "timestamp": 1646608901448,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "zXJKsNyQlrgl"
   },
   "outputs": [],
   "source": [
    "!pip install requests --quiet\n",
    "!pip install beautifulsoup4 --quiet\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1d4YUeDlrgm"
   },
   "source": [
    "The libraries are installed and imported.<br>\n",
    "\n",
    "To download the page, we can use `requests.get`, which returns a response object. We can access the content of the web page using `response.text`<br>\n",
    "Also the `response.ok` & [`response.status_code`](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status) can be used for error trapping &  tracking.<br> \n",
    "Finally, we can use `BeautifulSoup` to parse the HTML data. This will return `bs4.BeautifulSoup` object. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_url = 'https://finance.yahoo.com/topic/stock-market-news/'\n",
    "response = requests.get(my_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response.ok : True , response.status_code : 200\n"
     ]
    }
   ],
   "source": [
    "print(\"response.ok : {} , response.status_code : {}\".format(response.ok , response.status_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of response.text :  <!DOCTYPE html><html data-color-theme=\"light\" id=\"atomic\" class=\"NoJs chrome desktop failsafe\" lang=\"en-US\"><head prefix=\"og: http://ogp.me/ns#\"><script>window.performance && window.performance.mark && window.performance.mark('PageStart');</script><meta charset=\"utf-8\" /><title>Latest Stock Market News</title><meta name=\"keywords\" content=\"401k, Business, Financial Information, Investing, Investor, Market News, Stock Research, Stock Valuation, business news, economy, finance, investment tools, m\n"
     ]
    }
   ],
   "source": [
    "print(\"Preview of response.text : \", response.text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function to perform this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1646608901616,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "-FIURHiqlrgm"
   },
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    \"\"\"Download a webpage and return a beautiful soup doc\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if not response.ok:\n",
    "        print('Status code:', response.status_code)\n",
    "        raise Exception('Failed to load page {}'.format(url))\n",
    "    page_content = response.text\n",
    "    doc = BeautifulSoup(page_content, 'html.parser')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "susW06Hwlrgn"
   },
   "source": [
    "calling function `get_page` and analyzing the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 737,
     "status": "ok",
     "timestamp": 1646608902349,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "cGWBiRCJlrgn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of doc:  <class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "doc = get_page(my_url)\n",
    "print('Type of doc: ',type(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqEwtKgTlrgn"
   },
   "source": [
    "You can access different properties of HTML web page from doc. Following example will display Title of the web page.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1646608902477,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "S0xvsoyelrgo",
    "outputId": "1e7e65ff-5772-4fde-8877-4ee80216308e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Latest Stock Market News</title>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.find('title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hbs8VJS9lrgo"
   },
   "source": [
    "We can use the function `get_page` to download any web page and parse it using beautiful soup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvSMJZeClrgo"
   },
   "source": [
    "### 1.2 Exploring and locating Elements\n",
    "Now its time to explore the elements to find the required data point from the web page. Web pages are written in a language called HTML (Hyper Text Markup Language).  HTML is a fairly simple language comprised of *tags*  (also called *nodes* or *elements*) e.g. `<a href=\"https://finance.yahoo.com/\" target=\"_blank\">Go to Yahoo! Finance</a>`. An HTML tag has three parts:\n",
    "\n",
    "\n",
    "\n",
    "1. **Name**: (`html`, `head`, `body`, `div`, etc.) Indicates what the tag represents and how a browser should interpret the information inside it.\n",
    "2. **Attributes**: (`href`, `target`, `class`, `id`, etc.) Properties of tag used by the browser to customize how a tag is displayed and decide what happens on user interactions.\n",
    "3. **Children**: A tag can contain some text or other tags or both between the opening and closing segments, e.g., `<div>Some content</div>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAFbXWN2lrgo"
   },
   "source": [
    "Let's inspect the webpage source code by right-clicking and selecting the \"Inspect\" option. First, we need to identify the tag which represents the news listing.\n",
    "\n",
    "![](https://i.imgur.com/pGwXU1J.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fr5ofFpFlrgo"
   },
   "source": [
    "In this case we can see the `<div>` tag having class name `\"Ov(h) Pend(44px) Pstart(25px)\"` is representing news listing. We can apply `find_all` method to grab this information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1646608902478,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "WF_cAVptlrgo"
   },
   "outputs": [],
   "source": [
    "div_tags = doc.find_all('div', {'class': \"Ov(h) Pend(44px) Pstart(25px)\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_Y4L3Kglrgp"
   },
   "source": [
    "Total elements in the `<div>` tag list matching with the numbers of news displaying on the webpage, so we are heading towards the right direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1646608902478,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "FniLi1nElrgp",
    "outputId": "b3190a45-5fec-44e5-a80f-491f23bbf8cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(div_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4acfugiWlrgp"
   },
   "source": [
    "Next step to inspect the individual `<div>` tag and try to find more information. I am using \"Visual Studio Code\", but you can use any tool as simple as notepad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"Ov(h) Pend(44px) Pstart(25px)\"><div class=\"C(#959595) Fz(11px) D(ib) Mb(6px)\">Bloomberg</div><h3 class=\"Mb(5px)\"><a class=\"js-content-viewer wafer-caas Fw(b) Fz(18px) Lh(23px) LineClamp(2,46px) Fz(17px)--sm1024 Lh(19px)--sm1024 LineClamp(2,38px)--sm1024 mega-item-header-link Td(n) C(#0078ff):h C(#000) LineClamp(2,46px) LineClamp(2,38px)--sm1024 not-isInStreamVideoEnabled\" data-uuid=\"ee555bdf-8ced-3ec5-8527-f11ab477237d\" data-wf-caas-prefetch=\"1\" data-wf-caas-uuid=\"ee555bdf-8ced-3ec5-8527-f11ab477237d\" href=\"/news/bond-yields-jump-asia-stocks-223246963.html\"><u class=\"StretchedBox\"></u>U.S. Index Futures Drop as Yields, Netflix Watched: Markets Wrap</a></h3><p class=\"Fz(14px) Lh(19px) Fz(13px)--sm1024 Lh(17px)--sm1024 LineClamp(2,38px) LineClamp(2,34px)--sm1024 M(0)\">(Bloomberg) -- U.S. index futures fell as investors weighed the impact of rising real yields on the appeal of riskier assets and Netflix Inc. damped the earnings-season outlook. European stocks rose with focus turning to corporate results.Most Read from BloombergNetflix Tumbles as 200,000 Users Exit for First Drop in DecadeIn Defense of Elon Musk's Managerial ExcellenceTwitter Has a Poison Pill NowPutin Calls Time on Foreign Listings in Fresh Hit to TycoonsU.S. Stops Mask Requirement on Planes A</p></div>\n"
     ]
    }
   ],
   "source": [
    "print(div_tags[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaY4rqHxlrgp"
   },
   "source": [
    "![](https://i.imgur.com/ncnfg0z.png)\n",
    "\n",
    "Luckily, most of the required data points are available in this `<div>`, so we can use `find` method to grab each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1646608902479,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "Hn02jUnUlrgp",
    "outputId": "61cb85f9-1db6-43bf-df7d-817724911115"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:  Bloomberg\n",
      "Head Line : U.S. Index Futures Drop as Yields, Netflix Watched: Markets Wrap\n"
     ]
    }
   ],
   "source": [
    "print(\"Source: \", div_tags[1].find('div').text)\n",
    "print(\"Head Line : {}\".format(div_tags[1].find('a').text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54ry5o3-lrgp"
   },
   "source": [
    "If any tag is not accessible directly, then you can use methods like `findParent()` or `'findChild()` to point to the required tag.\n",
    "\n",
    "![](https://i.imgur.com/OnOAtT2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 344,
     "status": "ok",
     "timestamp": 1646608902819,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "BYm7cqy2lrgp",
    "outputId": "346f6220-9189-4158-bd6b-3ede2fc5309b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image URL:  https://s.yimg.com/uu/api/res/1.2/sLZ4pDqUIX51l.JGdlyleg--~B/Zmk9c3RyaW07aD0xMjM7cT04MDt3PTIyMDthcHBpZD15dGFjaHlvbg--/https://s.yimg.com/uu/api/res/1.2/HAUckSPP2QERhFOj_FkbiQ--~B/aD0xMzM0O3c9MjAwMDthcHBpZD15dGFjaHlvbg--/https://media.zenfs.com/en/bloomberg_markets_842/25526e5283a4af65d4c90846e175bb89.cf.jpg\n"
     ]
    }
   ],
   "source": [
    "print(\"Image URL: \",div_tags[1].findParent().find('img')['src'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_rZ3hP_lrgq"
   },
   "source": [
    "Key Takeout from this exercise is to identify the optimal tag which will provide us required information. Mostly this is straight forward, but sometimes you will have to perform a little more research.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylJlxrEhlrgq"
   },
   "source": [
    "### 1.3 Extract & Compile the information into python list\n",
    "\n",
    "We've identified all the required tags and information. Let's put this together in the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1646608902820,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "Ck_Buvi8lrgr"
   },
   "outputs": [],
   "source": [
    "def get_news_tags(doc):\n",
    "    \"\"\"Get the list of tags containing news information\"\"\"\n",
    "    news_class = \"Ov(h) Pend(44px) Pstart(25px)\" ## class name of div tag \n",
    "    news_list  = doc.find_all('div', {'class': news_class})\n",
    "    return news_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u93jXLLslrgr"
   },
   "source": [
    "sample run of the function `get_news_tags`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1646608902820,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "byS55R-9lrgr"
   },
   "outputs": [],
   "source": [
    "my_news_tags = get_news_tags(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfXnnMASlrgr"
   },
   "source": [
    "we will create one more function, to parse individual `<div>` tags and return the information in dictionary form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1646608902820,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "bsuD1-O4lrgr"
   },
   "outputs": [],
   "source": [
    "BASE_URL = 'https://finance.yahoo.com' #Global Variable \n",
    "\n",
    "def parse_news(news_tag):\n",
    "    \"\"\"Get the news data point and return dictionary\"\"\"\n",
    "    news_source = news_tag.find('div').text #source\n",
    "    news_headline = news_tag.find('a').text #heading\n",
    "    news_url = news_tag.find('a')['href'] #link\n",
    "    news_content = news_tag.find('p').text #content\n",
    "    news_image = news_tag.findParent().find('img')['src'] #thumb image\n",
    "    return { 'source' : news_source,\n",
    "            'headline' : news_headline,\n",
    "            'url' : BASE_URL + news_url,\n",
    "            'content' : news_content,\n",
    "            'image' : news_image\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q88hBshllrgr"
   },
   "source": [
    "Testing the `parse_news` function for first `<div>` tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1646608902820,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "7gO3YYf2lrgr",
    "outputId": "4e22e5f9-a830-4747-ce31-786682e3efbf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'Barrons.com',\n",
       " 'headline': 'A Buoyant Housing Market Complicates the Fed’s Job of Taming Inflation',\n",
       " 'url': 'https://finance.yahoo.com/m/72fe26d5-feca-3515-b60d-10e0f092562c/a-buoyant-housing-market.html',\n",
       " 'content': 'Housing activity is showing minimal effects from the sharp rise in mortgage interest rates.  New housing starts increased 0.3% to a seasonally adjusted annual rate of 1.793 million units in March, the Commerce Department reported Tuesday.  The backlog of houses under construction and the large number that have been authorized but haven’t started construction means homebuilders will remain busy through the year.',\n",
       " 'image': 'https://s.yimg.com/uu/api/res/1.2/TSeqis7904utWuz0Zr3fgQ--~B/Zmk9c3RyaW07aD0xMjM7cT04MDt3PTIyMDthcHBpZD15dGFjaHlvbg--/https://s.yimg.com/uu/api/res/1.2/WYiyjKWjCHToP4bVcMKmbA--~B/aD02NDA7dz0xMjgwO2FwcGlkPXl0YWNoeW9u/https://media.zenfs.com/en/Barrons.com/37892ac8a0c8df46df2dc51c4742e76d.cf.jpg'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_news(my_news_tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4Jt8PhXlrgr"
   },
   "source": [
    "We can use the `get_news_tags` & `parse_news` functions to pars news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ruE_5Q2lrgs"
   },
   "source": [
    "### 1.4 Save the extracted information to a CSV file\n",
    "\n",
    "This is the last step of this section. We are going to use Python library [`pandas`](https://pandas.pydata.org/docs/) to save the data in CSV format. Install and then import the pandas Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 5885,
     "status": "ok",
     "timestamp": 1646608908701,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "fFiWEPAalrgs"
   },
   "outputs": [],
   "source": [
    "!pip install pandas --upgrade --quiet\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JoZJq6GTlrgs"
   },
   "source": [
    "Creating wrapper function which will call previously created helper functions.<br>\n",
    "\n",
    "The `get_page` function will download HTML page, then we can pass the result in `get_news_tags` to identify list of `<div>` tags for news.<br>\n",
    "After that we will use [List Comprehension](https://www.w3schools.com/python/python_lists_comprehension.asp) technique to parse each `<div>` tag using `parse_news`, the output will be in the form of `lists` of `dictionaries`<br>\n",
    "Finally, we will use `DataFrame` method to create pandas [dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) and use `to_csv` method to store required data in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1646608909204,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "8evbvhJklrgs"
   },
   "outputs": [],
   "source": [
    "def scrape_yahoo_news(url, path=None):\n",
    "    \"\"\"Get the yahoo finance market news and write them to CSV file \"\"\"\n",
    "    if path is None:\n",
    "        path = 'stock-market-news.csv'\n",
    "        \n",
    "    print('Requesting html page')\n",
    "    doc = get_page(url)\n",
    "\n",
    "    print('Extracting news tags')\n",
    "    news_list = get_news_tags(doc)\n",
    "\n",
    "    print('Parsing news tags')\n",
    "    news_data = [parse_news(news_tag) for news_tag in news_list]\n",
    "\n",
    "    print('Save the data to a CSV')\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    news_df.to_csv(path, index=None)\n",
    "    \n",
    "    #This return statement is optional, we are doing this just analyze the final output \n",
    "    return news_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGYzOftflrgs"
   },
   "source": [
    "Scraping the news using `scrape_yahoo_news` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 192,
     "status": "ok",
     "timestamp": 1646608909391,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "ZhSeuZ6Pi8MD",
    "outputId": "3c77f53c-62db-42f5-8764-b8bd67eee639"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting html page\n",
      "Extracting news tags\n",
      "Parsing news tags\n",
      "Save the data to a CSV\n"
     ]
    }
   ],
   "source": [
    "YAHOO_NEWS_URL = BASE_URL+'/topic/stock-market-news/'\n",
    "news_df = scrape_yahoo_news(YAHOO_NEWS_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBD6nB50lrgs"
   },
   "source": [
    "The \"stock-market-news.csv\" should be available in File $\\rightarrow$ Open Menu. You can download the file or directly open it on browser. Please verify the file content and compare it with the actual information available on the webpage.\n",
    "\n",
    "You can also check the data by grabbing a few rows from the data frame returned by the `scrape_yahoo_news` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 159,
     "status": "ok",
     "timestamp": 1646608909544,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "1segLgXIlrgs",
    "outputId": "fb2b059c-e12c-49db-88ae-ad1ab61c66c2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>headline</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barrons.com</td>\n",
       "      <td>A Buoyant Housing Market Complicates the Fed’s...</td>\n",
       "      <td>https://finance.yahoo.com/m/72fe26d5-feca-3515...</td>\n",
       "      <td>Housing activity is showing minimal effects fr...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/TSeqis7904ut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>U.S. Index Futures Drop as Yields, Netflix Wat...</td>\n",
       "      <td>https://finance.yahoo.com/news/bond-yields-jum...</td>\n",
       "      <td>(Bloomberg) -- U.S. index futures fell as inve...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/sLZ4pDqUIX51...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>European Stocks Trade Steady as Traders Weigh ...</td>\n",
       "      <td>https://finance.yahoo.com/news/european-stocks...</td>\n",
       "      <td>(Bloomberg) -- European stocks were little cha...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/4qJEemBtZKvz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>Commodities Broker Marex Posts Record Profit o...</td>\n",
       "      <td>https://finance.yahoo.com/news/commodities-bro...</td>\n",
       "      <td>(Bloomberg) -- London commodities brokerage Ma...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/7FOZ3VTJiK4j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Barrons.com</td>\n",
       "      <td>Restaurant, Retail, and Travel Stocks Are Drop...</td>\n",
       "      <td>https://finance.yahoo.com/m/1d8dcb63-b28f-3f9a...</td>\n",
       "      <td>Rising interest rates are expected to reduce s...</td>\n",
       "      <td>https://s.yimg.com/uu/api/res/1.2/wVcBPISpEQl_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        source                                           headline  \\\n",
       "0  Barrons.com  A Buoyant Housing Market Complicates the Fed’s...   \n",
       "1    Bloomberg  U.S. Index Futures Drop as Yields, Netflix Wat...   \n",
       "2    Bloomberg  European Stocks Trade Steady as Traders Weigh ...   \n",
       "3    Bloomberg  Commodities Broker Marex Posts Record Profit o...   \n",
       "4  Barrons.com  Restaurant, Retail, and Travel Stocks Are Drop...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://finance.yahoo.com/m/72fe26d5-feca-3515...   \n",
       "1  https://finance.yahoo.com/news/bond-yields-jum...   \n",
       "2  https://finance.yahoo.com/news/european-stocks...   \n",
       "3  https://finance.yahoo.com/news/commodities-bro...   \n",
       "4  https://finance.yahoo.com/m/1d8dcb63-b28f-3f9a...   \n",
       "\n",
       "                                             content  \\\n",
       "0  Housing activity is showing minimal effects fr...   \n",
       "1  (Bloomberg) -- U.S. index futures fell as inve...   \n",
       "2  (Bloomberg) -- European stocks were little cha...   \n",
       "3  (Bloomberg) -- London commodities brokerage Ma...   \n",
       "4  Rising interest rates are expected to reduce s...   \n",
       "\n",
       "                                               image  \n",
       "0  https://s.yimg.com/uu/api/res/1.2/TSeqis7904ut...  \n",
       "1  https://s.yimg.com/uu/api/res/1.2/sLZ4pDqUIX51...  \n",
       "2  https://s.yimg.com/uu/api/res/1.2/4qJEemBtZKvz...  \n",
       "3  https://s.yimg.com/uu/api/res/1.2/7FOZ3VTJiK4j...  \n",
       "4  https://s.yimg.com/uu/api/res/1.2/wVcBPISpEQl_...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptBKMG_Alrgt"
   },
   "source": [
    "**Summary** : Hopefully I was able to explain this simple but very powerful Python technique to scrape the Yahoo! finance market news. These steps can be used to scrape any web page. You just have to do a little research to identify the required `<tags>` and use relevant python methods to collect the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaOUKkUOlrgt"
   },
   "source": [
    "## 2. Web Scraping Cryptocurrencies\n",
    "\n",
    "In phase One we were able to scrape the [yahoo market news](https://finance.yahoo.com/topic/stock-market-news/) web page. However, if you've noticed, as we scroll down the webpage more news will appear at the bottom of the page. This is called dynamic page loading. The previous technique is a basic Python method useful to scrape static data, To scrape the dynamically loading data will use a different method called web scraping using **Selenium**. Let's move ahead with this topic. The goal of this section is to extract top listing [Crypto currencies](https://finance.yahoo.com/cryptocurrencies) from Yahoo! finance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuViJR0Elrgt"
   },
   "source": [
    "![](https://i.imgur.com/sF6k0Pk.jpg)\n",
    "\n",
    "\n",
    "Here's an outline of the steps we'll follow<br>\n",
    "**2.1 Introduction of selenium**<br>\n",
    "**2.2 Downloads & Installation**<br>\n",
    "**2.3 Install & Import libraries**<br>\n",
    "**2.4 Create Web Driver**<br>\n",
    "**2.5 Exploring and locating Elements**<br>\n",
    "**2.6 Extract & Compile the information into a python list**<br>\n",
    "**2.7 Save the extracted information to a CSV file**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvUPJgUEQWZo"
   },
   "source": [
    "### 2.1 Introduction of selenium\n",
    "\n",
    "**[Selenium](https://www.selenium.dev/)** is an open-source web-based automation tool. Python language and other languages are used with Selenium for testing as well as web scraping. Here we will use Chrome browser, but you can try on any browser.<br>\n",
    "\n",
    "**Why you should use Selenium?**\n",
    "- Clicking on buttons\n",
    "- Filling forms\n",
    "- Scrolling\n",
    "- Taking a screen-shot\n",
    "- Refreshing the page\n",
    "\n",
    "You can find proper documentation on selenium [here](https://selenium-python.readthedocs.io/)<br>\n",
    "\n",
    "The following methods will help to find elements in a webpage (these methods will return a list):\n",
    "- `find_elements_by_name`\n",
    "- `find_elements_by_xpath`\n",
    "- `find_elements_by_link_text`\n",
    "- `find_elements_by_partial_link_text`\n",
    "- `find_elements_by_tag_name`\n",
    "- `find_elements_by_class_name`\n",
    "- `find_elements_by_css_selector`\n",
    "\n",
    "In this tutorial we will use only `find_elements_by_xpath` and `find_elements_by_tag_name` You can find complete documentation of these methods [here](https://selenium-python.readthedocs.io/locating-elements.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5IXSU3_QWZs"
   },
   "source": [
    "### 2.2 Downloads & Installation \n",
    "\n",
    "Unlike the previous section, here we'll have to do some prep work to implement this method. We will need to install Selenium & proper web browser driver<br>\n",
    "\n",
    "If you are using **Google Colab** platform then execute following code to perform Initial installation. This piece of code `'google.colab' in str(get_ipython())` is used to identify the Google Colab platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35298,
     "status": "ok",
     "timestamp": 1646608944835,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "ot7iEH0QQWZs",
    "outputId": "b6f3b009-28b7-4d91-9da7-8a6b827ae66b"
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Google CoLab Installation')\n",
    "    !apt update --quiet\n",
    "    !apt install chromium-chromedriver --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kj9N6faQWZs"
   },
   "source": [
    "To run it on **Locally** you will need **Webdriver for Chrome** on your machine. You can download it from this link https://chromedriver.chromium.org/downloads and just copy the file in the folder where we will create the python file (No need of installation). But make sure that the driver‘s version matches the Chrome browser version installed on the local machine.\n",
    "\n",
    "![](https://i.imgur.com/FvQ586e.gif)\n",
    "![](https://i.imgur.com/wQbjRIU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGOzu85gQWZs"
   },
   "source": [
    "### 2.3 Install & Import libraries\n",
    "\n",
    "Installation of the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9542,
     "status": "ok",
     "timestamp": 1646608954376,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "WmDBSb_qQWZt",
    "outputId": "33872365-9988-453f-ed9b-dc2fb6840d89"
   },
   "outputs": [],
   "source": [
    "!pip install selenium --quiet\n",
    "!pip install pandas --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pn1J-U1PQWZt"
   },
   "source": [
    "Once the Libraries installation is done, next step is to import all the required modules / libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1646608954377,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "PaHNd1hSQWZt",
    "outputId": "a159e25b-7ccd-44af-9bf9-af469657b81c"
   },
   "outputs": [],
   "source": [
    "print('Library Import')\n",
    "if 'google.colab' not in str(get_ipython()):\n",
    "    print('Not running on CoLab')\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    import os\n",
    "else:\n",
    "    print('Running on CoLab')\n",
    "    \n",
    "print('Common Library Import')\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlfwVrnVQWZt"
   },
   "source": [
    "So all the necessary prep work is done. Let's  move ahead to implement this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ms87q9huQWZu"
   },
   "source": [
    "### 2.4 Create Web Driver\n",
    "\n",
    "In this step first we will create the instance of Chrome WebDriver using `webdriver.Chrome()` method. and then the `driver.get()` method will navigate to a page given by the URL. In this case also there is slight variation based on platform. Also we have used `options` parameters for e.g. `--headless` option will load the driver in background. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1646608954377,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "cAn7GekCQWZu",
    "outputId": "1a01cc95-5eeb-4943-c921-3943dc78aedb"
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Running on CoLab')\n",
    "    def get_driver(url):\n",
    "        \"\"\"Return web driver\"\"\"\n",
    "        colab_options = webdriver.ChromeOptions()\n",
    "        colab_options.add_argument('--no-sandbox')\n",
    "        colab_options.add_argument('--disable-dev-shm-usage')\n",
    "        colab_options.add_argument('--headless')\n",
    "        colab_options.add_argument('--start-maximized') \n",
    "        colab_options.add_argument('--start-fullscreen')\n",
    "        colab_options.add_argument('--single-process')\n",
    "        driver = webdriver.Chrome(options=colab_options)\n",
    "        driver.get(url)\n",
    "        return driver\n",
    "else:\n",
    "    print('Not running on CoLab')\n",
    "    def get_driver(url):\n",
    "        \"\"\"Return web driver\"\"\"\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        chrome_options.add_argument('--headless')\n",
    "        chrome_options.add_argument('--start-maximized') \n",
    "        chrome_options.add_argument('--start-fullscreen')\n",
    "        chrome_options.add_argument('--single-process')\n",
    "        serv = Service(os.getcwd()+'/chromedriver')\n",
    "        driver = webdriver.Chrome(options=chrome_options, service=serv)\n",
    "        driver.get(url)\n",
    "        return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnCtyw66QWZu"
   },
   "source": [
    "Test run of `get_driver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4593,
     "status": "ok",
     "timestamp": 1646608958957,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "rd7DJYjaQWZu"
   },
   "outputs": [],
   "source": [
    "driver = get_driver('https://finance.yahoo.com/cryptocurrencies')\n",
    "print(driver.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOF6149XQWZu"
   },
   "source": [
    "### 2.5 Exploring and locating Elements\n",
    "\n",
    "This is almost similar step that we have done in phase 1. We will try to identify relevant information like `<tags>`, `class` , `XPath` etc from the web page. Right-click and select the \"Inspect\" to do further analysis.\n",
    "\n",
    "As the webpage showing cryptocurrency information in the Table form. We can grab the table header by using tag `<th>`, we will use find_elements by TAG to get the table headers. These headers can be used as columns for a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1646608959371,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "zR29b9xH8di9",
    "outputId": "23b77d94-c3dd-4ed8-997a-efed9779befb"
   },
   "outputs": [],
   "source": [
    "header = driver.find_elements(By.TAG_NAME, value= 'th')\n",
    "print(header[0].text)\n",
    "print(header[2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNZ0kwDhQWZv"
   },
   "source": [
    "Creating a helper function to get first 10 columns from header, we have used List comprehension with conditions. You can also check out usage of `enumerate` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 138,
     "status": "ok",
     "timestamp": 1646608959507,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "QTfDpiWrQWZv"
   },
   "outputs": [],
   "source": [
    "def get_table_header(driver):\n",
    "    \"\"\"Return Table columns in list form \"\"\"\n",
    "    header = driver.find_elements(By.TAG_NAME, value= 'th')\n",
    "    header_list = [item.text for index, item in enumerate(header) if index < 10]\n",
    "    return header_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqW7UgNwQWZv"
   },
   "source": [
    "Next we find out number of rows available in a Page, you can see table rows are placed in `<tr>` tag, we can capture the `XPath` by selection `<tr>` tag the Right Click $\\rightarrow$ Copy $\\rightarrow$ Copy XPath.\n",
    "\n",
    "![](https://i.imgur.com/DVAYMzY.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlfJ632vQWZv"
   },
   "source": [
    "So we get the  XPath value as `//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr[1]`, Let's use this with `find_element()` & `By.XPATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1646608959774,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "U3FVWjftQWZv",
    "outputId": "439acb74-631d-496e-cf3b-a3da959a6a13"
   },
   "outputs": [],
   "source": [
    "txt=driver.find_element(By.XPATH, value='//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr[1]').text\n",
    "txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6JcCdb1Trrp"
   },
   "source": [
    "Above `XPath` points to first row, we can get rid of row number part from XPath and use it with `find_elements` to get hold of all the available rows. Let's implement this in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1646608959898,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "dd9kJccjToHK"
   },
   "outputs": [],
   "source": [
    "def get_table_rows(driver):\n",
    "    \"\"\"Get number of rows available on the page \"\"\"\n",
    "    tablerows = len(driver.find_elements(By.XPATH, value='//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr'))\n",
    "    return tablerows    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 142,
     "status": "ok",
     "timestamp": 1646608960038,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "0v1phXVMRn2E",
    "outputId": "290bc4de-5360-4252-f5c2-54cc09dced85"
   },
   "outputs": [],
   "source": [
    "print(get_table_rows(driver))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A45Ekufq8di9"
   },
   "source": [
    "Similarly, we can take the XPath for any column value.\n",
    "\n",
    "![](https://i.imgur.com/aT3I3Ur.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0dHoGoVVzTv"
   },
   "source": [
    "This is the XPAth for a column `//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr[1]/td[2]`.<br>\n",
    "If you noticed the number after `tr` & `td` represents the `row_number` and `column_number`, we can check this with `find_element()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 367,
     "status": "ok",
     "timestamp": 1646608960404,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "1MNZ9bGilrgt",
    "outputId": "f5e1a1ae-485c-4350-8507-a3eed58b9a20"
   },
   "outputs": [],
   "source": [
    "driver.find_element(By.XPATH, value='//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr[1]/td[2]').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2KhgeZAW7d8"
   },
   "source": [
    "So we can change the `row_number` & `column_number` in `XPath` and loop it through row count and column count to get all the available column values. Let's generalize this and put it in a function. We will get the data for one row at a time and return column values in the form of a dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1646608960404,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "pEorS2flYikH"
   },
   "outputs": [],
   "source": [
    "def parse_table_rows(rownum, driver, header_list):\n",
    "    \"\"\"get the data for one row at a time and return column value in the form of dictionary\"\"\"\n",
    "    row_dictionary = {}\n",
    "    #time.sleep(1/3)\n",
    "    for index , item in enumerate(header_list):\n",
    "        time.sleep(1/20)\n",
    "        column_xpath = '//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr[{}]/td[{}]'.format(rownum, index+1)\n",
    "        row_dictionary[item] = driver.find_element(By.XPATH, value=column_xpath).text\n",
    "    return row_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96fzN44rXu-u"
   },
   "source": [
    "The Yahoo! Finance web page shows only 25 Cryptocurrencies per page and user will have to click `Next` button to load next set of crypto currencies. This is called **Pagination**. This is the main reason we are implementing selenium methods to handle events like pagination. you can perform multiple events like clicking, scrolling , refreshing etc. on a webpage using selenium methods.\n",
    "\n",
    "Now we will grab the `XPath` of `Next` button, find the element using `find_element` method, and after that we can perform click action using `.click()` method \n",
    "\n",
    "![](https://i.imgur.com/tCxQKfR.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 126,
     "status": "ok",
     "timestamp": 1646608960528,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "s45SMjxligLN"
   },
   "outputs": [],
   "source": [
    "button_element = driver.find_element(By.XPATH, value = '//*[@id=\"scr-res-table\"]/div[2]/button[3]')\n",
    "button_element.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=driver.find_element(By.XPATH, value='//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr[1]').text\n",
    "txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am trying to check the first row on the web page to verify if `.click()` really worked, and you will see first row has changed. Click action was successful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osU8bvFYeIgu"
   },
   "source": [
    "In this section we have learned how to get required data points, and perform events on webpage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 130,
     "status": "ok",
     "timestamp": 1646608960657,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "J0CKtbmTf5aF"
   },
   "outputs": [],
   "source": [
    "#terminating driver from test runs \n",
    "driver.close()\n",
    "driver.quit() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdpxUXQCd5LH"
   },
   "source": [
    "### 2.6 Extract & Compile the information into python list\n",
    "\n",
    "Let's put all the pieces in the puzzle, we will pass the integer `total_crypto` i.e. numbers of rows to be scraped (in this case 100 rows) in the function. Parse each row from the page and append the data in the `List` till the total parsed row count reach to `total_crypto`. In addition, we will perform `Next` button click if we are at the last row of the table. \n",
    "\n",
    "**Please Note** : Here to identify the `Next` button element we have used [WebDriverWait](https://www.selenium.dev/selenium/docs/api/java/org/openqa/selenium/support/ui/WebDriverWait.html) class instead of using `find_element()` method. In this technique we can pass some wait-time before grabbing the element. This type of implementation is done to avoid the [`StaleElementReferenceException`](https://stackoverflow.com/questions/27003423/staleelementreferenceexception-on-python-selenium).\n",
    "\n",
    "Code Sample:\n",
    "```\n",
    "element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"scr-res-table\"]/div[2]/button[3]')))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1646608960657,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "FgH4XUG9SNAD"
   },
   "outputs": [],
   "source": [
    "def parse_multiple_pages(driver, total_crypto):\n",
    "    \"\"\"Loop through each row, perform Next button click at the end of page \n",
    "    return total_crypto numbers of rows \n",
    "    \"\"\"\n",
    "    table_data = []\n",
    "    page_num = 1\n",
    "    is_scraping = True\n",
    "    header_list = get_table_header(driver)\n",
    "\n",
    "    while is_scraping:\n",
    "        table_rows = get_table_rows(driver)\n",
    "        print('Found {} rows on Page : {}'.format(table_rows, page_num))\n",
    "        print('Parsing Page : {}'.format(page_num))\n",
    "        table_data += [parse_table_rows(i, driver, header_list) for i in range (1, table_rows + 1)]\n",
    "        total_count = len(table_data)\n",
    "        print('Total rows scraped : {}'.format(total_count))\n",
    "        if total_count >= total_crypto:\n",
    "            print('Done Parsing..')\n",
    "            is_scraping = False\n",
    "        else:    \n",
    "            print('Clicking Next Button')\n",
    "            element = WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"scr-res-table\"]/div[2]/button[3]')))\n",
    "            element.click() \n",
    "            page_num += 1\n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZL49rJqd5aa"
   },
   "source": [
    "### 2.7 Save the extracted information to a CSV file.\n",
    "\n",
    "This is the last step of this section, we are creating a last function which will be the placeholder for all helper functions and at the and we will save the data in CSV format using `pd.to_csv` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1646608960657,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "a8AxKJLrYikH"
   },
   "outputs": [],
   "source": [
    "def scrape_yahoo_crypto(url, total_crypto, path=None):\n",
    "    \"\"\"Get the list of yahoo finance crypto-currencies and write them to CSV file \"\"\"\n",
    "    if path is None:\n",
    "        path = 'crypto-currencies.csv'\n",
    "    print('Creating driver')\n",
    "    driver = get_driver(url)    \n",
    "    table_data = parse_multiple_pages(driver, total_crypto)\n",
    "    driver.close()\n",
    "    driver.quit()\n",
    "    print('Save the data to a CSV')\n",
    "    table_df = pd.DataFrame(table_data)\n",
    "    table_df.to_csv(path, index=None)\n",
    "    #This return statement is optional, we are doing this just analyze the final output \n",
    "    return table_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJc4JNBaSNAD"
   },
   "source": [
    "Time to scrape some cryptos!!! , we will scrape top 100 cryptos in Yahoo! Finance webpage by calling `scrape_yahoo_crypto` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 90871,
     "status": "ok",
     "timestamp": 1646609051527,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "1fEtHtL2jXxR",
    "outputId": "7a159beb-3cf1-40d5-b02c-33d5793ff853"
   },
   "outputs": [],
   "source": [
    "YAHOO_FINANCE_URL = BASE_URL+'/cryptocurrencies'\n",
    "TOTAL_CRYPTO = 100\n",
    "crypto_df = scrape_yahoo_crypto(YAHOO_FINANCE_URL, TOTAL_CRYPTO,'crypto-currencies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tf7cgOSVYikI"
   },
   "source": [
    "The \"crypto-currencies.csv\" should be available in File $\\rightarrow$ Open Menu. You can download the file or directly open it on browser. Please verify the file content and compare it with the actual information available on the webpage.\n",
    "\n",
    "You can also check the data by grabbing a few rows from the data frame returned by the `scrape_yahoo_crypto` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1646609051527,
     "user": {
      "displayName": "Vinod Dhole",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09410569866548331327"
     },
     "user_tz": 300
    },
    "id": "4-fahUk4SNAE",
    "outputId": "52809182-57ce-4ffc-a0ed-dd5c563877d2"
   },
   "outputs": [],
   "source": [
    "crypto_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ion8RYbZ8di_"
   },
   "source": [
    "**Summary** : Hope you've enjoyed this tutorial. Selenium enables us to perform multiple actions on the web browser, which is really very handy for scraping different types of data from any webpage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-klaoqb8di_"
   },
   "source": [
    "## 3. Web Scraping Market Events Calendar\n",
    "\n",
    "This is the final segment of the tutorial in this section, we will learn how to extract embedded [JSON](https://www.w3schools.com/js/js_json_intro.asp) formatted data which can be easily converted to Python dictionary. Problem statement for section is to scrape date-wise market events from [Yahoo! finance](https://finance.yahoo.com/calendar)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EsDeulcAJAV"
   },
   "source": [
    "![](https://i.imgur.com/bKQoAjs.png)\n",
    "\n",
    "Here's an outline of the steps we'll follow<br>\n",
    "**3.1 Install & Import libraries**<br>\n",
    "**3.2 Download & Parse web page**<br>\n",
    "**3.3 Get Embedded Json data**<br>\n",
    "**3.4 Locating Json Keys**<br>\n",
    "**3.5 Pagination & Compiling the information into a python list**<br>\n",
    "**3.6 Save the extracted information to a CSV file**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30uQai53AJAV"
   },
   "source": [
    "### 3.1 Install & Import libraries\n",
    "\n",
    "First step to install and import Python Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5h5BUiRoAJAV"
   },
   "outputs": [],
   "source": [
    "!pip install requests --quiet\n",
    "!pip install beautifulsoup4 --quiet\n",
    "!pip install pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2m7QniEX8di_"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HV8zDkvJAJAV"
   },
   "source": [
    "### 3.2 Download & Parse web page\n",
    "\n",
    "This is exactly the same step that we've performed to download webpage in section 1.1, Here we have used [custom header](https://docs.python-requests.org/en/master/user/quickstart/#custom-headers) in `requests.get()`\n",
    "\n",
    "Most of the things are explained in section 1.1, creating the helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qFhyZG558di_"
   },
   "outputs": [],
   "source": [
    "def get_event_page(scraper_url):\n",
    "    \"\"\"Download a webpage and return a beautiful soup doc\"\"\"\n",
    "    headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
    "                  \"(KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19582\"\n",
    "    }\n",
    "    response = requests.get(scraper_url, headers=headers)\n",
    "    if not response.ok:\n",
    "        print('Status code:', response.status_code)\n",
    "        raise Exception('Failed to fetch web page ' + scraper_url)\n",
    "    # Construct a beautiful soup document\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qBYaeOWDAJAV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Company Earnings Calendar - Yahoo Finance</title>\n"
     ]
    }
   ],
   "source": [
    "doc = get_event_page('https://finance.yahoo.com/calendar/earnings?from=2022-02-27&to=2022-03-05&day=2022-02-28')\n",
    "print(doc.find('title'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7TGB4SZAJAV"
   },
   "source": [
    "### 3.3 Get Embedded Json data\n",
    "\n",
    "\n",
    "In this step we will locate the Jason formatted data, Open the web page and do Right Click $\\rightarrow$ View Page Source, If you scroll down to source page you will notice the [Json](https://www.w3schools.com/whatis/whatis_json.asp) formatted data. Apparently this information is `<script>` tag which contains the following text `/* -- Data -- */`.\n",
    "\n",
    "![](https://i.imgur.com/2xlpNbw.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcePCvsCAJAV"
   },
   "source": [
    "We will use [Regular expressions](https://docs.python.org/3/library/re.html) to get text  inside `<script>` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YkAMhCnRAJAW"
   },
   "outputs": [],
   "source": [
    "pattern = re.compile(r'\\s--\\sData\\s--\\s')\n",
    "#script_data = doc.find('script', text=pattern).text\n",
    "script_data = doc.find('script', text=pattern).contents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_t2zpCGAJAW"
   },
   "source": [
    "Further, the `Json` formatted string has the first key as `context` and it ends at 12 characters from the end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "AecrY-WkAJAW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(function (root) {\n",
      "/* -- Data -- */\n",
      "root.App || (root.App = {});\n",
      "root.App.now = 1650440790947;\n",
      "root.App.main = {\"context\":{\"dispatcher\":{\"stores\":{\"P\n",
      "odal\":{\"strings\":1},\"tdv2-wafer-header\":{\"strings\":1},\"yahoodotcom-layout\":{\"strings\":1}}},\"options\":{\"defaultBundle\":\"td-app-finance\"}}}};\n",
      "}(this));\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(script_data[:150])\n",
    "print(script_data[-150:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiYPaUpPAJAW"
   },
   "source": [
    "We can grab the Json string using Python slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-WYEbgQWAJAW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"context\":{\"dispatcher\":{\"stores\":{\"PageStore\":{\"currentPageName\":\"calendar\",\"currentEvent\":{\"event\n"
     ]
    }
   ],
   "source": [
    "start  = script_data.find('context')-2\n",
    "json_text  = script_data[start:-12]\n",
    "print(json_text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "605zLEjAAJAW"
   },
   "source": [
    "Using `json.loads()`method to convert Jason string into Python Dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ShAdpupKAJAW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_dictionary = json.loads(json_text)\n",
    "type(parsed_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUjp0BqkAJAW"
   },
   "source": [
    "Creating a function using above information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rgcsfbSx8di_"
   },
   "outputs": [],
   "source": [
    "def get_json_dictionary(doc):\n",
    "    \"\"\"Get Json formated data in the form of Python Dictionary\"\"\"\n",
    "    pattern = re.compile(r'\\s--\\sData\\s--\\s')\n",
    "    script_data = doc.find('script', text=pattern).text\n",
    "    script_data = doc.find('script', text=pattern).contents[0]\n",
    "    \n",
    "    start  = script_data.find('context')-2\n",
    "    json_text  = script_data[start:-12]\n",
    "    \n",
    "    parsed_dictionary = json.loads(json_text)\n",
    "    return parsed_dictionary    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIolGWTkAJAW"
   },
   "source": [
    "### 3.4 Locating Json Keys\n",
    "\n",
    "So basically the Json text is multi level nested dictionaries, and some keys are used to store all the meta data displayed on the webpage. In this section we will identify the keys for the data we are trying to scrape.\n",
    "\n",
    "We'll need some `Json Formatter` tool to navigate through multiple keys, I am using online tool https://jsonblob.com/. However, you can choose any tool.\n",
    "\n",
    "We will write the Json text into `my_json_file.json` file, then grab the file content and paste it to the left panel of https://jsonblob.com/. The JSON Blob it will do nice formatting. We can easily navigate through each Keys and search any item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SUWFo_MG8di_"
   },
   "outputs": [],
   "source": [
    "with open('my_json_file.json', 'w', encoding=\"utf-8\") as file:\n",
    "    file.write(json_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8FwV_y1AJAW"
   },
   "source": [
    "Next step is to find the Required Key location. Let's search the company name `3D Systems Corporation` displayed in the webpage in the [JSON Blob](https://jsonblob.com/) formatter.\n",
    "![](https://i.imgur.com/Iv8b7vl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XklVfiv-AJAW"
   },
   "source": [
    "![](https://i.imgur.com/jpJYOCy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKY1688xAJAX"
   },
   "source": [
    "You can see the table data is stored in the `rows` key, and we can track down the parent keys as shown in the above screen, checkout the content of `row` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "00fa9Sx9AJAX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ticker': 'DDD',\n",
       "  'companyshortname': '3D Systems Corporation',\n",
       "  'startdatetime': '2022-02-28T16:05:00.000Z',\n",
       "  'startdatetimetype': 'TAS',\n",
       "  'epsestimate': 0.03,\n",
       "  'epsactual': 0.09,\n",
       "  'epssurprisepct': 181.25,\n",
       "  'timeZoneShortName': 'EST',\n",
       "  'gmtOffsetMilliSeconds': -18000000,\n",
       "  'quoteType': 'EQUITY'},\n",
       " {'ticker': 'BDRFF',\n",
       "  'companyshortname': 'Beiersdorf Aktiengesellschaft',\n",
       "  'startdatetime': '2022-03-01T02:15:00.000Z',\n",
       "  'startdatetimetype': 'TAS',\n",
       "  'epsestimate': None,\n",
       "  'epsactual': None,\n",
       "  'epssurprisepct': None,\n",
       "  'timeZoneShortName': 'EST',\n",
       "  'gmtOffsetMilliSeconds': -18000000,\n",
       "  'quoteType': 'EQUITY'},\n",
       " {'ticker': 'AY',\n",
       "  'companyshortname': 'Atlantica Sustainable Infrastructure plc',\n",
       "  'startdatetime': '2022-02-28T16:22:00.000Z',\n",
       "  'startdatetimetype': 'TAS',\n",
       "  'epsestimate': -0.04,\n",
       "  'epsactual': -0.1,\n",
       "  'epssurprisepct': -122.22,\n",
       "  'timeZoneShortName': 'EST',\n",
       "  'gmtOffsetMilliSeconds': -18000000,\n",
       "  'quoteType': 'EQUITY'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_dictionary['context']['dispatcher']['stores']['ScreenerResultsStore']['results']['rows'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "mnq4KHKXAJAX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows on the Current page : 100\n"
     ]
    }
   ],
   "source": [
    "print('Total Rows on the Current page :',len(parsed_dictionary['context']['dispatcher']['stores']['ScreenerResultsStore']['results']['rows']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6khD1Pn6AJAX"
   },
   "source": [
    "This sub-dictionary shows all the data displayed on the current page.<br>\n",
    "You can do more research and exploration to get different information from the web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "DcGw1gDRAJAX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows for the search criteria : 273\n"
     ]
    }
   ],
   "source": [
    "print('Total Rows for the search criteria :',parsed_dictionary['context']['dispatcher']['stores']['ScreenerResultsStore']['results']['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "I7_WMaROAJAX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'data': 'ticker', 'content': 'Symbol'},\n",
       " {'data': 'companyshortname', 'content': 'Company Name'},\n",
       " {'data': 'startdatetime', 'content': 'Event Start Date'},\n",
       " {'data': 'startdatetimetype', 'content': 'Event Start Time'},\n",
       " {'data': 'epsestimate', 'content': 'EPS Estimate'},\n",
       " {'data': 'epsactual', 'content': 'Reported EPS'},\n",
       " {'data': 'epssurprisepct', 'content': 'Surprise (%)'},\n",
       " {'data': 'timeZoneShortName', 'content': 'Timezone short name'},\n",
       " {'data': 'gmtOffsetMilliSeconds', 'content': 'GMT Offset'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Columns\")\n",
    "parsed_dictionary['context']['dispatcher']['stores']['ScreenerResultsStore']['results']['columns']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbgxoTdPAJAX"
   },
   "source": [
    "Putting this in a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "wyX1dtPy8di_"
   },
   "outputs": [],
   "source": [
    "def get_total_rows(parsed_dictionary):\n",
    "    '''Get the Total Rows for the search criteria & Columns detail''' \n",
    "    total_rows = parsed_dictionary['context']['dispatcher']['stores']['ScreenerResultsStore']['results']['total']\n",
    "    return total_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "435fS9Vv8di_"
   },
   "outputs": [],
   "source": [
    "def get_page_rows(parsed_dictionary):\n",
    "    \"\"\"Get the Content current page\"\"\"    \n",
    "    data_dictionary = parsed_dictionary['context']['dispatcher']['stores']['ScreenerResultsStore']['results']['rows']\n",
    "    return data_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoJ-lzeNAJAX"
   },
   "source": [
    "### 3.5 Pagination & Compiling the information into python list\n",
    "\n",
    "As we saw in the previous section on how to handle `Pagination` using selenium methods, here we'll learn a new technique for accessing multiple pages.<br>\n",
    "\n",
    "Most of the times webpage url gets changed at runtime depending on the user selection, e.g. In the below screen-shot, I selected the **Earnings** for **1-March-2022**. You can notice how that information is passed in the URL. \n",
    "![](https://i.imgur.com/h5QU99h.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8g-j5xJAJAX"
   },
   "source": [
    "Similarly, when i click next button, `offset`& `size` values gets changed in the url.\n",
    "![](https://i.imgur.com/jYa1vq5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5LQ_YkvAJAX"
   },
   "source": [
    "So we can figure out the pattern & structure of the url and how it affects page navigation.<br> \n",
    "\n",
    "In this case webpage url pattern is mentioned below:<br>\n",
    "- The following values are used for calendar event types \n",
    "`event_types = ['splits','economic','ipo','earnings']`\n",
    "- Date passed in `yyyy-mm-dd` format\n",
    "- Page number is controlled by `offset` value (for first page `offset=0`)\n",
    "- Maximum number of rows in a page is assigned to `size`\n",
    "\n",
    "Based on the above information, we can build the URL at runtime and download the page, then extract the information. This is how we handle pagination.<br>\n",
    "\n",
    "Putting all things together in a function. In this function we will pass `event_type` and `date`, then we will calculate the total rows for matching criteria using `get_total_rows` function. Maximum rows per page are constant (i.e. 100), so we can build iterating summation logic to calculate the total number of pages involved for the current criteria and extract each page data in the loop.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3magUBX48di_"
   },
   "outputs": [],
   "source": [
    "def scrape_all_pages(event_type, date):\n",
    "    \"\"\"Loop through each row and return lists of data dictionary\"\"\"\n",
    "    YAHOO_CAL_URL = BASE_URL+'/calendar/{}?day={}&offset={}&size={}'\n",
    "    max_rows_per_page = '100' # this indicates max rows per page \n",
    "    page_number = 1\n",
    "    final_data_dictionary = []\n",
    "    \n",
    "    while page_number > 0:\n",
    "        print(\"Processing page # {}\".format(page_number))\n",
    "        page_url = str((page_number - 1 ) * int(max_rows_per_page))\n",
    "        scrape_url = YAHOO_CAL_URL.format(event_type, date, page_url, max_rows_per_page)\n",
    "        print(\"Scrape url for page {} is {}\".format(page_number,scrape_url))\n",
    "        page_doc = get_event_page(scrape_url)\n",
    "        parse_dict = get_json_dictionary(page_doc)\n",
    "        if page_number == 1:\n",
    "            total_rows = get_total_rows(parse_dict)        \n",
    "        final_data_dictionary += get_page_rows(parse_dict)\n",
    "        if len(final_data_dictionary) >= total_rows:\n",
    "            page_number = 0\n",
    "            return final_data_dictionary\n",
    "        page_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating one more variant of same function for the date range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_pages_daterange(event_type, dates):\n",
    "    \"\"\"Loop through each row and return lists of data dictionary\"\"\"\n",
    "    YAHOO_CAL_URL = BASE_URL+'/calendar/{}?day={}&offset={}&size={}'\n",
    "    max_rows_per_page = '100' # this indicates max rows per page \n",
    "    data_dictionary_date = []\n",
    "    for date in dates:\n",
    "        print(\"Processing for date : {}\".format(date))\n",
    "        page_number = 1\n",
    "        final_data_dictionary = []\n",
    "    \n",
    "        while page_number > 0:\n",
    "            print(\"Processing page # {}\".format(page_number))\n",
    "            page_url = str((page_number - 1 ) * int(max_rows_per_page))\n",
    "            scrape_url = YAHOO_CAL_URL.format(event_type, date, page_url, max_rows_per_page)\n",
    "            print(\"Scrape url for page {} is {}\".format(page_number,scrape_url))\n",
    "            page_doc = get_event_page(scrape_url)\n",
    "            parse_dict = get_json_dictionary(page_doc)\n",
    "            if page_number == 1:\n",
    "                total_rows = get_total_rows(parse_dict)        \n",
    "            final_data_dictionary += get_page_rows(parse_dict)\n",
    "            if len(final_data_dictionary) >= total_rows:\n",
    "                page_number = 0\n",
    "                data_dictionary_date += [{'primary-key':date, **item} for item in final_data_dictionary]\n",
    "            else:    \n",
    "                page_number += 1\n",
    "        print(\"Processing done\")    \n",
    "    return data_dictionary_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scLr8_DFAJAY"
   },
   "source": [
    "### 3.6 Save the extracted information to a CSV file.\n",
    "\n",
    "In this last section, we will save the data to csv format using `pd.DataFrame()` & `to_csv()` and call everything in a single placeholder function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "VLfoy_33QWZz"
   },
   "outputs": [],
   "source": [
    "def scrape_yahoo_calendar(event_types, date_param):\n",
    "    \"\"\"Get the list of yahoo finance calendar and write them to CSV file \"\"\"\n",
    "    for event in event_types:\n",
    "        data_dict = {}\n",
    "        print('Web Scraping for ', event  )\n",
    "        data_dict = scrape_all_pages(event, date_param)\n",
    "        if len(data_dict) > 0:\n",
    "            scraped_df = pd.DataFrame(data_dict)\n",
    "            scraped_df.to_csv(event+'_'+date_param+'.csv',index=False)\n",
    "            print(\"checking few rows.. for event : {} & date : {}\".format(event, date_param))\n",
    "            display(scraped_df.head())\n",
    "        else:\n",
    "            print(\"No data found for event : {} & date : {}\".format(event, date_param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating one more variant of same function for the date range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_yahoo_calendar_daterange(event_types, date_param):\n",
    "    \"\"\"Get the list of yahoo finance calendar and write them to CSV file \"\"\"\n",
    "    for event in event_types:\n",
    "        data_dict = {}\n",
    "        print('Web Scraping for ', event  )\n",
    "        data_dict = scrape_all_pages_daterange(event, date_param)\n",
    "        if len(data_dict) > 0:\n",
    "            scraped_df = pd.DataFrame(data_dict)\n",
    "            scraped_df.to_csv(event+'.csv',index=False)\n",
    "            print(\"checking few rows.. for event : {}\".format(event))\n",
    "            display(scraped_df.head())\n",
    "        else:\n",
    "            print(\"No data found for event : {} \".format(event))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cUgyY0IAJAY"
   },
   "source": [
    "calling the final function `scrape_yahoo_calendar_daterange`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web Scraping for  splits\n",
      "Processing for date : 2022-02-28\n",
      "Processing page # 1\n",
      "Scrape url for page 1 is https://finance.yahoo.com/calendar/splits?day=2022-02-28&offset=0&size=100\n",
      "Processing done\n",
      "Processing for date : 2022-03-01\n",
      "Processing page # 1\n",
      "Scrape url for page 1 is https://finance.yahoo.com/calendar/splits?day=2022-03-01&offset=0&size=100\n",
      "Processing done\n",
      "Processing for date : 2022-03-02\n",
      "Processing page # 1\n",
      "Scrape url for page 1 is https://finance.yahoo.com/calendar/splits?day=2022-03-02&offset=0&size=100\n",
      "Processing done\n",
      "checking few rows.. for event : splits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary-key</th>\n",
       "      <th>ticker</th>\n",
       "      <th>companyshortname</th>\n",
       "      <th>startdatetime</th>\n",
       "      <th>optionable</th>\n",
       "      <th>old_share_worth</th>\n",
       "      <th>share_worth</th>\n",
       "      <th>gmtOffsetMilliSeconds</th>\n",
       "      <th>quoteType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>543272.BO</td>\n",
       "      <td>Easy Trip Planners Ltd</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>TDRK</td>\n",
       "      <td>Tiderock Companies Inc</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>RI4.SG</td>\n",
       "      <td>POET Technologies Inc</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>POETF</td>\n",
       "      <td>POET Technologies Inc</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>GWAV</td>\n",
       "      <td>Greenwave Technology Solutions Inc</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>False</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  primary-key     ticker                    companyshortname  \\\n",
       "0  2022-02-28  543272.BO              Easy Trip Planners Ltd   \n",
       "1  2022-02-28       TDRK              Tiderock Companies Inc   \n",
       "2  2022-02-28     RI4.SG               POET Technologies Inc   \n",
       "3  2022-02-28      POETF               POET Technologies Inc   \n",
       "4  2022-02-28       GWAV  Greenwave Technology Solutions Inc   \n",
       "\n",
       "              startdatetime  optionable  old_share_worth  share_worth  \\\n",
       "0  2022-02-28T05:00:00.000Z       False                1            2   \n",
       "1  2022-02-28T05:00:00.000Z       False                1            4   \n",
       "2  2022-02-28T05:00:00.000Z       False               10            1   \n",
       "3  2022-02-28T05:00:00.000Z       False               10            1   \n",
       "4  2022-02-28T05:00:00.000Z       False              300            1   \n",
       "\n",
       "   gmtOffsetMilliSeconds quoteType  \n",
       "0                      0    EQUITY  \n",
       "1                      0    EQUITY  \n",
       "2                      0    EQUITY  \n",
       "3                      0    EQUITY  \n",
       "4                      0    EQUITY  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web Scraping for  economic\n",
      "Processing for date : 2022-02-28\n",
      "Processing page # 1\n",
      "Scrape url for page 1 is https://finance.yahoo.com/calendar/economic?day=2022-02-28&offset=0&size=100\n",
      "Processing done\n",
      "Processing for date : 2022-03-01\n",
      "Processing page # 1\n",
      "Scrape url for page 1 is https://finance.yahoo.com/calendar/economic?day=2022-03-01&offset=0&size=100\n",
      "Processing done\n",
      "Processing for date : 2022-03-02\n",
      "Processing page # 1\n",
      "Scrape url for page 1 is https://finance.yahoo.com/calendar/economic?day=2022-03-02&offset=0&size=100\n",
      "Processing done\n",
      "checking few rows.. for event : economic\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary-key</th>\n",
       "      <th>econ_release</th>\n",
       "      <th>country_code</th>\n",
       "      <th>startdatetime</th>\n",
       "      <th>period</th>\n",
       "      <th>after_release_actual</th>\n",
       "      <th>consensus_estimate</th>\n",
       "      <th>prior_release_actual</th>\n",
       "      <th>originally_reported_actual</th>\n",
       "      <th>gmtOffsetMilliSeconds</th>\n",
       "      <th>quoteType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>GDP Quarterly QQ</td>\n",
       "      <td>FI</td>\n",
       "      <td>2022-02-28T06:00:00.000Z</td>\n",
       "      <td>Q4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>None</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>GDP Quarterly YY</td>\n",
       "      <td>FI</td>\n",
       "      <td>2022-02-28T06:00:00.000Z</td>\n",
       "      <td>Q4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>None</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>GDP QQ Prelim</td>\n",
       "      <td>DK</td>\n",
       "      <td>2022-02-28T07:00:00.000Z</td>\n",
       "      <td>Q4</td>\n",
       "      <td>1.1</td>\n",
       "      <td>None</td>\n",
       "      <td>1.1</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>GDP YY Prelim</td>\n",
       "      <td>DK</td>\n",
       "      <td>2022-02-28T07:00:00.000Z</td>\n",
       "      <td>Q4</td>\n",
       "      <td>4.4</td>\n",
       "      <td>None</td>\n",
       "      <td>3.6</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>GDP Quarterly YY*</td>\n",
       "      <td>TR</td>\n",
       "      <td>2022-02-28T07:00:00.000Z</td>\n",
       "      <td>Q4</td>\n",
       "      <td>9.1</td>\n",
       "      <td>9</td>\n",
       "      <td>7.4</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  primary-key       econ_release country_code             startdatetime  \\\n",
       "0  2022-02-28   GDP Quarterly QQ           FI  2022-02-28T06:00:00.000Z   \n",
       "1  2022-02-28   GDP Quarterly YY           FI  2022-02-28T06:00:00.000Z   \n",
       "2  2022-02-28      GDP QQ Prelim           DK  2022-02-28T07:00:00.000Z   \n",
       "3  2022-02-28      GDP YY Prelim           DK  2022-02-28T07:00:00.000Z   \n",
       "4  2022-02-28  GDP Quarterly YY*           TR  2022-02-28T07:00:00.000Z   \n",
       "\n",
       "  period after_release_actual consensus_estimate prior_release_actual  \\\n",
       "0     Q4                  0.6               None                  0.8   \n",
       "1     Q4                  2.9               None                  4.2   \n",
       "2     Q4                  1.1               None                  1.1   \n",
       "3     Q4                  4.4               None                  3.6   \n",
       "4     Q4                  9.1                  9                  7.4   \n",
       "\n",
       "  originally_reported_actual  gmtOffsetMilliSeconds quoteType  \n",
       "0                        0.9                      0    EQUITY  \n",
       "1                        3.5                      0    EQUITY  \n",
       "2                       None                      0    EQUITY  \n",
       "3                       None                      0    EQUITY  \n",
       "4                        7.5                      0    EQUITY  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web Scraping for  ipo\n",
      "Processing for date : 2022-02-28\n",
      "Processing page # 1\n",
      "Scrape url for page 1 is https://finance.yahoo.com/calendar/ipo?day=2022-02-28&offset=0&size=100\n",
      "Processing done\n",
      "Processing for date : 2022-03-01\n",
      "Processing page # 1\n",
      "Scrape url for page 1 is https://finance.yahoo.com/calendar/ipo?day=2022-03-01&offset=0&size=100\n",
      "Processing done\n",
      "Processing for date : 2022-03-02\n",
      "Processing page # 1\n",
      "Scrape url for page 1 is https://finance.yahoo.com/calendar/ipo?day=2022-03-02&offset=0&size=100\n",
      "Processing done\n",
      "checking few rows.. for event : ipo\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary-key</th>\n",
       "      <th>ticker</th>\n",
       "      <th>companyshortname</th>\n",
       "      <th>exchange_short_name</th>\n",
       "      <th>filingdate</th>\n",
       "      <th>startdatetime</th>\n",
       "      <th>amendeddate</th>\n",
       "      <th>pricefrom</th>\n",
       "      <th>priceto</th>\n",
       "      <th>offerprice</th>\n",
       "      <th>currencyname</th>\n",
       "      <th>shares</th>\n",
       "      <th>dealtype</th>\n",
       "      <th>gmtOffsetMilliSeconds</th>\n",
       "      <th>quoteType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>CXAC</td>\n",
       "      <td>C5 Acquisition Corporation</td>\n",
       "      <td>NYSE</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>Expected</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>VBOC</td>\n",
       "      <td>Viscogliosi Brothers Acquisition Corp Common S...</td>\n",
       "      <td>Nasdaq</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Expected</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>VBOCW</td>\n",
       "      <td>Viscogliosi Brothers Acquisition Corp Warrant</td>\n",
       "      <td>Nasdaq</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Expected</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>COSM</td>\n",
       "      <td>Cosmos Holdings Inc. Common Stock</td>\n",
       "      <td>Nasdaq</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Expected</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>SCRMW</td>\n",
       "      <td>Screaming Eagle Acquisition Corp. Warrant</td>\n",
       "      <td>Nasdaq</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Expected</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  primary-key ticker                                   companyshortname  \\\n",
       "0  2022-02-28   CXAC                         C5 Acquisition Corporation   \n",
       "1  2022-02-28   VBOC  Viscogliosi Brothers Acquisition Corp Common S...   \n",
       "2  2022-02-28  VBOCW      Viscogliosi Brothers Acquisition Corp Warrant   \n",
       "3  2022-02-28   COSM                  Cosmos Holdings Inc. Common Stock   \n",
       "4  2022-02-28  SCRMW          Screaming Eagle Acquisition Corp. Warrant   \n",
       "\n",
       "  exchange_short_name filingdate             startdatetime amendeddate  \\\n",
       "0                NYSE       None  2022-02-28T05:00:00.000Z        None   \n",
       "1              Nasdaq       None  2022-02-28T05:00:00.000Z        None   \n",
       "2              Nasdaq       None  2022-02-28T05:00:00.000Z        None   \n",
       "3              Nasdaq       None  2022-02-28T05:00:00.000Z        None   \n",
       "4              Nasdaq       None  2022-02-28T05:00:00.000Z        None   \n",
       "\n",
       "   pricefrom  priceto  offerprice currencyname  shares  dealtype  \\\n",
       "0        NaN      NaN         NaN                  NaN  Expected   \n",
       "1        NaN      NaN         NaN          USD     NaN  Expected   \n",
       "2        NaN      NaN         NaN          USD     NaN  Expected   \n",
       "3        NaN      NaN         NaN          USD     NaN  Expected   \n",
       "4        NaN      NaN         NaN          USD     NaN  Expected   \n",
       "\n",
       "   gmtOffsetMilliSeconds quoteType  \n",
       "0                    0.0    EQUITY  \n",
       "1                    0.0    EQUITY  \n",
       "2                    0.0    EQUITY  \n",
       "3                    0.0    EQUITY  \n",
       "4                    0.0    EQUITY  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web Scraping for  earnings\n",
      "Processing for date : 2022-02-28\n",
      "Processing page # 1\n",
      "Scrape url for page 1 is https://finance.yahoo.com/calendar/earnings?day=2022-02-28&offset=0&size=100\n",
      "Processing page # 2\n",
      "Scrape url for page 2 is https://finance.yahoo.com/calendar/earnings?day=2022-02-28&offset=100&size=100\n",
      "Processing page # 3\n",
      "Scrape url for page 3 is https://finance.yahoo.com/calendar/earnings?day=2022-02-28&offset=200&size=100\n",
      "Processing done\n",
      "Processing for date : 2022-03-01\n",
      "Processing page # 1\n",
      "Scrape url for page 1 is https://finance.yahoo.com/calendar/earnings?day=2022-03-01&offset=0&size=100\n",
      "Processing page # 2\n",
      "Scrape url for page 2 is https://finance.yahoo.com/calendar/earnings?day=2022-03-01&offset=100&size=100\n",
      "Processing page # 3\n",
      "Scrape url for page 3 is https://finance.yahoo.com/calendar/earnings?day=2022-03-01&offset=200&size=100\n",
      "Processing done\n",
      "Processing for date : 2022-03-02\n",
      "Processing page # 1\n",
      "Scrape url for page 1 is https://finance.yahoo.com/calendar/earnings?day=2022-03-02&offset=0&size=100\n",
      "Processing page # 2\n",
      "Scrape url for page 2 is https://finance.yahoo.com/calendar/earnings?day=2022-03-02&offset=100&size=100\n",
      "Processing done\n",
      "checking few rows.. for event : earnings\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary-key</th>\n",
       "      <th>ticker</th>\n",
       "      <th>companyshortname</th>\n",
       "      <th>startdatetime</th>\n",
       "      <th>startdatetimetype</th>\n",
       "      <th>epsestimate</th>\n",
       "      <th>epsactual</th>\n",
       "      <th>epssurprisepct</th>\n",
       "      <th>timeZoneShortName</th>\n",
       "      <th>gmtOffsetMilliSeconds</th>\n",
       "      <th>quoteType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>DDD</td>\n",
       "      <td>3D Systems Corporation</td>\n",
       "      <td>2022-02-28T16:05:00.000Z</td>\n",
       "      <td>TAS</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.09</td>\n",
       "      <td>181.25</td>\n",
       "      <td>EST</td>\n",
       "      <td>-18000000</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>AAON</td>\n",
       "      <td>AAON, Inc.</td>\n",
       "      <td>2022-02-28T16:01:00.000Z</td>\n",
       "      <td>TAS</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-35.02</td>\n",
       "      <td>EST</td>\n",
       "      <td>-18000000</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>BDRFF</td>\n",
       "      <td>Beiersdorf Aktiengesellschaft</td>\n",
       "      <td>2022-03-01T02:15:00.000Z</td>\n",
       "      <td>TAS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EST</td>\n",
       "      <td>-18000000</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>AY</td>\n",
       "      <td>Atlantica Sustainable Infrastructure plc</td>\n",
       "      <td>2022-02-28T16:22:00.000Z</td>\n",
       "      <td>TAS</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>EST</td>\n",
       "      <td>-18000000</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>SEYMF</td>\n",
       "      <td>Solaria Energía y Medio Ambiente, S.A.</td>\n",
       "      <td>2022-02-28T19:28:00.000Z</td>\n",
       "      <td>TAS</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-23.81</td>\n",
       "      <td>EST</td>\n",
       "      <td>-18000000</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  primary-key ticker                          companyshortname  \\\n",
       "0  2022-02-28    DDD                    3D Systems Corporation   \n",
       "1  2022-02-28   AAON                                AAON, Inc.   \n",
       "2  2022-02-28  BDRFF             Beiersdorf Aktiengesellschaft   \n",
       "3  2022-02-28     AY  Atlantica Sustainable Infrastructure plc   \n",
       "4  2022-02-28  SEYMF    Solaria Energía y Medio Ambiente, S.A.   \n",
       "\n",
       "              startdatetime startdatetimetype  epsestimate  epsactual  \\\n",
       "0  2022-02-28T16:05:00.000Z               TAS         0.03       0.09   \n",
       "1  2022-02-28T16:01:00.000Z               TAS         0.28       0.18   \n",
       "2  2022-03-01T02:15:00.000Z               TAS          NaN        NaN   \n",
       "3  2022-02-28T16:22:00.000Z               TAS        -0.04      -0.10   \n",
       "4  2022-02-28T19:28:00.000Z               TAS         0.11       0.08   \n",
       "\n",
       "   epssurprisepct timeZoneShortName  gmtOffsetMilliSeconds quoteType  \n",
       "0          181.25               EST              -18000000    EQUITY  \n",
       "1          -35.02               EST              -18000000    EQUITY  \n",
       "2             NaN               EST              -18000000    EQUITY  \n",
       "3         -122.22               EST              -18000000    EQUITY  \n",
       "4          -23.81               EST              -18000000    EQUITY  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BASE_URL = 'https://finance.yahoo.com' #Global Variable\n",
    "date_param = ['2022-02-28','2022-03-01','2022-03-02']\n",
    "event_types = ['splits','economic','ipo','earnings']\n",
    "scrape_yahoo_calendar_daterange(event_types, date_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calling the final function `scrape_yahoo_calendar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "WpvYTWyx8djA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web Scraping for  splits\n",
      "Processing page # 1\n",
      "Scrape url for page 1 is https://finance.yahoo.com/calendar/splits?day=2022-02-28&offset=0&size=100\n",
      "checking few rows.. for event : splits & date : 2022-02-28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>companyshortname</th>\n",
       "      <th>startdatetime</th>\n",
       "      <th>optionable</th>\n",
       "      <th>old_share_worth</th>\n",
       "      <th>share_worth</th>\n",
       "      <th>gmtOffsetMilliSeconds</th>\n",
       "      <th>quoteType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RI4A.MU</td>\n",
       "      <td>POET Technologies Inc</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MXMG</td>\n",
       "      <td>Maxima Group Inc</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>False</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>543272.BO</td>\n",
       "      <td>Easy Trip Planners Ltd</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RI4.SG</td>\n",
       "      <td>POET Technologies Inc</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RI4A.F</td>\n",
       "      <td>POET Technologies Inc</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker        companyshortname             startdatetime  optionable  \\\n",
       "0    RI4A.MU   POET Technologies Inc  2022-02-28T05:00:00.000Z       False   \n",
       "1       MXMG        Maxima Group Inc  2022-02-28T05:00:00.000Z       False   \n",
       "2  543272.BO  Easy Trip Planners Ltd  2022-02-28T05:00:00.000Z       False   \n",
       "3     RI4.SG   POET Technologies Inc  2022-02-28T05:00:00.000Z       False   \n",
       "4     RI4A.F   POET Technologies Inc  2022-02-28T05:00:00.000Z       False   \n",
       "\n",
       "   old_share_worth  share_worth  gmtOffsetMilliSeconds quoteType  \n",
       "0               10            1                      0    EQUITY  \n",
       "1             1000            1                      0    EQUITY  \n",
       "2                1            2                      0    EQUITY  \n",
       "3               10            1                      0    EQUITY  \n",
       "4               10            1                      0    EQUITY  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web Scraping for  economic\n",
      "Processing page # 1\n",
      "Scrape url for page 1 is https://finance.yahoo.com/calendar/economic?day=2022-02-28&offset=0&size=100\n",
      "checking few rows.. for event : economic & date : 2022-02-28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>econ_release</th>\n",
       "      <th>country_code</th>\n",
       "      <th>startdatetime</th>\n",
       "      <th>period</th>\n",
       "      <th>after_release_actual</th>\n",
       "      <th>consensus_estimate</th>\n",
       "      <th>prior_release_actual</th>\n",
       "      <th>originally_reported_actual</th>\n",
       "      <th>gmtOffsetMilliSeconds</th>\n",
       "      <th>quoteType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GDP Quarterly QQ</td>\n",
       "      <td>FI</td>\n",
       "      <td>2022-02-28T06:00:00.000Z</td>\n",
       "      <td>Q4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>None</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GDP Quarterly YY</td>\n",
       "      <td>FI</td>\n",
       "      <td>2022-02-28T06:00:00.000Z</td>\n",
       "      <td>Q4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>None</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GDP QQ Prelim</td>\n",
       "      <td>DK</td>\n",
       "      <td>2022-02-28T07:00:00.000Z</td>\n",
       "      <td>Q4</td>\n",
       "      <td>1.1</td>\n",
       "      <td>None</td>\n",
       "      <td>1.1</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GDP YY Prelim</td>\n",
       "      <td>DK</td>\n",
       "      <td>2022-02-28T07:00:00.000Z</td>\n",
       "      <td>Q4</td>\n",
       "      <td>4.4</td>\n",
       "      <td>None</td>\n",
       "      <td>3.6</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GDP Quarterly YY*</td>\n",
       "      <td>TR</td>\n",
       "      <td>2022-02-28T07:00:00.000Z</td>\n",
       "      <td>Q4</td>\n",
       "      <td>9.1</td>\n",
       "      <td>9</td>\n",
       "      <td>7.4</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        econ_release country_code             startdatetime period  \\\n",
       "0   GDP Quarterly QQ           FI  2022-02-28T06:00:00.000Z     Q4   \n",
       "1   GDP Quarterly YY           FI  2022-02-28T06:00:00.000Z     Q4   \n",
       "2      GDP QQ Prelim           DK  2022-02-28T07:00:00.000Z     Q4   \n",
       "3      GDP YY Prelim           DK  2022-02-28T07:00:00.000Z     Q4   \n",
       "4  GDP Quarterly YY*           TR  2022-02-28T07:00:00.000Z     Q4   \n",
       "\n",
       "  after_release_actual consensus_estimate prior_release_actual  \\\n",
       "0                  0.6               None                  0.8   \n",
       "1                  2.9               None                  4.2   \n",
       "2                  1.1               None                  1.1   \n",
       "3                  4.4               None                  3.6   \n",
       "4                  9.1                  9                  7.4   \n",
       "\n",
       "  originally_reported_actual  gmtOffsetMilliSeconds quoteType  \n",
       "0                        0.9                      0    EQUITY  \n",
       "1                        3.5                      0    EQUITY  \n",
       "2                       None                      0    EQUITY  \n",
       "3                       None                      0    EQUITY  \n",
       "4                        7.5                      0    EQUITY  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web Scraping for  ipo\n",
      "Processing page # 1\n",
      "Scrape url for page 1 is https://finance.yahoo.com/calendar/ipo?day=2022-02-28&offset=0&size=100\n",
      "checking few rows.. for event : ipo & date : 2022-02-28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>companyshortname</th>\n",
       "      <th>exchange_short_name</th>\n",
       "      <th>filingdate</th>\n",
       "      <th>startdatetime</th>\n",
       "      <th>amendeddate</th>\n",
       "      <th>pricefrom</th>\n",
       "      <th>priceto</th>\n",
       "      <th>offerprice</th>\n",
       "      <th>currencyname</th>\n",
       "      <th>shares</th>\n",
       "      <th>dealtype</th>\n",
       "      <th>gmtOffsetMilliSeconds</th>\n",
       "      <th>quoteType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VBOC</td>\n",
       "      <td>Viscogliosi Brothers Acquisition Corp Common S...</td>\n",
       "      <td>Nasdaq</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Expected</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CXAC</td>\n",
       "      <td>C5 Acquisition Corporation</td>\n",
       "      <td>NYSE</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>Expected</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VBOCW</td>\n",
       "      <td>Viscogliosi Brothers Acquisition Corp Warrant</td>\n",
       "      <td>Nasdaq</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Expected</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COSM</td>\n",
       "      <td>Cosmos Holdings Inc. Common Stock</td>\n",
       "      <td>Nasdaq</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Expected</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SCRMW</td>\n",
       "      <td>Screaming Eagle Acquisition Corp. Warrant</td>\n",
       "      <td>Nasdaq</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-02-28T05:00:00.000Z</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Expected</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker                                   companyshortname  \\\n",
       "0   VBOC  Viscogliosi Brothers Acquisition Corp Common S...   \n",
       "1   CXAC                         C5 Acquisition Corporation   \n",
       "2  VBOCW      Viscogliosi Brothers Acquisition Corp Warrant   \n",
       "3   COSM                  Cosmos Holdings Inc. Common Stock   \n",
       "4  SCRMW          Screaming Eagle Acquisition Corp. Warrant   \n",
       "\n",
       "  exchange_short_name filingdate             startdatetime amendeddate  \\\n",
       "0              Nasdaq       None  2022-02-28T05:00:00.000Z        None   \n",
       "1                NYSE       None  2022-02-28T05:00:00.000Z        None   \n",
       "2              Nasdaq       None  2022-02-28T05:00:00.000Z        None   \n",
       "3              Nasdaq       None  2022-02-28T05:00:00.000Z        None   \n",
       "4              Nasdaq       None  2022-02-28T05:00:00.000Z        None   \n",
       "\n",
       "   pricefrom  priceto  offerprice currencyname  shares  dealtype  \\\n",
       "0        NaN      NaN         NaN          USD     NaN  Expected   \n",
       "1        NaN      NaN         NaN                  NaN  Expected   \n",
       "2        NaN      NaN         NaN          USD     NaN  Expected   \n",
       "3        NaN      NaN         NaN          USD     NaN  Expected   \n",
       "4        NaN      NaN         NaN          USD     NaN  Expected   \n",
       "\n",
       "   gmtOffsetMilliSeconds quoteType  \n",
       "0                    0.0    EQUITY  \n",
       "1                    0.0    EQUITY  \n",
       "2                    0.0    EQUITY  \n",
       "3                    0.0    EQUITY  \n",
       "4                    0.0    EQUITY  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web Scraping for  earnings\n",
      "Processing page # 1\n",
      "Scrape url for page 1 is https://finance.yahoo.com/calendar/earnings?day=2022-02-28&offset=0&size=100\n",
      "Processing page # 2\n",
      "Scrape url for page 2 is https://finance.yahoo.com/calendar/earnings?day=2022-02-28&offset=100&size=100\n",
      "Processing page # 3\n",
      "Scrape url for page 3 is https://finance.yahoo.com/calendar/earnings?day=2022-02-28&offset=200&size=100\n",
      "checking few rows.. for event : earnings & date : 2022-02-28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>companyshortname</th>\n",
       "      <th>startdatetime</th>\n",
       "      <th>startdatetimetype</th>\n",
       "      <th>epsestimate</th>\n",
       "      <th>epsactual</th>\n",
       "      <th>epssurprisepct</th>\n",
       "      <th>timeZoneShortName</th>\n",
       "      <th>gmtOffsetMilliSeconds</th>\n",
       "      <th>quoteType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DDD</td>\n",
       "      <td>3D Systems Corporation</td>\n",
       "      <td>2022-02-28T16:05:00.000Z</td>\n",
       "      <td>TAS</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.09</td>\n",
       "      <td>181.25</td>\n",
       "      <td>EST</td>\n",
       "      <td>-18000000</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DIDAF</td>\n",
       "      <td>Distribuidora Internacional de Alimentación, S.A.</td>\n",
       "      <td>2022-02-28T18:30:00.000Z</td>\n",
       "      <td>TAS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EST</td>\n",
       "      <td>-18000000</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CLQDF</td>\n",
       "      <td>Cliq Digital AG</td>\n",
       "      <td>2022-03-01T01:30:00.000Z</td>\n",
       "      <td>TAS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EST</td>\n",
       "      <td>-18000000</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RCEL</td>\n",
       "      <td>AVITA Medical, Inc.</td>\n",
       "      <td>2022-02-28T16:01:00.000Z</td>\n",
       "      <td>TAS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EST</td>\n",
       "      <td>-18000000</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASTE</td>\n",
       "      <td>Astec Industries, Inc.</td>\n",
       "      <td>2022-02-28T07:03:00.000Z</td>\n",
       "      <td>TAS</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-220.00</td>\n",
       "      <td>EST</td>\n",
       "      <td>-18000000</td>\n",
       "      <td>EQUITY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker                                   companyshortname  \\\n",
       "0    DDD                             3D Systems Corporation   \n",
       "1  DIDAF  Distribuidora Internacional de Alimentación, S.A.   \n",
       "2  CLQDF                                    Cliq Digital AG   \n",
       "3   RCEL                                AVITA Medical, Inc.   \n",
       "4   ASTE                             Astec Industries, Inc.   \n",
       "\n",
       "              startdatetime startdatetimetype  epsestimate  epsactual  \\\n",
       "0  2022-02-28T16:05:00.000Z               TAS         0.03       0.09   \n",
       "1  2022-02-28T18:30:00.000Z               TAS          NaN        NaN   \n",
       "2  2022-03-01T01:30:00.000Z               TAS          NaN        NaN   \n",
       "3  2022-02-28T16:01:00.000Z               TAS          NaN      -0.34   \n",
       "4  2022-02-28T07:03:00.000Z               TAS         0.03      -0.03   \n",
       "\n",
       "   epssurprisepct timeZoneShortName  gmtOffsetMilliSeconds quoteType  \n",
       "0          181.25               EST              -18000000    EQUITY  \n",
       "1             NaN               EST              -18000000    EQUITY  \n",
       "2             NaN               EST              -18000000    EQUITY  \n",
       "3             NaN               EST              -18000000    EQUITY  \n",
       "4         -220.00               EST              -18000000    EQUITY  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BASE_URL = 'https://finance.yahoo.com' #Global Variable \n",
    "#date_param = '2022-03-18' # no data condition\n",
    "date_param = '2022-02-28'\n",
    "event_types = ['splits','economic','ipo','earnings']\n",
    "scrape_yahoo_calendar(event_types, date_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBjhZrB7AJAY"
   },
   "source": [
    "Total 4 csv files \"event_type_yyyy-mm-dd.csv\" should be available in File $\\rightarrow$ Open Menu. You can download the file or directly open it in a browser. Please verify the file content and compare it with the actual information available on the webpage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpW06QKr8djA"
   },
   "source": [
    "**Summary** : This is a very useful technique which can be easily replicable. Without writing any customized code, we were able to extract the data from multiple types of web pages just by changing one variable (in this case `event_type`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jA-c9qFnAJAY"
   },
   "source": [
    "## References\n",
    "\n",
    "References to some useful links.\n",
    "\n",
    "- https://htmldog.com/guides/html/\n",
    "- https://selenium-python.readthedocs.io/index.html\n",
    "- https://stackoverflow.com/questions/27003423/staleelementreferenceexception-on-python-selenium\n",
    "- https://www.w3schools.com/js/js_json_intro.asp\n",
    "- https://hhsm95.dev/blog/the-importance-of-using-user-agent-to-scraping-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIcjbAgOAJAY"
   },
   "source": [
    "## Future Work\n",
    "\n",
    "Ideas for future work<br>\n",
    "- Automate this process using [AWS Lambda](https://aws.amazon.com/lambda/) to download daily market calendar, crypto-currencies & market news in CSV format.\n",
    "- Move the old files to an Archive folder append date-stamp to the file if required, also  delete the Archived files older than 2 weeks.\n",
    "- Process the raw data extracted from third technique using different methods of pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkY7RsOeAJAY"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial we implement the following web scraping techniques.\n",
    " - Use Requests, BeautifulSoup and HTML tags to extract web page.\n",
    " - Use Selenium to scrape data from dynamically loading websites.\n",
    " - Use embedded JSON data to scrape website.\n",
    "\n",
    "I hope I was able to teach you these webscraping methods and I hope you can use this knowledge to scrape any website.\n",
    "\n",
    "Thank you for reading. Happy coding!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7aSVLdJ1lrgw"
   },
   "outputs": [],
   "source": [
    "jovian.commit(project=\"yahoo-finance-web-scraper\",files=['stock-market-news.csv','crypto-currencies.csv','splits_2022-02-28.csv','economic_2022-02-28.csv','ipo_2022-02-28.csv','earnings_2022-02-28.csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2huL77IORxre"
   },
   "outputs": [],
   "source": [
    "jovian.commit(project=\"yahoo-finance-web-scraper\",files=['yahooscraper.py','lambda_function.py'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jovian.commit(project=\"yahoo-finance-web-scraper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "DvSMJZeClrgo"
   ],
   "name": "yahoo-finance-web-scraper.ipynb",
   "provenance": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
